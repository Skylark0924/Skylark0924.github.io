<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Blog | Junjia LIU </title> <meta name="author" content="Junjia LIU"> <meta name="description" content=""> <meta name="keywords" content="Robotics, Embodied AI, Humanoid Robots"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://skylark0924.github.io/blog/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Junjia LIU </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <div class="header-bar"> <h1>Junjia LIU's blog</h1> <h2></h2> </div> <div class="tag-category-list"> <ul class="p-0 m-0"> <li> <i class="fa-solid fa-hashtag fa-sm"></i> <a href="/blog/tag/formatting">formatting</a> </li> <p>•</p> <li> <i class="fa-solid fa-hashtag fa-sm"></i> <a href="/blog/tag/images">images</a> </li> <p>•</p> <li> <i class="fa-solid fa-hashtag fa-sm"></i> <a href="/blog/tag/links">links</a> </li> <p>•</p> <li> <i class="fa-solid fa-hashtag fa-sm"></i> <a href="/blog/tag/math">math</a> </li> <p>•</p> <li> <i class="fa-solid fa-hashtag fa-sm"></i> <a href="/blog/tag/code">code</a> </li> <p>•</p> <li> <i class="fa-solid fa-tag fa-sm"></i> <a href="/blog/category/blockquotes">blockquotes</a> </li> </ul> </div> <ul class="post-list"> <li> <div class="row"> <div class="col-sm-9"> <h3> <a class="post-title" href="/blog/2024/Rofunc0026/">Rofunc-The Full Process Python Package for Robot Learning from Demonstration and Robot Manipulation</a> </h3> <p></p> <p class="post-meta"> 5 min read   ·   January 24, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/humanoid-robots"> <i class="fa-solid fa-hashtag fa-sm"></i> humanoid-robots</a>   <a href="/blog/tag/reinforcement-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> reinforcement-learning</a>   <a href="/blog/tag/skill-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> skill-learning</a>     ·   <a href="/blog/category/code-project"> <i class="fa-solid fa-tag fa-sm"></i> code-project</a>   </p> </div> <div class="col-sm-3"> <img class="card-img" src="/assets/img/publication_preview/logo2_square.png" style="object-fit: cover; height: 90%" alt="image"> </div> </div> </li> <li> <h3> <a class="post-title" href="https://zhuanlan.zhihu.com/p/632495382" target="_blank" rel="external nofollow noopener">HuggingFace Transformers使用教程</a> <svg width="2rem" height="2rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </h3> <p></p> <blockquote data-pid="IwDrJ6_I">本教程基本是从Transformers的官方文档翻译而来，只是作为个人学习使用的总结。</blockquote> <p data-pid="LM5_0k_q"> Transformers 提供 API 和工具来轻松下载和训练最先进的预训练模型。使用预训练模型可以降低您的计算成本和碳足迹，并节省您从头开始训练模型所需的时间和资源。这些模型支持不同模式的常见任务，例如：</p> <p data-pid="Yb6-KOlh"> <b>自然语言处理</b>：文本分类、命名实体识别、问答、语言建模、摘要、翻译、多项选择和文本生成。<br> ️<b>计算机视觉</b>：图像分类、对象检测和分割。<br> ️<b>音频</b>：自动语音识别和音频分类。<b>r/&gt; 多模态</b>：表格问答、光学字符识别、扫描文档信息提取、视频分类、视觉问答。</p> <h2>安装</h2> <div class="highlight"><pre><code class="language-text"><span></span>pip install transformers
</code></pre></div> <blockquote data-pid="9zv6hpnp">安装数据集<br>pip install dataset<br><br>安装Accelerate<br>pip install accelerate<br><br>安装bitsandbytes<br>pip install bitsandbytes</blockquote> <h2>教程</h2> <h3>Pipeline</h3> <p data-pid="aIDht_AV">pipeline是使用Transformers库最简单直接的方法，它会自动下载并缓存默认的pre-trained model和tokenizer以进行相应的任务推理。</p> <div class="highlight"><pre><code class="language-python"><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>

<span class="n">classifier</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">"sentiment-analysis"</span><span class="p">)</span>

<span class="c1"># Inference</span>
<span class="n">classifier</span><span class="p">(</span><span class="s2">"We are very happy to show you the   Transformers library."</span><span class="p">)</span>
<span class="c1"># Output: [{'label': 'POSITIVE', 'score': 0.9998}]</span>

<span class="c1"># Inference for more than one input</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">([</span><span class="s2">"We are very happy to show you the   Transformers library."</span><span class="p">,</span> <span class="s2">"We hope you don't hate it."</span><span class="p">])</span>
<span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"label: </span><span class="si">{result['label']}</span><span class="s2">, with score: {round(result['score'], 4)}"</span><span class="p">)</span>
<span class="c1"># Output: label: POSITIVE, with score: 0.9998</span>
<span class="c1">#         label: NEGATIVE, with score: 0.5309</span>
</code></pre></div> <p data-pid="5XpG8rXZ">pipelinez支持的任务有</p> <table data-draft-node="block" data-draft-type="table" data-size="normal" data-row-style="striped"><tbody> <tr> <th>Task</th> <th>Description</th> <th>Modality</th> <th>Pipeline identifier</th> </tr> <tr> <td>Text classification</td> <td>assign a label to a given sequence of text</td> <td>NLP</td> <td>pipeline(task=“sentiment-analysis”)</td> </tr> <tr> <td>Text generation</td> <td>generate text given a prompt</td> <td>NLP</td> <td>pipeline(task=“text-generation”)</td> </tr> <tr> <td>Summarization</td> <td>generate a summary of a sequence of text or document</td> <td>NLP</td> <td>pipeline(task=“summarization”)</td> </tr> <tr> <td>Image classification</td> <td>assign a label to an image</td> <td>Computer vision</td> <td>pipeline(task=“image-classification”)</td> </tr> <tr> <td>Image segmentation</td> <td>assign a label to each individual pixel of an image (supports semantic, panoptic, and instance segmentation)</td> <td>Computer vision</td> <td>pipeline(task=“image-segmentation”)</td> </tr> <tr> <td>Object detection</td> <td>predict the bounding boxes and classes of objects in an image</td> <td>Computer vision</td> <td>pipeline(task=“object-detection”)</td> </tr> <tr> <td>Audio classification</td> <td>assign a label to some audio data</td> <td>Audio</td> <td>pipeline(task=“audio-classification”)</td> </tr> <tr> <td>Automatic speech recognition</td> <td>transcribe speech into text</td> <td>Audio</td> <td>pipeline(task=“automatic-speech-recognition”)</td> </tr> <tr> <td>Visual question answering</td> <td>answer a question about the image, given an image and a question</td> <td>Multimodal</td> <td>pipeline(task=“vqa”)</td> </tr> <tr> <td>Document question answering</td> <td>answer a question about a document, given an image and a question</td> <td>Multimodal</td> <td>pipeline(task=“document-question-answering”)</td> </tr> <tr> <td>Image captioning</td> <td>generate a caption for a given image</td> <td>Multimodal</td> <td>pipeline(task=“image-to-text”)</td> </tr> </tbody></table> <p data-pid="1jQ_MaXW">Pipeline还支持自定义pre-trained model和tokenizer</p> <div class="highlight"><pre><code class="language-python"><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForSequenceClassification</span>

<span class="n">model_name</span> <span class="o">=</span> <span class="s2">"nlptown/bert-base-multilingual-uncased-sentiment"</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>

<span class="n">classifier</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">"sentiment-analysis"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>
<span class="n">classifier</span><span class="p">(</span><span class="s2">"Nous sommes très heureux de vous présenter la bibliothèque   Transformers."</span><span class="p">)</span>
<span class="c1"># Output: [{'label': '5 stars', 'score': 0.7273}]</span>
</code></pre></div> <h3>pipeline参数</h3> <p data-pid="TGu0SZyU"><b>Devices</b></p> <div class="highlight"><pre><code class="language-python"><span></span><span class="n">generator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">"openai/whisper-large"</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># pip install accelerate</span>
<span class="c1"># 允许Accelerate自动确定如何在多个GPU上加载和存储模型权重。</span>
<span class="n">generator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">"openai/whisper-large"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">)</span>
</code></pre></div> <p data-pid="deAAcCiZ"><b>Batch size</b></p> <p data-pid="ne2iYb71">不一定比单个推理快</p> <div class="highlight"><pre><code class="language-python"><span></span><span class="n">generator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">"openai/whisper-large"</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">audio_filenames</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">"audio_</span><span class="si">{i}</span><span class="s2">.flac"</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)]</span>
<span class="n">texts</span> <span class="o">=</span> <span class="n">generator</span><span class="p">(</span><span class="n">audio_filenames</span><span class="p">)</span>
</code></pre></div> <p data-pid="MhoSAWlA"><b>Task specific parameters</b></p> <div class="highlight"><pre><code class="language-python"><span></span><span class="c1"># wav2vec2-large-960h-lv60-self的特定参数return_timestamps</span>
<span class="n">generator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">"facebook/wav2vec2-large-960h-lv60-self"</span><span class="p">,</span> <span class="n">return_timestamps</span><span class="o">=</span><span class="s2">"word"</span><span class="p">)</span>
<span class="n">generator</span><span class="p">(</span><span class="s2">"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac"</span><span class="p">)</span>
</code></pre></div> <h3>在自定义数据集上使用pipline</h3> <div class="highlight"><pre><code class="language-python"><span></span><span class="k">def</span> <span class="nf">data</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
        <span class="k">yield</span> <span class="sa">f</span><span class="s2">"My example </span><span class="si">{i}</span><span class="s2">"</span>


<span class="n">pipe</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">"gpt2"</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">generated_characters</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">out</span> <span class="ow">in</span> <span class="n">pipe</span><span class="p">(</span><span class="n">data</span><span class="p">()):</span>
    <span class="n">generated_characters</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">out</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">"generated_text"</span><span class="p">])</span>

<span class="c1"># KeyDataset is a util that will just output the item we're interested in.</span>
<span class="kn">from</span> <span class="nn">transformers.pipelines.pt_utils</span> <span class="kn">import</span> <span class="n">KeyDataset</span>
<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>

<span class="n">pipe</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">"hf-internal-testing/tiny-random-wav2vec2"</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">"hf-internal-testing/librispeech_asr_dummy"</span><span class="p">,</span> <span class="s2">"clean"</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">"validation[:10]"</span><span class="p">)</span>

<span class="k">for</span> <span class="n">out</span> <span class="ow">in</span> <span class="n">pipe</span><span class="p">(</span><span class="n">KeyDataset</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="s2">"audio"</span><span class="p">)):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</code></pre></div> <h3>视觉pipeline</h3> <div class="highlight"><pre><code class="language-python"><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>

<span class="n">vision_classifier</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">"google/vit-base-patch16-224"</span><span class="p">)</span>
<span class="n">preds</span> <span class="o">=</span> <span class="n">vision_classifier</span><span class="p">(</span>
    <span class="n">images</span><span class="o">=</span><span class="s2">"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"</span>
<span class="p">)</span>
<span class="n">preds</span> <span class="o">=</span> <span class="p">[{</span><span class="s2">"score"</span><span class="p">:</span> <span class="nb">round</span><span class="p">(</span><span class="n">pred</span><span class="p">[</span><span class="s2">"score"</span><span class="p">],</span> <span class="mi">4</span><span class="p">),</span> <span class="s2">"label"</span><span class="p">:</span> <span class="n">pred</span><span class="p">[</span><span class="s2">"label"</span><span class="p">]}</span> <span class="k">for</span> <span class="n">pred</span> <span class="ow">in</span> <span class="n">preds</span><span class="p">]</span>
<span class="c1"># Output: [{'score': 0.4335, 'label': 'lynx, catamount'}, {'score': 0.0348, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.0324, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.0239, 'label': 'Egyptian cat'}, {'score': 0.0229, 'label': 'tiger cat'}]</span>
</code></pre></div> <h3>文本pipeline</h3> <div class="highlight"><pre><code class="language-python"><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>

<span class="c1"># This model is a `zero-shot-classification` model.</span>
<span class="c1"># It will classify text, except you are free to choose any label you might imagine</span>
<span class="n">classifier</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">"facebook/bart-large-mnli"</span><span class="p">)</span>
<span class="n">classifier</span><span class="p">(</span>
    <span class="s2">"I have a problem with my iphone that needs to be resolved asap!!"</span><span class="p">,</span>
    <span class="n">candidate_labels</span><span class="o">=</span><span class="p">[</span><span class="s2">"urgent"</span><span class="p">,</span> <span class="s2">"not urgent"</span><span class="p">,</span> <span class="s2">"phone"</span><span class="p">,</span> <span class="s2">"tablet"</span><span class="p">,</span> <span class="s2">"computer"</span><span class="p">],</span>
<span class="p">)</span>
<span class="c1"># Output: {'sequence': 'I have a problem with my iphone that needs to be resolved asap!!', 'labels': ['urgent', 'phone', 'computer', 'not urgent', 'tablet'], 'scores': [0.504, 0.479, 0.013, 0.003, 0.002]}</span>
</code></pre></div> <h3>多模态pipeline</h3> <div class="highlight"><pre><code class="language-python"><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>

<span class="n">vqa</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">"impira/layoutlm-document-qa"</span><span class="p">)</span>
<span class="n">vqa</span><span class="p">(</span>
    <span class="n">image</span><span class="o">=</span><span class="s2">"https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png"</span><span class="p">,</span>
    <span class="n">question</span><span class="o">=</span><span class="s2">"What is the invoice number?"</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1"># Output: [{'score': 0.42515, 'answer': 'us-001', 'start': 16, 'end': 16}]</span>
</code></pre></div> <h3>结合accelerate使用大模型</h3> <div class="highlight"><pre><code class="language-python"><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>

<span class="n">pipe</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">"facebook/opt-1.3b"</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">pipe</span><span class="p">(</span><span class="s2">"This is a cool example!"</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>

<span class="n">pipe</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">"facebook/opt-1.3b"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">,</span> <span class="n">model_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">"load_in_8bit"</span><span class="p">:</span> <span class="kc">True</span><span class="p">})</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">pipe</span><span class="p">(</span><span class="s2">"This is a cool example!"</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">)</span>
</code></pre></div> <h3>AutoClass</h3> <p data-pid="W0Psnqbx">Transformers提供的AutoClass支持从名称或路径自动检索预训练模型的架构。</p> <h3>数据预处理</h3> <p data-pid="SE2Ot_NF">在数据集上训练模型之前，需要将其预处理为预期的模型输入格式。无论数据是文本、图像还是音频，都需要将它们转换成批次的张量。</p> <p data-pid="dgGJQ9Te">AutoTokenizer</p> <p data-pid="EQwu6Rc2">分词器负责将文本预处理为array，作为模型的输入。有多个规则管理标记化过程，包括如何拆分单词以及应在什么级别拆分单词（在标记器摘要中了解有关标记化的更多信息）。最重要的是要记住，您需要使用相同的模型名称实例化分词器，以确保您使用的是与预训练模型相同的分词规则。</p> <div class="highlight"><pre><code class="language-python"><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">model_name</span> <span class="o">=</span> <span class="s2">"nlptown/bert-base-multilingual-uncased-sentiment"</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>

<span class="c1"># 用tokenizer来编码文本</span>
<span class="n">encoded_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">"Do not meddle in the affairs of wizards, for they are subtle and quick to anger."</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">encoded_input</span><span class="p">)</span>
<span class="c1"># Output: {'input_ids': [101, 2079, 2025, 19960, 10362, 1999, 1996, 3821, 1997, 16657, 1010, 2005, 2027, 2024, 11259, 1998, 4248, 2000, 4963, 1012, 102], </span>
<span class="c1">#  				'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], </span>
<span class="c1">#         'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}</span>

<span class="c1"># 解码文本</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">encoded_input</span><span class="p">[</span><span class="s2">"input_ids"</span><span class="p">])</span>
<span class="c1"># Output： '[CLS] Do not meddle in the affairs of wizards, for they are subtle and quick to anger. [SEP]'</span>


<span class="c1"># 多序列输入</span>
<span class="n">batch_sentences</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">"But what about second breakfast?"</span><span class="p">,</span>
    <span class="s2">"Don't think he knows about second breakfast, Pip."</span><span class="p">,</span>
    <span class="s2">"What about elevensies?"</span><span class="p">,</span>
<span class="p">]</span>
<span class="n">encoded_inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">batch_sentences</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">encoded_inputs</span><span class="p">)</span>
<span class="c1"># Output：{'input_ids': [[101, 1252, 1184, 1164, 1248, 6462, 136, 102], </span>
<span class="c1">#          [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102], </span>
<span class="c1">#         [101, 1327, 1164, 5450, 23434, 136, 102]], </span>
<span class="c1"># 				'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0], </span>
<span class="c1">#                    				[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], </span>
<span class="c1">#                    				[0, 0, 0, 0, 0, 0, 0]], </span>
<span class="c1"># 				'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], </span>
<span class="c1">#                    				[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], </span>
<span class="c1">#                    				[1, 1, 1, 1, 1, 1, 1]]}</span>
</code></pre></div> <p data-pid="ZedEOOWl">tokenizer返回一个字典，其中包含：</p> <ul> <li data-pid="r3Gp8TSM"> <a href="http://link.zhihu.com/?target=https%3A//huggingface.co/docs/transformers/v4.29.1/en/glossary%23input-ids" class=" wrap external" target="_blank" rel="nofollow noreferrer">input_ids</a>：tokens的数字表征。</li> <li data-pid="3Ld_9Hkr"> <a href="http://link.zhihu.com/?target=https%3A//huggingface.co/docs/transformers/v4.29.1/en/glossary%23attention-mask" class=" wrap external" target="_blank" rel="nofollow noreferrer">attention_mask</a>：表示应注意哪些tokens。</li> <li data-pid="5amkxRkf"> <a href="http://link.zhihu.com/?target=https%3A//huggingface.co/docs/transformers/v4.29.1/en/glossary%23token-type-ids" class=" wrap external" target="_blank" rel="nofollow noreferrer">token_type_ids</a>：表示当存在多个序列时token属于哪个序列。</li> </ul> <p data-pid="Mn5CQK2D">Pad</p> <p data-pid="wEcALTgM">句子的长度并不总是相同，这可能是一个问题，因为模型输入的张量需要具有统一的形状。填充是一种通过向较短的句子添加特殊的<i>padding token</i>来确保张量是矩形的策略。</p> <div class="highlight"><pre><code class="language-python"><span></span><span class="n">batch_sentences</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">"But what about second breakfast?"</span><span class="p">,</span>
    <span class="s2">"Don't think he knows about second breakfast, Pip."</span><span class="p">,</span>
    <span class="s2">"What about elevensies?"</span><span class="p">,</span>
<span class="p">]</span>
<span class="n">encoded_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">batch_sentences</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">encoded_input</span><span class="p">)</span>
<span class="c1"># Output: {'input_ids': [[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0], </span>
<span class="c1">#          [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102], </span>
<span class="c1">#         [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]], </span>
<span class="c1"># 				'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], </span>
<span class="c1">#                    				[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], </span>
<span class="c1">#                    				[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], </span>
<span class="c1"># 				'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], </span>
<span class="c1">#                    				[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], </span>
<span class="c1">#                    				[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]}</span>
</code></pre></div> <p data-pid="IGvw5PqV">Truncation</p> <p data-pid="vGxGgH7e">另一方面，有时序列对于模型来说可能太长而无法处理。在这种情况下，您需要将序列截断为更短的长度。</p> <p data-pid="bApU8mpd">将参数设置<code>truncation</code>为<code>True</code>将序列截断为模型接受的最大长度：</p> <div class="highlight"><pre><code class="language-python"><span></span><span class="n">batch_sentences</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">"But what about second breakfast?"</span><span class="p">,</span>
    <span class="s2">"Don't think he knows about second breakfast, Pip."</span><span class="p">,</span>
    <span class="s2">"What about elevensies?"</span><span class="p">,</span>
<span class="p">]</span>
<span class="n">encoded_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">batch_sentences</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">encoded_input</span><span class="p">)</span>
<span class="c1"># Output: {'input_ids': [[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0], </span>
<span class="c1">#          [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102], </span>
<span class="c1">#         [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]], </span>
<span class="c1"># 				'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], </span>
<span class="c1">#                    				[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], </span>
<span class="c1">#                    				[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], </span>
<span class="c1"># 				'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], </span>
<span class="c1">#                    				[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], </span>
<span class="c1">#                    				[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]}</span>
</code></pre></div> <p data-pid="WrdQJJ7X">Build tensors</p> <div class="highlight"><pre><code class="language-python"><span></span><span class="n">batch_sentences</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">"But what about second breakfast?"</span><span class="p">,</span>
    <span class="s2">"Don't think he knows about second breakfast, Pip."</span><span class="p">,</span>
    <span class="s2">"What about elevensies?"</span><span class="p">,</span>
<span class="p">]</span>
<span class="n">encoded_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">batch_sentences</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">encoded_input</span><span class="p">)</span>
<span class="c1"># Output: {'input_ids': tensor([[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0], </span>
<span class="c1">#          [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102], </span>
<span class="c1">#         [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]]), </span>
<span class="c1"># 				'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], </span>
<span class="c1">#                    								[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], </span>
<span class="c1">#                    								[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), </span>
<span class="c1"># 				'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], </span>
<span class="c1">#                    								[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], </span>
<span class="c1">#                    								[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}</span>
</code></pre></div> <p data-pid="hx4KM8c-">AutoImageProcessor 图像预处理</p> <div class="highlight"><pre><code class="language-python"><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoImageProcessor</span>
<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>
<span class="kn">from</span> <span class="nn">torchvision.transforms</span> <span class="kn">import</span> <span class="n">RandomResizedCrop</span><span class="p">,</span> <span class="n">ColorJitter</span><span class="p">,</span> <span class="n">Compose</span>

<span class="c1"># Load the food101 dataset, split参数可以指定加载数据集的大小</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">"food101"</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">"train[:100]"</span><span class="p">)</span>

<span class="n">image_processor</span> <span class="o">=</span> <span class="n">AutoImageProcessor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"google/vit-base-patch16-224"</span><span class="p">)</span>

<span class="c1"># 图像增强</span>
<span class="n">size</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">image_processor</span><span class="o">.</span><span class="n">size</span><span class="p">[</span><span class="s2">"shortest_edge"</span><span class="p">]</span>
    <span class="k">if</span> <span class="s2">"shortest_edge"</span> <span class="ow">in</span> <span class="n">image_processor</span><span class="o">.</span><span class="n">size</span>
    <span class="k">else</span> <span class="p">(</span><span class="n">image_processor</span><span class="o">.</span><span class="n">size</span><span class="p">[</span><span class="s2">"height"</span><span class="p">],</span> <span class="n">image_processor</span><span class="o">.</span><span class="n">size</span><span class="p">[</span><span class="s2">"width"</span><span class="p">])</span>
<span class="p">)</span>
<span class="n">_transforms</span> <span class="o">=</span> <span class="n">Compose</span><span class="p">([</span><span class="n">RandomResizedCrop</span><span class="p">(</span><span class="n">size</span><span class="p">),</span> <span class="n">ColorJitter</span><span class="p">(</span><span class="n">brightness</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)])</span>

<span class="k">def</span> <span class="nf">transforms</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>
    <span class="n">images</span> <span class="o">=</span> <span class="p">[</span><span class="n">_transforms</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="s2">"RGB"</span><span class="p">))</span> <span class="k">for</span> <span class="n">img</span> <span class="ow">in</span> <span class="n">examples</span><span class="p">[</span><span class="s2">"image"</span><span class="p">]]</span>
    <span class="n">examples</span><span class="p">[</span><span class="s2">"pixel_values"</span><span class="p">]</span> <span class="o">=</span> <span class="n">image_processor</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">do_resize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)[</span><span class="s2">"pixel_values"</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">examples</span>
  
<span class="n">dataset</span><span class="o">.</span><span class="n">set_transform</span><span class="p">(</span><span class="n">transforms</span><span class="p">)</span>
</code></pre></div> <p data-pid="dLSGnPBI">Pad</p> <p data-pid="0ZXbuxhW">在某些情况下，例如，在微调 DETR 时，模型会在训练时应用规模增强。这可能会导致图像在批处理中具有不同的大小。您可以使用 DetrImageProcessor 中的 DetrImageProcessor.pad_and_create_pixel_mask（） 并定义自定义collate_fn以将图像批处理在一起。</p> <div class="highlight"><pre><code class="language-python"><span></span><span class="k">def</span> <span class="nf">collate_fn</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span>
    <span class="n">pixel_values</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span><span class="p">[</span><span class="s2">"pixel_values"</span><span class="p">]</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">]</span>
    <span class="n">encoding</span> <span class="o">=</span> <span class="n">image_processor</span><span class="o">.</span><span class="n">pad_and_create_pixel_mask</span><span class="p">(</span><span class="n">pixel_values</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span><span class="p">[</span><span class="s2">"labels"</span><span class="p">]</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">]</span>
    <span class="n">batch</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">batch</span><span class="p">[</span><span class="s2">"pixel_values"</span><span class="p">]</span> <span class="o">=</span> <span class="n">encoding</span><span class="p">[</span><span class="s2">"pixel_values"</span><span class="p">]</span>
    <span class="n">batch</span><span class="p">[</span><span class="s2">"pixel_mask"</span><span class="p">]</span> <span class="o">=</span> <span class="n">encoding</span><span class="p">[</span><span class="s2">"pixel_mask"</span><span class="p">]</span>
    <span class="n">batch</span><span class="p">[</span><span class="s2">"labels"</span><span class="p">]</span> <span class="o">=</span> <span class="n">labels</span>
    <span class="k">return</span> <span class="n">batch</span>
</code></pre></div> <p data-pid="4OWyoKH2">AutoFeatureExtractor 语音预处理</p> <div class="highlight"><pre><code class="language-python"><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoFeatureExtractor</span>
<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span><span class="p">,</span> <span class="n">Audio</span>

<span class="c1"># Load the MInDS-14 dataset</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">"PolyAI/minds14"</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">"en-US"</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">"train"</span><span class="p">)</span>
<span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">"audio"</span><span class="p">]</span>
<span class="c1"># Output: {'array': array([ 0.        ,  0.00024414, -0.00024414, ..., -0.00024414,</span>
<span class="c1">#        						 0.        ,  0.        ], dtype=float32),</span>
<span class="c1"># 					'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav', 'sampling_rate': 8000}</span>

<span class="c1"># 在本教程中，您将使用 Wav2Vec2 模型。看看模型卡，您将了解到Wav2Vec2是在16kHz采样语音音频上预先训练的。音频数据的采样率与用于预训练模型的数据集的采样率相匹配非常重要。如果数据的采样率不同，则需要对数据重新采样。</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">cast_column</span><span class="p">(</span><span class="s2">"audio"</span><span class="p">,</span> <span class="n">Audio</span><span class="p">(</span><span class="n">sampling_rate</span><span class="o">=</span><span class="mi">16_000</span><span class="p">))</span>
<span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">"audio"</span><span class="p">]</span>
<span class="c1"># Output: {'array': array([ 2.3443763e-05,  2.1729663e-04,  2.2145823e-04, ...,</span>
<span class="c1">#         					3.8356509e-05, -7.3497440e-06, -2.1754686e-05], dtype=float32),</span>
<span class="c1"># 					'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav', 'sampling_rate': 16000}</span>


<span class="c1"># 加载特征提取器以规范化和填充输入。填充文本数据时，为较短的序列添加 0。同样的想法适用于音频数据。特征提取器向数组添加一个 0 - 解释为静音。</span>
<span class="n">feature_extractor</span> <span class="o">=</span> <span class="n">AutoFeatureExtractor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"facebook/wav2vec2-base"</span><span class="p">)</span>

<span class="n">audio_input</span> <span class="o">=</span> <span class="p">[</span><span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">"audio"</span><span class="p">][</span><span class="s2">"array"</span><span class="p">]]</span>
<span class="n">feature_extractor</span><span class="p">(</span><span class="n">audio_input</span><span class="p">,</span> <span class="n">sampling_rate</span><span class="o">=</span><span class="mi">16000</span><span class="p">)</span>

<span class="c1"># 创建一个函数来预处理数据集，使音频样本的长度相同。指定最大样本长度，特征提取器将填充或截断序列以匹配它：</span>
<span class="k">def</span> <span class="nf">preprocess_function</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>
    <span class="n">audio_arrays</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="s2">"array"</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">examples</span><span class="p">[</span><span class="s2">"audio"</span><span class="p">]]</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">feature_extractor</span><span class="p">(</span>
        <span class="n">audio_arrays</span><span class="p">,</span>
        <span class="n">sampling_rate</span><span class="o">=</span><span class="mi">16000</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">max_length</span><span class="o">=</span><span class="mi">100000</span><span class="p">,</span>
        <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">inputs</span>

<span class="n">processed_dataset</span> <span class="o">=</span> <span class="n">preprocess_function</span><span class="p">(</span><span class="n">dataset</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>

<span class="n">processed_dataset</span><span class="p">[</span><span class="s2">"input_values"</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
<span class="c1"># Output: (100000,)</span>
<span class="n">processed_dataset</span><span class="p">[</span><span class="s2">"input_values"</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
<span class="c1"># Output: (100000,)</span>
</code></pre></div> <p data-pid="r9Q4_R4g">AutoProcessor 多模态预处理</p> <div class="highlight"><pre><code class="language-python"><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoProcessor</span>
<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>

<span class="c1"># Load the LJ Speech dataset (for automatic speech recognition (ASR))</span>
<span class="n">lj_speech</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">"lj_speech"</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">"train"</span><span class="p">)</span>

<span class="c1"># 删掉不关注的列</span>
<span class="n">lj_speech</span> <span class="o">=</span> <span class="n">lj_speech</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">remove_columns</span><span class="o">=</span><span class="p">[</span><span class="s2">"file"</span><span class="p">,</span> <span class="s2">"id"</span><span class="p">,</span> <span class="s2">"normalized_text"</span><span class="p">])</span>
<span class="n">lj_speech</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">"audio"</span><span class="p">]</span>
<span class="c1"># Output：{'array': array([-7.3242188e-04, -7.6293945e-04, -6.4086914e-04, ...,</span>
<span class="c1">#         7.3242188e-04,  2.1362305e-04,  6.1035156e-05], dtype=float32),</span>
<span class="c1"># 'path': '/root/.cache/huggingface/datasets/downloads/extracted/917ece08c95cf0c4115e45294e3cd0dee724a1165b7fc11798369308a465bd26/LJSpeech-1.1/wavs/LJ001-0001.wav', 'sampling_rate': 22050}</span>
<span class="n">lj_speech</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">"text"</span><span class="p">]</span>
<span class="c1"># Output：'Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition'</span>

<span class="n">processor</span> <span class="o">=</span> <span class="n">AutoProcessor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"facebook/wav2vec2-base-960h"</span><span class="p">)</span>

<span class="c1"># 创建一个函数来处理数组中包含的音频数据作为input_values，并将文本标记为标签。</span>
<span class="k">def</span> <span class="nf">prepare_dataset</span><span class="p">(</span><span class="n">example</span><span class="p">):</span>
    <span class="n">audio</span> <span class="o">=</span> <span class="n">example</span><span class="p">[</span><span class="s2">"audio"</span><span class="p">]</span>
    <span class="n">example</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">processor</span><span class="p">(</span><span class="n">audio</span><span class="o">=</span><span class="n">audio</span><span class="p">[</span><span class="s2">"array"</span><span class="p">],</span> <span class="n">text</span><span class="o">=</span><span class="n">example</span><span class="p">[</span><span class="s2">"text"</span><span class="p">],</span> <span class="n">sampling_rate</span><span class="o">=</span><span class="mi">16000</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">example</span>

<span class="c1"># 调用函数处理一个样本</span>
<span class="n">prepare_dataset</span><span class="p">(</span><span class="n">lj_speech</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div> <h3>AutoModel</h3> <p data-pid="1MflDdh4">为任务选择正确的<a href="http://link.zhihu.com/?target=https%3A//huggingface.co/docs/transformers/v4.29.1/en/model_doc/auto%23transformers.AutoModel" class=" wrap external" target="_blank" rel="nofollow noreferrer">自动模型。</a>对于文本（或序列）分类，您应该加载<a href="http://link.zhihu.com/?target=https%3A//huggingface.co/docs/transformers/v4.29.1/en/model_doc/auto%23transformers.AutoModelForSequenceClassification" class=" wrap external" target="_blank" rel="nofollow noreferrer">AutoModelForSequenceClassification</a>：</p> <div class="highlight"><pre><code class="language-python"><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForSequenceClassification</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="n">model_name</span> <span class="o">=</span> <span class="s2">"nlptown/bert-base-multilingual-uncased-sentiment"</span>
<span class="n">pt_model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>

<span class="n">pt_outputs</span> <span class="o">=</span> <span class="n">pt_model</span><span class="p">(</span><span class="o">**</span><span class="n">pt_batch</span><span class="p">)</span>

<span class="n">pt_predictions</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">pt_outputs</span><span class="o">.</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pt_predictions</span><span class="p">)</span>
<span class="c1"># Output: tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],</span>
<span class="c1">#                 [0.2084, 0.1826, 0.1969, 0.1755, 0.2365]], grad_fn=&lt;SoftmaxBackward0&gt;)</span>
</code></pre></div> <h3>Save &amp; load models</h3> <div class="highlight"><pre><code class="language-python"><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForSequenceClassification</span>

<span class="c1"># Save the model</span>
<span class="n">pt_save_directory</span> <span class="o">=</span> <span class="s2">"./pt_save_pretrained"</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">pt_save_directory</span><span class="p">)</span>
<span class="n">pt_model</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">pt_save_directory</span><span class="p">)</span>

<span class="c1"># Load the model</span>
<span class="n">pt_model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"./pt_save_pretrained"</span><span class="p">)</span>

<span class="c1"># 支持加载不同架构（torch, tensorflow）学习的模型</span>
<span class="n">tf_save_directory</span> <span class="o">=</span> <span class="s2">"./tf_save_pretrained"</span> 
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">tf_save_directory</span><span class="p">)</span>
<span class="n">pt_model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">tf_save_directory</span><span class="p">,</span> <span class="n">from_tf</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div> <h3>数据预处理</h3> <ul> <li data-pid="Ynu8Pb9n">文本，使用<a href="http://link.zhihu.com/?target=https%3A//huggingface.co/docs/transformers/v4.29.1/en/main_classes/tokenizer" class=" wrap external" target="_blank" rel="nofollow noreferrer">Tokenizer</a>将文本转换为一系列标记，创建标记的数字表示，并将它们组装成张量。</li> <li data-pid="c3rmBvxu">语音和音频，使用<a href="http://link.zhihu.com/?target=https%3A//huggingface.co/docs/transformers/v4.29.1/en/main_classes/feature_extractor" class=" wrap external" target="_blank" rel="nofollow noreferrer">特征提取器</a>从音频波形中提取序列特征并将其转换为张量。</li> <li data-pid="uj2FTXw4">图像输入使用<a href="http://link.zhihu.com/?target=https%3A//huggingface.co/docs/transformers/v4.29.1/en/main_classes/image" class=" wrap external" target="_blank" rel="nofollow noreferrer">ImageProcessor</a>将图像转换为张量。</li> <li data-pid="BsKS9xQf">多模态输入，使用<a href="http://link.zhihu.com/?target=https%3A//huggingface.co/docs/transformers/v4.29.1/en/main_classes/processors" class=" wrap external" target="_blank" rel="nofollow noreferrer">处理器</a>来组合分词器和特征提取器或图像处理器。</li> </ul> <h3>自定义模型构建</h3> <p data-pid="1thD8Bm1">您可以修改模型的配置类以更改模型的构建方式。该配置指定模型的属性，例如隐藏层或注意头的数量。当您从自定义配置类初始化模型时，您是从头开始的。模型属性是随机初始化的，您需要先训练模型，然后才能使用它获得有意义的结果。</p> <div class="highlight"><pre><code class="language-python"><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoConfig</span><span class="p">,</span> <span class="n">AutoModel</span>

<span class="n">my_config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"distilbert-base-uncased"</span><span class="p">,</span> <span class="n">n_heads</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">my_model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="o">.</span><span class="n">from_config</span><span class="p">(</span><span class="n">my_config</span><span class="p">)</span>
</code></pre></div> <p data-pid="HhmOnJSS"><a href="http://link.zhihu.com/?target=https%3A//huggingface.co/docs/transformers/v4.29.1/en/create_a_model" class=" external" target="_blank" rel="nofollow noreferrer"><span class="invisible">https://</span><span class="visible">huggingface.co/docs/tra</span><span class="invisible">nsformers/v4.29.1/en/create_a_model</span><span class="ellipsis"></span></a></p> <h3>训练</h3> <p data-pid="1YijM-Tb">所有模型都是标准模型<code><a href="http://link.zhihu.com/?target=https%3A//pytorch.org/docs/stable/nn.html%23torch.nn.Module" class=" wrap external" target="_blank" rel="nofollow noreferrer">torch.nn.Module</a></code>，因此您可以在任何典型的训练循环中使用它们。虽然您可以编写自己的训练循环，但 Transformers为 PyTorch提供了一个<a href="http://link.zhihu.com/?target=https%3A//huggingface.co/docs/transformers/v4.29.1/en/main_classes/trainer%23transformers.Trainer" class=" wrap external" target="_blank" rel="nofollow noreferrer">Trainer类，其中包含基本的训练循环并为分布式训练、混合精度等特性添加了额外的功能。</a></p> <div class="highlight"><pre><code class="language-python"><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForSequenceClassification</span><span class="p">,</span> <span class="n">TrainingArguments</span><span class="p">,</span> <span class="n">AutoTokenizer</span><span class="err">，</span> <span class="n">DataCollatorWithPadding</span><span class="p">,</span> <span class="n">Trainer</span>

<span class="c1"># A PreTrainedModel or a torch.nn.Module</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"distilbert-base-uncased"</span><span class="p">)</span>

<span class="c1"># TrainingArguments 包含可以更改的模型超参数，例如学习率、批量大小和要训练的 epoch 数。如果未指定任何训练参数，则使用默认值</span>
<span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>
    <span class="n">output_dir</span><span class="o">=</span><span class="s2">"path/to/save/folder/"</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">2e-5</span><span class="p">,</span>
    <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># 预处理类，如分词器、图像处理器、特征提取器或处理器</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"distilbert-base-uncased"</span><span class="p">)</span>

<span class="c1"># 加载数据集</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">"rotten_tomatoes"</span><span class="p">)</span> 

<span class="c1"># 创建一个函数来标记数据集，并用它处理整个数据集</span>
<span class="k">def</span> <span class="nf">tokenize_dataset</span><span class="p">(</span><span class="n">dataset</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s2">"text"</span><span class="p">])</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_dataset</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># DataCollatorWithPadding 用于从数据集创建一批示例</span>
<span class="n">data_collator</span> <span class="o">=</span> <span class="n">DataCollatorWithPadding</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>

<span class="c1"># 开始训练</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">args</span><span class="o">=</span><span class="n">training_args</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">],</span>
    <span class="n">eval_dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">[</span><span class="s2">"test"</span><span class="p">],</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">data_collator</span><span class="o">=</span><span class="n">data_collator</span><span class="p">,</span>
<span class="p">)</span>  <span class="c1"># doctest: +SKIP</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</code></pre></div> <blockquote data-pid="VguwoGGU">对于使用序列到序列模型的任务（如翻译或摘要），请改用<a href="http://link.zhihu.com/?target=https%3A//huggingface.co/docs/transformers/v4.29.1/en/main_classes/trainer%23transformers.Seq2SeqTrainer" class=" wrap external" target="_blank" rel="nofollow noreferrer">Seq2SeqTrainer</a>和<a href="http://link.zhihu.com/?target=https%3A//huggingface.co/docs/transformers/v4.29.1/en/main_classes/trainer%23transformers.Seq2SeqTrainingArguments" class=" wrap external" target="_blank" rel="nofollow noreferrer">Seq2SeqTrainingArguments</a>类。</blockquote> <p data-pid="Rcn8deMI"><a href="http://link.zhihu.com/?target=https%3A//huggingface.co/docs/transformers/v4.29.1/en/main_classes/trainer%23transformers.Trainer" class=" wrap external" target="_blank" rel="nofollow noreferrer">您可以通过子类化Trainer</a>中的方法来自定义训练循环行为。这使您可以自定义损失函数、优化器和调度程序等功能。查看<a href="http://link.zhihu.com/?target=https%3A//huggingface.co/docs/transformers/v4.29.1/en/main_classes/trainer%23transformers.Trainer" class=" wrap external" target="_blank" rel="nofollow noreferrer">Trainer</a>参考，了解哪些方法可以被子类化。</p> <p data-pid="-GrWiPyM">自定义训练循环的另一种方法是使用<a href="http://link.zhihu.com/?target=https%3A//huggingface.co/docs/transformers/v4.29.1/en/main_classes/callbacks" class=" wrap external" target="_blank" rel="nofollow noreferrer">回调</a>。您可以使用回调与其他库集成并检查训练循环以报告进度或提前停止训练。回调不会修改训练循环本身的任何内容。要自定义损失函数之类的东西，您需要改为对<a href="http://link.zhihu.com/?target=https%3A//huggingface.co/docs/transformers/v4.29.1/en/main_classes/trainer%23transformers.Trainer" class=" wrap external" target="_blank" rel="nofollow noreferrer">Trainer</a>进行子类化。</p> <h3>Fine-tune预训练模型</h3> <div class="highlight"><pre><code class="language-python"><span></span><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizerf</span><span class="p">,</span> <span class="n">AutoModelForSequenceClassification</span><span class="p">,</span> <span class="n">TrainingArguments</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">evaluate</span>

<span class="c1"># --- Prepare dataset ---</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">"yelp_review_full"</span><span class="p">)</span>
<span class="n">dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">][</span><span class="mi">100</span><span class="p">]</span>
<span class="c1"># {'label': 0,</span>
<span class="c1">#  'text': 'My expectations for McDonalds are t rarely high. But for one to still fail so spectacularly...that takes something special!\\nThe cashier took my friends\'s order, then promptly ignored me. I had to force myself in front of a cashier who opened his register to wait on the person BEHIND me. I waited over five minutes for a gigantic order that included precisely one kid\'s meal. After watching two people who ordered after me be handed their food, I asked where mine was. The manager started yelling at the cashiers for \\"serving off their orders\\" when they didn\'t have their food. But neither cashier was anywhere near those controls, and the manager was the one serving food to customers and clearing the boards.\\nThe manager was rude when giving me my order. She didn\'t make sure that I had everything ON MY RECEIPT, and never even had the decency to apologize that I felt I was getting poor service.\\nI\'ve eaten at various McDonalds restaurants for over 30 years. I\'ve worked at more than one location. I expect bad days, bad moods, and the occasional mistake. But I have yet to have a decent experience at this store. It will remain a place I avoid unless someone in my party needs to avoid illness from low blood sugar. Perhaps I should go back to the racially biased service of Steak n Shake instead!'}</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"bert-base-cased"</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">tokenize_function</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">examples</span><span class="p">[</span><span class="s2">"text"</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="s2">"max_length"</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">tokenized_datasets</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># --- Train ---</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"bert-base-cased"</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span><span class="n">output_dir</span><span class="o">=</span><span class="s2">"test_trainer"</span><span class="p">)</span>

<span class="n">metric</span> <span class="o">=</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">"accuracy"</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">compute_metrics</span><span class="p">(</span><span class="n">eval_pred</span><span class="p">):</span>
    <span class="n">logits</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">eval_pred</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">metric</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="n">references</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
  
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">args</span><span class="o">=</span><span class="n">training_args</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">small_train_dataset</span><span class="p">,</span>
    <span class="n">eval_dataset</span><span class="o">=</span><span class="n">small_eval_dataset</span><span class="p">,</span>
    <span class="n">compute_metrics</span><span class="o">=</span><span class="n">compute_metrics</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</code></pre></div> <blockquote data-pid="8Xd-grdJ">本文使用 <a href="https://zhuanlan.zhihu.com/p/106057556" class="internal" rel="external nofollow noopener" target="_blank">Zhihu On VSCode</a> 创作并发布</blockquote> <p class="post-meta"> 1 min read   ·   May 26, 2023   ·   Rofunc (Zhihu) </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="https://zhuanlan.zhihu.com/p/630472611" target="_blank" rel="external nofollow noopener">从零开始认识大模型：一篇非从业者指南</a> <svg width="2rem" height="2rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </h3> <p></p> <p data-pid="Y3h7fUTT">【LM 00】从零开始认识大模型：一篇非从业者指南</p> <blockquote data-pid="hREbVEpP">什么是ChatGPT？什么是GPT-4？Bard和PaLM又是什么？它们有什么关系？它们对我们有什么影响？</blockquote> <p data-pid="dGtvHMav">AI技术第一次这么密集地冲击着非从业者们，身处变革之中的我们或许会迷茫，但这本来就是工业革命之后的常态，我们要做的就是像九十年代学习计算机一样拥抱未来。</p> <p data-pid="nebQzgAW">在ChatGPT刚出的时候，我对未来的知识创作是抱着悲观态度的。我认为个人创作者会被ChatGPT挤兑，或者直接投降借助ChatGPT来创作。久而久之，知识会陷入到封闭的循环当中，知乎也会沦为一个ChatGPT版公众号的聚集地，所以有一段时间我卸载了知乎。但是不久之后，我就发现自己这样的想法就和1900年的保皇派一样，看似是看清了变革的方向，实际却是裹足不前。</p> <p data-pid="Wo4zmNBy">对于一个新事物，体系性的学习是理解和掌握它的必要途径，也是最便捷的方式。如果只是试图从公众号庞杂的文章中了解，无异于管中窥豹。</p> <h2>大模型是什么</h2> <p data-pid="-FamsNwD">首先，<b>大模型</b>这个词是建立在<b>神经网络</b>模型上的。</p> <p data-pid="mwNFZHHr"><b>神经网络</b>是一种在过去二十年得到广泛研究的人工智能方法，我们试图使用这样的模型模拟人类的思维和决策。<b>神经网络</b>的本质是在每个<b>人工神经元</b>（图1a的圆形）上设定一些可以调节的<b>参数</b>，并能够根据这些参数对输入到该神经元的数据进行<b>加权求和 </b><img src="https://www.zhihu.com/equation?tex=%5CSigma" alt="\Sigma" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer"> 和<b>非线性变换 </b><img src="https://www.zhihu.com/equation?tex=f" alt="f" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer"> ，使其能够借助<b>学习/优化算法</b>实现从输入值 <img src="https://www.zhihu.com/equation?tex=x" alt="x" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer"> 到输出值 <img src="https://www.zhihu.com/equation?tex=y" alt="y" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer"> 的映射 <img src="https://www.zhihu.com/equation?tex=y%3DF%28x%29" alt="y=F(x)" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer">。</p> <p data-pid="5u4xQixG">这个描述映射关系的函数 <img src="https://www.zhihu.com/equation?tex=F" alt="F" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer"> 是图1a中众多小 <img src="https://www.zhihu.com/equation?tex=f" alt="f" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer"> 们和加权求和函数叠加作用的结果，它在早些年（21世纪初）还是一门玄学，但是之后就有很多针对神经网络的可解释性工作，试图揭开这背后的隐秘。</p> <figure data-size="normal"><img src="https://pica.zhimg.com/v2-e62a5496354362e9926f92743254f68f_720w.jpg?source=d16d100b" data-size="normal" data-rawwidth="720" data-rawheight="241" class="origin_image zh-lightbox-thumb" width="720" data-original="https://pic1.zhimg.com/v2-e62a5496354362e9926f92743254f68f_720w.jpg?source=d16d100b" style="max-width: 100%;" referrerpolicy="no-referrer"><figcaption>图1 神经网络</figcaption></figure><p data-pid="j5zOs3w7">这种映射关系的作用十分广泛，你可以用特定的数据（一些提前制作好的输入-输出对）作为它的<b>训练数据</b>，用<b>学习算法</b>来调节网络<b>参数</b>，使其实现类似于<b>训练数据</b>的映射关系。广义上来说，给它什么样的训练数据，就能通过神经网络建立什么样的映射关系。例如，</p> <ul> <li data-pid="FFGSp2lY">动物的图片 -- 动物的类别</li> <li data-pid="cPRx4i29">中文 -- 对应的英文翻译</li> <li data-pid="rnZHTeUW">语音 -- 对应的文字内容</li> <li data-pid="gjHRGZny">视频 -- 视频中的事件描述</li> <li data-pid="YCSrLkKR">机器人的视觉反馈 -- 机器人下一步的动作</li> </ul> <p data-pid="uJeebSRI">听起来是十分美好的，仿佛我们可以借助神经网络构建万物之间的联系、帮助我们更好地理解世界。</p> <p data-pid="Htv1eN5H">但是这其实并不容易，映射关系建立的好与坏通常受到以下几种条件的影响：</p> <ul> <li data-pid="sNoLzYvz">模型架构</li> <li data-pid="UWpIjU9R">训练数据的数量</li> <li data-pid="mCeN7puB">训练数据的质量</li> <li data-pid="U0v0VnWb">学习算法</li> <li data-pid="J24idtHv">计算硬件算力</li> </ul> <p data-pid="6B2CsnJd">训练数据的数量和质量在这二十年间已经得到了长足的发展，很多TB级别的数据集被开源出来作为模型的测试基准。这不得不感谢东南亚及印度的廉价数据标注工人，在资本温柔的剥削下，他们做着这个世纪最伟大的事情。</p> <p data-pid="_iIeaP9W">有了大量的数据以及英伟达一年翻一倍的硬件算力，学术界更聚焦在如何设计出一个<b>美妙的模型架构</b>。我们找到了很多好用的基础模型，也设计过很多眼花缭乱的网络架构，但最后发现还是大力出奇迹---好的基础模型的大量堆叠就能出现意想不到的效果。</p> <p data-pid="BpivjQSK">图1b展示了一个大一点的神经网络，它会被叫做<b>深度神经网络</b>，但还不值得被称为<b>大模型</b>。</p> <p data-pid="cMLq2sBP"><b>大模型</b>的模型参数量达到了亿的级别，近期的研究成果基本在百亿到千亿的范围。当然这不会是终点，大模型的神经元数量和相关参数量必定会朝着远超人类大脑神经元数量的方向发展。这既体现了现有人工神经网络方法的低效性（当然是比不过地球亿万年的进化），也彰显着人类的雄心。</p> <p data-pid="u2zTIG8e">相信至此，你已经对大模型中的<b>大</b>和<b>模型</b>都有了一定的了解，那么我们就可以回答一开始的问题了。</p> <ol> <li data-pid="tozf7_m8">ChatGPT是美国著名AI研究所/公司---OpenAI的一款AI在线问答产品。之所以能够火出圈，是因为实在效果太震撼了。你可以以一种全新的、交互式的方式获取之前需要搜索引擎查找的信息。同时，这些信息是由模型生成出来的，也就意味着它们在这之前是并不完全地存在于互联网（以及人类知识库）中的。它基于的技术就是一种叫做 Generative Pre-trained Transformer (GPT, 生成式预训练Transformer) 的模型。以上面的方式简单来讲，<b>ChatGPT以一种更加玄学的方式构建了你的问题和你可接受的答案之间的映射</b>。</li> <li data-pid="sa0JHyf0">很明显，GPT-4就代表了GPT这种模型的第四版（更大地参数量、更多的模型优化）。</li> <li data-pid="e4nvWtG8">Bard呢，是谷歌公司的竞品，也是在线问答，可以做表格也可以写文案，总体来说略逊于ChatGPT。它背后的技术也在不断地迭代，从LaMDA到最近刚发布的PaLM2。</li> </ol> <a href="https://www.zhihu.com/question/600313536/answer/3025052708" data-draft-node="block" data-draft-type="link-card" data-image="https://picx.zhimg.com/v2-05dc23a6450c1aca6dc94891c7c5d828_qhd.jpg?source=d16d100b" data-image-width="1168" data-image-height="905" class="internal" rel="external nofollow noopener" target="_blank">最强语言模型 PaLM2 亮相，Bard 能力跃升，它可以实现哪些功能？算是 ChatGPT 杀手吗？</a><h2>大模型的优势</h2> <p data-pid="Dx8MeNtj">那么为什么模型越大，构建映射的能力就越强了呢？</p> <p data-pid="umZb7BcC">尽管这看起来是很理所当然的事情，也可以从生物演化的角度来解释，但是总归是一种无法描述的玄学感。近期的一些工作将模型大的优势解释为大模型所带来的涌现能力和思维链构建能力。</p> <h3>涌现</h3> <p data-pid="-rLK8cWS"><b>涌现</b>还确实是个生物演化的概念。它描述的是一种现象，即<b>复杂系统具有某些组成它的小系统所不具备的特性</b>。</p> <p><br></p> <figure data-size="normal"><img src="https://pic1.zhimg.com/v2-d6c5408e2dbc9675f68adb9dec4dc3fb_720w.jpg?source=d16d100b" data-size="normal" data-rawwidth="720" data-rawheight="480" class="origin_image zh-lightbox-thumb" width="720" data-original="https://pic1.zhimg.com/v2-d6c5408e2dbc9675f68adb9dec4dc3fb_720w.jpg?source=d16d100b" style="max-width: 100%;" referrerpolicy="no-referrer"><figcaption>图2 图源：http://slide.tech.sina.com.cn/d/slide_5_453_74691.html#p=1</figcaption></figure><p><br></p> <p data-pid="Bi_o15P1">这种现象在自然界比较常见，例如，水分子在构建成雪花时普遍呈现出的六角型状态，生物能够由基本粒子构成却具备高级生物功能的神奇现象，以及生物群体能够自发形成秩序社会的复杂行为。这些都暗示着大模型的成功背后具备着某种自然界的神秘力量，一种我们暂时还没有能力解析的混沌系统。但我相信，随着大模型的发展，这种解析并不遥远。</p> <h3>思维链</h3> <p data-pid="qkW-am0w">大模型涌现能力的直接结果就是能够构建出<b>思维链</b>，这是一种推理能力。</p> <p data-pid="5sXULZ0O">下图就是思维链这篇论文给出的例子</p> <p><br></p> <figure data-size="normal"><img src="https://pic1.zhimg.com/v2-792346e71f690906803b94f5a22e2171_720w.jpg?source=d16d100b" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"><figcaption>图3 图源：Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</figcaption></figure><p><br></p> <p data-pid="pnr5AUMK">如果只是单纯的文本生成，大模型在简单的加减法上都会有很大的问题，因为这里隐含着逻辑推理的能力，并非单纯的文本。但是如果我们同时将问题的解题思路描述给大模型，就会得到有理有据的正确结论。</p> <p data-pid="m_libqvU">通过这样的方式训练的大模型，就<b>具备了将一个问题拆解成许多子问题的能力</b>，从而提升了答案的可靠性。这种能力将在机器人操作中大放异彩，会成为促进机器人应用落地的重要一环。我之后会专门就<b>机器人大模型</b>和<b>决策大模型</b>的研究现状与光明前景写一篇文章，<b>感兴趣的小伙伴可以关注我的知乎哦</b>。</p> <p data-pid="QmbfgJ5g"><b>参考文献</b></p> <ol> <li data-pid="xcqa4Uy6"><a href="http://link.zhihu.com/?target=https%3A//arxiv.org/abs/2206.07682" class=" wrap external" target="_blank" rel="nofollow noreferrer">Emergent Abilities of Large Language Models</a></li> <li data-pid="_7BtggFD"><a href="http://link.zhihu.com/?target=https%3A//arxiv.org/abs/2201.11903" class=" wrap external" target="_blank" rel="nofollow noreferrer">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</a></li> </ol> <h2>大语言模型(LLM)族谱</h2> <p data-pid="z-t3ZDSg">上文一直在使用大模型一词，实际上大模型已在短短四年间发展壮大成为一个人工智能的主流研究方向。大模型的发展最初是伴随着自然语言处理技术的不断发展的，这是由于文本数据的数据量更大且更容易获取。所以目前大模型最大的分类还是大语言模型，近两年衍生出一些语言与其他形式融合的大模型，例如，文字生成音乐（<a href="http://link.zhihu.com/?target=https%3A//google-research.github.io/seanet/musiclm/examples/" class=" wrap external" target="_blank" rel="nofollow noreferrer">MusicLM</a>）、文字生成图像（<a href="http://link.zhihu.com/?target=https%3A//openai.com/product/dall-e-2" class=" wrap external" target="_blank" rel="nofollow noreferrer">DALL-E2</a>, <a href="http://link.zhihu.com/?target=https%3A//www.midjourney.com/home/%3FcallbackUrl%3D%252Fapp%252F" class=" wrap external" target="_blank" rel="nofollow noreferrer">Midjourney</a>）、文字图像生成机器人动作（<a href="http://link.zhihu.com/?target=https%3A//robotics-transformer.github.io/" class=" wrap external" target="_blank" rel="nofollow noreferrer">RT-1</a>）等。</p> <p data-pid="ueVTL6sn">大模型包括但不限于以下几类：</p> <ul> <li data-pid="cKUw6Evq">大语言模型</li> <li data-pid="N-qfQ-ac">视觉大模型</li> <li data-pid="Y12Df7gI">多模态大模型</li> <li data-pid="pnLFltlU">决策大模型</li> <li data-pid="oDjecUk7">机器人大模型</li> </ul> <p data-pid="t8wAAaF4">下图是我绘制的大语言模型从2019-2023的发展路径。</p> <p data-pid="RoIGgkXl">目前大多数的大语言模型都是建立在一个叫做 <b>Transformer</b> 的基础模型之上的（下一篇会介绍）。而根据使用的 Transformer 的方式不同，LLM的构建就被分为三条研究路径：编码器-解码器结构、只使用解码器、只使用编码器。绿色标注的是开源可用的模型，部分模型后有相应的参数量（以B--十亿为单位）。</p> <figure data-size="normal"><img src="https://pic1.zhimg.com/v2-6510d73a14d8df6fb282ffc56929b3b2_720w.jpg?source=d16d100b" data-size="normal" data-rawwidth="3742" data-rawheight="3541" class="origin_image zh-lightbox-thumb" width="3742" data-original="https://pic1.zhimg.com/v2-6510d73a14d8df6fb282ffc56929b3b2_720w.jpg?source=d16d100b" style="max-width: 100%;" referrerpolicy="no-referrer"><figcaption>大语言模型（实时更新）</figcaption></figure><p data-pid="OtoG4J7K">上图在实时更新中。</p> <p data-pid="pJfry3jK"><b>参考文献</b></p> <ol> <li data-pid="CTDMMuNa"><a href="http://link.zhihu.com/?target=http%3A//arxiv.org/abs/2304.13712" class=" wrap external" target="_blank" rel="nofollow noreferrer">Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond</a></li> <li data-pid="9ske9ihY"><a href="http://link.zhihu.com/?target=https%3A//arxiv.org/abs/2211.09110" class=" wrap external" target="_blank" rel="nofollow noreferrer">Holistic Evaluation of Language Models</a></li> </ol> <h2>多模态大模型(MLM)族谱</h2> <p data-pid="evEHJ_xR">绿色为开源模型。部分基础模型例如ViT和MAE等并非视觉语言模型，而是单纯的视觉模型，这里列出是出于其重要性的考虑。</p> <figure data-size="normal"><img src="https://pic1.zhimg.com/v2-d4223b0aba574dae703a0f0e35b06e3a_720w.jpg?source=d16d100b" data-size="normal" data-rawwidth="4096" data-rawheight="3370" class="origin_image zh-lightbox-thumb" width="4096" data-original="https://picx.zhimg.com/v2-d4223b0aba574dae703a0f0e35b06e3a_720w.jpg?source=d16d100b" style="max-width: 100%;" referrerpolicy="no-referrer"><figcaption>多模态大模型（实时更新）</figcaption></figure><p data-pid="Ei7r7vNf">上图在实时更新中。</p> <h2>如何与大模型共生</h2> <p data-pid="VRe2mGaW"><b>我们有一个无法摆脱的命运，那就是和技术共生。</b>田园牧歌的生活只会逐渐成为奢望，我们在不断被迫接受着过量的信息和超出认知的技术革新，否则就会处于被革新的尴尬境地。那么大模型也一样，我们要寻求共生之道。我在这个回答中给出了一个理想主义的答案，即在大模型等技术的支撑下，我们能够从劳作中解脱出来，可以有更多的精力探寻人存在的价值。</p> <a href="https://www.zhihu.com/question/302824343/answer/2965475934" data-draft-node="block" data-draft-type="link-card" class="internal" rel="external nofollow noopener" target="_blank">怎么看待人工智能与人类的关系？</a><p data-pid="QMnXXXMH">但这终究是个过分理想的叙事角度，不难想象，在不久的将来，我们会在高频迭代的AI技术的逼促下，更辛苦地劳作以赶上时代的发展（毕竟我就在加班看大模型论文），或者彻底屈服于其下做一个便宜稳定的数据源。所以看起来，路子只有一条，了解它并学会使用它，就像学会使用电脑为我们创造价值一样。</p> <p data-pid="6151NnZt">现在还不晚，还来得及拥抱未来。</p> <blockquote data-pid="4tR73k4x">本文使用 <a href="https://zhuanlan.zhihu.com/p/106057556" class="internal" rel="external nofollow noopener" target="_blank">Zhihu On VSCode</a> 创作并发布</blockquote> <p class="post-meta"> 1 min read   ·   May 18, 2023   ·   Rofunc (Zhihu) </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="https://zhuanlan.zhihu.com/p/615104963" target="_blank" rel="external nofollow noopener">ChatGPT版Siri: 基于ChatGPT和ROS的机器人语音问答</a> <svg width="2rem" height="2rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </h3> <p></p> <blockquote data-pid="ZaCpqHxQ">ChatGPT 的出现让很多有趣的想法成为了可能。我最近一直在思考 LLM 及其相关技术与 Robot Manipulation 相结合的可能，也尝试着做了一些工作，效果还是可以的。如果能够 accept，再给大家分享一下。</blockquote> <p data-pid="W-nnTi8W">今天主要介绍一个有趣的开源小组件，即<b>利用 ChatGPT API 以及 AWS 的语音 API，在 ROS 上实现机器人与人类之间的交互式语音问答</b>。开源代码库如下，详细的 API 获取和安装方式已经写在其中了，详见 <a href="http://link.zhihu.com/?target=https%3A//github.com/Skylark0924/Rofunc-ros/blob/main/docs/Installation.md" class=" wrap external" target="_blank" rel="nofollow noreferrer">Installation Manual</a></p> <a data-draft-node="block" data-draft-type="link-card" href="http://link.zhihu.com/?target=https%3A//github.com/Skylark0924/Rofunc-ros" class=" external" target="_blank" rel="nofollow noreferrer"><span class="invisible">https://</span><span class="visible">github.com/Skylark0924/</span><span class="invisible">Rofunc-ros</span><span class="ellipsis"></span></a><p data-pid="qt9j5M3L">具体来说，这个功能由 <code>Speech2text</code>, <code>Chat</code> 和<code>Text2speech</code> 三部分组成。给大家画个简单的图</p> <figure data-size="normal"><img src="https://picx.zhimg.com/v2-0e54d85113f7746db1d82b5fcd7263dd_720w.jpg?source=d16d100b" data-rawwidth="4173" data-rawheight="1200" data-size="normal" data-caption="" class="origin_image zh-lightbox-thumb" width="4173" data-original="https://pic1.zhimg.com/v2-0e54d85113f7746db1d82b5fcd7263dd_720w.jpg?source=d16d100b" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p><br></p> <h2>Voice Q&amp;A 使用方式</h2> <p data-pid="gTpgg9jF">得益于ROS天然的多线程结构，我们可以以一行代码开启这个功能，并让各个模块持续长时间地保持监听。</p> <p data-pid="KujpATgc"><b>English</b></p> <div class="highlight"><pre><code class="language-text"><span></span>roslaunch rofunc_ros voice_qa_en.launch
</code></pre></div> <p data-pid="AnC3N1KF"><b>Chinese</b></p> <div class="highlight"><pre><code class="language-text"><span></span>roslaunch rofunc_ros voice_qa_cn.launch
</code></pre></div> <h2>子模块使用方式</h2> <h3>Speech2text</h3> <p data-pid="VA-SiVXg"><b>English</b></p> <div class="highlight"><pre><code class="language-text"><span></span>roslaunch rofunc_ros speech2text_en.launch
</code></pre></div> <p data-pid="AdvcoS0N"><b>Chinese</b></p> <div class="highlight"><pre><code class="language-text"><span></span>roslaunch rofunc_ros speech2text_cn.launch
</code></pre></div> <h3>Chat</h3> <p data-pid="o_WZ1XVK">Open a terminal to run the launch file</p> <div class="highlight"><pre><code class="language-text"><span></span>roslaunch rofunc_ros start.launch
</code></pre></div> <p data-pid="l0R8NOiM">Open another terminal to feed the question</p> <div class="highlight"><pre><code class="language-text"><span></span>rosrun rofunc_ros str_pub.py "Hello"
</code></pre></div> <h3>Text2speech</h3> <p data-pid="4ju4dtXo">Open a terminal to run the launch file</p> <div class="highlight"><pre><code class="language-text"><span></span>roslaunch rofunc_ros text2speech.launch
</code></pre></div> <p data-pid="N43V0yjo">Open another terminal to give the text</p> <div class="highlight"><pre><code class="language-text"><span></span> rosrun rofunc_ros voicer.py 'Hello everyone! I am CURI, a humanoid robot designed by \
       collaborative and versatile robots laboratory. Our lab focuses on the co-evolutionary\
       development of human-centered robotics and AI technologies for advanced robots, such as \
       human-like mobile manipulators, humanoid robots, to perform autonomous, assistive and \
       collaborative tasks by learning and transferring the skills from humans.'
</code></pre></div> <p><br></p> <p data-pid="YyHSNT1L">希望能为大家的机器人研究增加一些乐趣 </p> <p class="post-meta"> 1 min read   ·   March 18, 2023   ·   Rofunc (Zhihu) </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="https://zhuanlan.zhihu.com/p/585106491" target="_blank" rel="external nofollow noopener">Rofunc：迈向高冗余人型机器人的多模态模仿学习</a> <svg width="2rem" height="2rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </h3> <p></p> <blockquote data-pid="G0xww-ta">读博一周年纪念专栏</blockquote> <figure data-size="normal"><img src="https://picx.zhimg.com/v2-06bce73d0a2553ab162b6d600c6b33f0_720w.jpg?source=d16d100b" data-rawwidth="836" data-rawheight="380" data-size="normal" data-caption="" class="origin_image zh-lightbox-thumb" width="836" data-original="https://picx.zhimg.com/v2-06bce73d0a2553ab162b6d600c6b33f0_720w.jpg?source=d16d100b" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p data-pid="3Vu88nU0">把 <code>Rofunc</code> 的 <code>Github Repository</code> 放个显眼的位置</p> <a data-draft-node="block" data-draft-type="link-card" href="http://link.zhihu.com/?target=https%3A//github.com/Skylark0924/Rofunc" class=" wrap external" target="_blank" rel="nofollow noreferrer">Skylark0924/Rofunc - Github</a><p data-pid="V41c5-h8">欢迎 <b>Star</b>，欢迎 <b>Issue</b>，欢迎 <b>Contributor</b>，欢迎一切志同道合的小伙伴！！！</p> <hr> <h2>序</h2> <p data-pid="E86BHr8y">早就想开新专栏了，一直拖延到一周年零几个月。。。截至目前，旧的 RL in Robotics 专栏已累计98篇内容、5728个赞同。感谢各位对当年那个在迷雾中肆意探索的我给予的支持与鼓励，让我们缅怀一下这个即将完结的专栏：</p> <a data-draft-node="block" data-draft-type="link-card" href="https://www.zhihu.com/column/c_1188392852261134336" data-image="https://pic1.zhimg.com/v2-efdeaf1dedf54009a1b318663acb457c_l.jpg?source=d16d100b" data-image-width="400" data-image-height="400" class="internal" rel="external nofollow noopener" target="_blank">RL in Robotics</a><p data-pid="Zogj7H8p">新专栏会更关注于机器人本身，尤其是更具挑战性的高冗余人型/类人型机器人的操作和行为，而不会像之前一样沉迷于某种听起来仿佛很厉害、但可能没大用的新技术。这可能也是我读博一年来的心得体会吧，总算是学会了从问题出发，而不是拿着锤子找钉子。这里就要感谢一下我导陈翡老师啦，以及一直为我提供硬核技术指导的 Sylvain Calinon 老师。</p> <p data-pid="3TfiiC1C">还有一点与之前不同的是，这次的专栏其实也是我们正在推进的开源机器人模仿学习库 <code>Rofunc</code> 的中文教程。在科研的过程中分心搞工具包开发的初衷也很简单。在深度学习领域，我们有 <code>torch</code>；在图学习领域，我们有 <code>DGL</code>；在强化学习领域，我们有 <code>RLlib</code>。为什么机器人领域就不能拥有这种易用的、普适的，且能够一览机器人任务从数据到部署全流程的工具包呢？</p> <p data-pid="szGaeFUM">技术的本质就应该是解蔽，而不是打着科研招牌的故弄玄虚。只有机器人门槛放低，才会吸引更多志同道合的朋友投入到机器人社区的建设中，才会最终迎来那姗姗来迟的机器人革命。那么，先上 <code>Rofunc</code> 的 <code>Github Repository</code> 吧！</p> <a data-draft-node="block" data-draft-type="link-card" href="http://link.zhihu.com/?target=https%3A//github.com/Skylark0924/Rofunc" class=" wrap external" target="_blank" rel="nofollow noreferrer">Skylark0924/Rofunc - Github</a><p data-pid="G0J5BeGw">欢迎 <b>Star</b>，欢迎 <b>Issue</b>，欢迎 <b>Contributor</b>，欢迎一切志同道合的小伙伴！！！</p> <p><br></p> <figure data-size="normal"><img src="https://picx.zhimg.com/v2-3a01193cedef7d4296e32e14f834c3f3_720w.jpg?source=d16d100b" data-size="normal" data-caption="" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><h2>概览</h2> <p data-pid="meEqGCXL">机器人的模仿学习/示教学习并不是一个很新的话题，甚至读博这一年我也接触到了欧洲和美国两种不同的思想流派。像 Billard 教授和 Calinon 老师这样的欧洲研究者坚守在拖动示教以及由此衍生基于概率和流形的示教学习方法（可以参考一下 <a href="https://www.zhihu.com/question/265564182/answer/309412387" class="internal" rel="external nofollow noopener" target="_blank">李淼老师的回答</a>）。而美国呢，貌似更喜欢基于视频示教直接使用DL、RL等 NN-based 方法。这两种思路是各有优势，也是各有缺点的，优劣是需要根据具体任务来评判的。我们想做的，就是在 <code>Rofunc</code> 中同时引入这两种思路，为 peer researchers 提供一个模仿学习的 baseline 甚至是 benchmark package。</p> <p data-pid="_kVaQGpS">除了 learning 模块外，我们还有更大的野心：<b>提供从多模态数据采集与处理、示教学习到机器人规划与控制、以及具有多种类人机器人仿真器的全流程示教学习 pipeline。</b> 对于高冗余度的类人型机器人来说，从人类演示中学习 (learning from human demonstration) 是获取新的、复杂的技能最自然且便捷的解决方案。我们提供了多视角视觉（ZED camera）、人类运动学（Xsens MTw Awinda）、物体运动学（Optitrack）以及人体生物力学（Delsys sEMG）的多传感器、多模态采集与处理方案，提供了基于优化的机器人规划与控制方法，也同样提供了基于 Isaac Gym 的多种机器人（Franka, CURI, Ubtech Walker, Diablo等）仿真器。</p> <figure data-size="normal"><img src="https://pic1.zhimg.com/v2-00905175c7f71fc54c0f6d34dabbb476_720w.jpg?source=d16d100b" data-rawwidth="3040" data-rawheight="1062" data-size="normal" data-caption="" class="origin_image zh-lightbox-thumb" width="3040" data-original="https://picx.zhimg.com/v2-00905175c7f71fc54c0f6d34dabbb476_720w.jpg?source=d16d100b" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><h2>目录</h2> <blockquote data-pid="IOX5LZ7n">持续更新中</blockquote> <p><br></p> <blockquote data-pid="-qajmpI3">本文使用 <a href="https://zhuanlan.zhihu.com/p/106057556" class="internal" rel="external nofollow noopener" target="_blank">Zhihu On VSCode</a> 创作并发布</blockquote> <p class="post-meta"> 1 min read   ·   November 20, 2022   ·   RL in Robotics (Zhihu) </p> <p class="post-tags"> <a href="/blog/2022"> <i class="fa-solid fa-calendar fa-sm"></i> 2022 </a> </p> </li> </ul> <nav aria-label="Blog page naviation"> <ul class="pagination pagination-lg justify-content-center"> <li class="page-item disabled"> <a class="page-link" href="" tabindex="-1" aria-disabled="">Newer</a> </li> <li class="page-item active"> <a class="page-link" href="/blog/index.html" title="Blog">1</a> </li> <li class="page-item "> <a class="page-link" href="/blog/page/2/index.html" title="Blog - page 2">2</a> </li> <li class="page-item "> <a class="page-link" href="/blog/page/3/index.html" title="Blog - page 3">3</a> </li> <li class="page-item "> <a class="page-link" href="/blog/page/4/index.html" title="Blog - page 4">4</a> </li> <li class="page-item "> <a class="page-link" href="/blog/page/5/index.html" title="Blog - page 5">5</a> </li> <li class="page-item "> <a class="page-link" href="/blog/page/2/">Older</a> </li> </ul> </nav> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Junjia LIU. Last updated: March 30, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?5d75c11f89cd96294bf5e6dd1ee1bb30"></script> <script defer src="/assets/js/common.js?fcfacfb8c6281f5e68d5a7d348186eb1"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>