<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Blog - page 5 | Junjia LIU </title> <meta name="author" content="Junjia LIU"> <meta name="description" content=""> <meta name="keywords" content="Robotics, Embodied AI, Humanoid Robots"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://skylark0924.github.io/blog/page/5/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Junjia LIU </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <div class="header-bar"> <h1>Junjia LIU's blog</h1> <h2></h2> </div> <div class="tag-category-list"> <ul class="p-0 m-0"> <li> <i class="fa-solid fa-hashtag fa-sm"></i> <a href="/blog/tag/formatting">formatting</a> </li> <p>•</p> <li> <i class="fa-solid fa-hashtag fa-sm"></i> <a href="/blog/tag/images">images</a> </li> <p>•</p> <li> <i class="fa-solid fa-hashtag fa-sm"></i> <a href="/blog/tag/links">links</a> </li> <p>•</p> <li> <i class="fa-solid fa-hashtag fa-sm"></i> <a href="/blog/tag/math">math</a> </li> <p>•</p> <li> <i class="fa-solid fa-hashtag fa-sm"></i> <a href="/blog/tag/code">code</a> </li> <p>•</p> <li> <i class="fa-solid fa-tag fa-sm"></i> <a href="/blog/category/blockquotes">blockquotes</a> </li> </ul> </div> <ul class="post-list"> <li> <h3> <a class="post-title" href="https://zhuanlan.zhihu.com/p/359620737" target="_blank" rel="external nofollow noopener">PR Efficient Ⅱ：Bayesian Transfer RL with prior knowledge</a> <svg width="2rem" height="2rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </h3> <p></p> <blockquote data-pid="uvbm-wHD"> <p data-pid="Sf8Wj2fQ">本文使用 <a href="https://zhuanlan.zhihu.com/p/106057556" class="internal" rel="external nofollow noopener" target="_blank">Zhihu On VSCode</a> 创作并发布</p> <br><p data-pid="wPPATbrF"><strong>总第100篇文章纪念</strong></p> <p data-pid="QuvS5cqq"><strong>每天一篇 Efficient，离 robot learning 落地更进一步。</strong></p> </blockquote> <p></p> <figure data-size="normal"><img src="https://pica.zhimg.com/v2-9a118cb6bd1335ec9269ba18a5299073_720w.jpg?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p></p> <h2>Bayesian Transfer RL</h2> <p data-pid="VFbcib-5">本文将 off-policy RL 中的 behavior policy 定义为一个 Bayesian posterior distribution，用来结合task-specific的先验。期望以这种方式实现 transfer learning 以及 meta-learning。基于当前Q函数估计的 behaviour policy 通常采用 softmax 或Boltzmann形式：</p> <p></p> <figure data-size="normal"><img src="https://pica.zhimg.com/v2-9d9417040f1b04af1cb785731b5b5745_720w.jpg?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p></p> <p data-pid="kl_6-vt3">这个策略只用到了 task-specific information, 例如任务的 state-action value function，我们可以这样将先验结合进来：</p> <p></p> <figure data-size="normal"><img src="https://picx.zhimg.com/v2-95d118a3a976b4d012a9d29cd0d6e90c_720w.jpg?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p></p> <p data-pid="6lX93i7w">这里的 <img src="https://www.zhihu.com/equation?tex=f%28%5Calpha%3B%5Cmathcal%7BM%7D_t%29%5Cge+0" alt="f(\alpha;\mathcal{M}_t)\ge 0" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer">是个非负的函数，且是一个非归一的概率分布，当动作 <img src="https://www.zhihu.com/equation?tex=%5Calpha_t" alt="\alpha_t" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer"> 符合先验时返回 high value，反之 small value。<img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BM%7D_t" alt="\mathcal{M}_t" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer"> 表示表达先验知识并计算 <img src="https://www.zhihu.com/equation?tex=f%28%5Ccdot%29" alt="f(\cdot)" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer"> 值所需的所有存储信息。这里的超参数 <img src="https://www.zhihu.com/equation?tex=%5Cbeta" alt="\beta" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer"> 也起到了平衡 task-specific information 与先验的作用。</p> <p data-pid="VYMZq75e">显然，这是避免了机器人对无关紧要的部分进行重复探索，并且可以算是一种实现 sample-efficient 的方式。</p> <blockquote data-pid="6P5ko3uP"><p data-pid="F_I2rLud">这里原文说的是：It is <strong>the only way</strong> to achieve sample efficient learning in very large or continuous state spaces. 太过确定，不敢苟同。</p></blockquote> <p data-pid="cU9U28mO">除了可以用于 off-policy 算法之外，本文还给出了 on-policy 算法结合的方式，实际上和 of-policy 没什么区别：</p> <p></p> <figure data-size="normal"><img src="https://picx.zhimg.com/v2-bb1646b02e2591aa28edeee45457a771_720w.jpg?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p></p> <p data-pid="6Qc62Orc"><strong>先验函数 <img src="https://www.zhihu.com/equation?tex=f%28%5Calpha%3B%5Cmathcal%7BM%7D_t%29" alt="f(\alpha;\mathcal{M}_t)" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer"> 该怎么定义？</strong></p> <p data-pid="E8-_l1Q0">本文给出了先验思想是：In a roughly deterministic and stationary world, past plans that resulted in no progress for solving a task need to be tried out less frequently in the future Humans.</p> <p data-pid="f2lLqfsS">实际上就是减少 useless re-exploration。所以文中给出的例子也是很简单的列出一些不应该执行的 deterministic rules，并根据 rollout length 定义为 M-order rules:</p> <ul> <li data-pid="yX7gZxuV">1-order rule: 假设我们处于状态<img src="https://www.zhihu.com/equation?tex=s_t+%3D+s" alt="s_t = s" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer">，执行动作 <img src="https://www.zhihu.com/equation?tex=%CE%B1_t%3D%CE%B1" alt="α_t=α" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer"> 并且发现状态保持不变，即下一个状态还是 <img src="https://www.zhihu.com/equation?tex=s_%7Bt%2B1%7D+%3D+s" alt="s_{t+1} = s" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer">。 那么，无论何时我们处于状态s时，都不应尝试采取动作 <img src="https://www.zhihu.com/equation?tex=%CE%B1" alt="α" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer">，除非停留在 <img src="https://www.zhihu.com/equation?tex=s" alt="s" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer"> 有 reward。</li> <li data-pid="7atPOeRM">2-order rule: 假设我们处于状态 <img src="https://www.zhihu.com/equation?tex=s_t+%3D+s" alt="s_t = s" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer">，应用动作 <img src="https://www.zhihu.com/equation?tex=%CE%B1_t%3D%CE%B1" alt="α_t=α" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer">，移动到状态 <img src="https://www.zhihu.com/equation?tex=s_%7Bt+%2B+1%7D+%3D+s%27%5Cneq+s" alt="s_{t + 1} = s'\neq s" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer">，执行第二个动作 <img src="https://www.zhihu.com/equation?tex=%CE%B1_%7Bt%2B+1%7D+%3D%CE%B1%27" alt="α_{t+ 1} =α'" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer">，我们返回到初始状态 <img src="https://www.zhihu.com/equation?tex=s_%7Bt+%2B+2%7D+%3D+s" alt="s_{t + 2} = s" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer">。 那么，永远不要尝试在处于状态 <img src="https://www.zhihu.com/equation?tex=s" alt="s" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer"> 时采取一系列动作<img src="https://www.zhihu.com/equation?tex=%28%CE%B1%EF%BC%8C%CE%B1%27%29" alt="(α，α')" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer">的计划，除非停留在<img src="https://www.zhihu.com/equation?tex=s" alt="s" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer">或<img src="https://www.zhihu.com/equation?tex=s%27" alt="s'" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer">有reward。</li> </ul> <blockquote data-pid="OTBBrprG"><p data-pid="qCrY0EPW">这个先验还真是相当 simple &amp; naive。很明显文中这种先验的结合会更适合用在概率RL上，例如 soft-Q 或 SAC 之类的有熵正则的策略梯度算法。所以本文接下来开始讨论 probabilistic RL。</p></blockquote> <h2>Probabilistic RL</h2> <p data-pid="pj_Gcdlz">考虑一个 episodic RL，我们根据如下的联合分布来生成状态和动作：</p> <p></p> <figure data-size="normal"><img src="https://pica.zhimg.com/v2-073fbaaf9d173cf46471498b7a6ff5aa_720w.jpg?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p></p> <p data-pid="5vey0FdE">由于 RL 学习的目标是最大化 <img src="https://www.zhihu.com/equation?tex=%5Csum%5E%7Bh-1%7D_%7Bt%3D0%7D+r_t" alt="\sum^{h-1}_{t=0} r_t" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer">，那么可以把它视为一种约束并加入到联合分布中：</p> <p></p> <figure data-size="normal"><img src="https://pic1.zhimg.com/v2-d3300aa6b7efc6450286ae9fbd38088c_720w.jpg?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p></p> <p data-pid="ZNFzx0wu">这里将上式的整体分解视为一个势函数（类似于无向图模型），可以定义以下后验分布，</p> <p></p> <figure data-size="normal"><img src="https://pic1.zhimg.com/v2-791a05823a382cf96d31bd90344a7f98_720w.jpg?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p></p> <p data-pid="A73osboz">假设我们处于状态 <img src="https://www.zhihu.com/equation?tex=s_t" alt="s_t" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer">，我们感兴趣的是根据所有奖励 <img src="https://www.zhihu.com/equation?tex=r_%7B0%3Ah-1%7D" alt="r_{0:h-1}" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer">以及所有过去状态和动作<img src="https://www.zhihu.com/equation?tex=%28s_%7B0%3At-1%7D%EF%BC%8C%CE%B1_%7B0%3At%7D%29" alt="(s_{0:t-1}，α_{0:t})" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer">来计算作用 <img src="https://www.zhihu.com/equation?tex=%CE%B1_t" alt="α_t" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer"> 的边际后验分布。基于 Markov 独立性，这个条件分布可以简化为：</p> <p></p> <figure data-size="normal"><img src="https://picx.zhimg.com/v2-3d47c4ea2d2dfdbe1c780985ab17166d_720w.jpg?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p></p> <p data-pid="H4LC6m1w">这样的策略满足如下所述的Bellman型递归方程</p> <p></p> <figure data-size="normal"><img src="https://picx.zhimg.com/v2-6d48dbe9e21c0174922ac7a634c52884_720w.jpg?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p></p> <p data-pid="osGSp2Cf">在常规强化学习中，状态作用值函数 <img src="https://www.zhihu.com/equation?tex=B%28s_t%2C%CE%B1_t%29" alt="B(s_t,α_t)" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer"> 与最佳Q函数联系在一起:</p> <p></p> <figure data-size="normal"><img src="https://picx.zhimg.com/v2-13ec33fbe4a0dc62c1bb90d3273823cf_720w.jpg?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p></p> <p data-pid="OhRpVk_6">更准确地说，对于离散状态和动作，我们希望直接近似命题1中的Bellman方程，以便随机近似状态动作值 <img src="https://www.zhihu.com/equation?tex=B%28s_t%EF%BC%8C%CE%B1_t%29" alt="B(s_t，α_t)" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer">。</p> <p></p> <figure data-size="normal"><img src="https://pic1.zhimg.com/v2-bd3ec5dbcd18886afb34e132ebfd31a5_720w.jpg?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p></p> <p data-pid="r1zNhOBw">然后在此基础上进行 stochastic optimization update： </p> <figure data-size="normal"><img src="https://pic1.zhimg.com/v2-94d2d922f40e30aa226d266fc8629516_720w.jpg?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p></p> <p data-pid="xDQ-thGe">最后将这个近似策略和先验像上一节一样结合起来：</p> <p></p> <figure data-size="normal"><img src="https://pica.zhimg.com/v2-fbf72cc31434c5c0e55ebf5f38f9a99c_720w.jpg?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p></p> <h2>Experiment</h2> <p data-pid="A1_J-SO5">实验部分堪称潦草，对比了一下无先验、1/2-order 在迷宫环境下的曲线，倒是确实有些 efficient，但是也没那么明显：</p> <p></p> <figure data-size="normal"><img src="https://picx.zhimg.com/v2-1ce5ee85186c816df7ed2173d3781030_720w.jpg?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p></p> <h2>Conclusion</h2> <p data-pid="3JB-6qxr">虽然是篇没什么人看的半成品文章，但是他想把 probabilistic RL 和先验结合的思路是可以借鉴借鉴的，所以看多了大佬的文章，也要看看这种集思广益一下。</p> <p data-pid="xo4C9M64">Btw. 我发现一作竟然是个专攻 probability theory in learning 的 DeepMind Research Scientist，但是仍然无法改变这是个实验十分敷衍的半成品的事实。</p> <p class="post-meta"> 1 min read   ·   March 24, 2021   ·   RL in Robotics (Zhihu) </p> <p class="post-tags"> <a href="/blog/2021"> <i class="fa-solid fa-calendar fa-sm"></i> 2021 </a> </p> </li> <li> <h3> <a class="post-title" href="https://zhuanlan.zhihu.com/p/358560363" target="_blank" rel="external nofollow noopener">小雅 ElegantRL: 基于PyTorch的轻量-高效-稳定的深度强化学习框架</a> <svg width="2rem" height="2rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </h3> <p></p> <h2>我们开源小雅 ElegantRL库的初衷：</h2> <ul> <li data-pid="nwy6byur"> <b>伯克利的RLlib ray</b>：它是优点最多的DRL开源库，实现了多种主流DRL算法，支持分布式训练，支持三个深度学习框架（TensorFlow1、PyTorch，TensorFlow2）。这个库也有缺点是：代码量多，上手难，耦合度高，难改，安装时存在大量依赖。</li> <li data-pid="v6iFNftq"> <b>OpenAI的baselines</b>：很早开源的DRL库，训练很慢，且不稳定</li> <li data-pid="vjqHwScE"> <b>hill 的 stable baselines</b>：baselines的不稳定催生了它。它使用了旧风格的TensorFlow 1，可读性较差，训练依然不够稳定</li> <li data-pid="NMgj69to"> <b>stable baselines 3</b>：TensorFlow 1的不方便，催生了基于PyTorch的<b>stable baselines 3</b>。可惜代码是照着 TF1 直译的，沿用了 baselines的旧框架，不适应 2018年后的DRL算法。直到2021年3月，它还不支持多GPU训练。</li> <li data-pid="hK59Xku3"> <b>莫烦的教学代码</b>：几年前中文社区只有他的教学代码，牺牲性能换来了可读性：训练只需几秒钟的入门级环境，他的代码需要训练十几分钟； 无法在稍高难度的环境下训练。它为推广强化学习做出了贡献，但社区需要好用的DRL库。</li> <li data-pid="daB5sVXs"> <b>「天授」</b>：一年前中文社区开源了「天授」，自媒体用力做了宣传——知乎上对它的风评可以查看：<a href="https://www.zhihu.com/question/384871366" class="internal" rel="external nofollow noopener" target="_blank">知乎：如何看待清华大学深度强化学习框架“天授”</a>。</li> </ul> <p data-pid="3q1c0cPY">还有一些比较好的库没有提及。我会找时间写一篇文章：评价主流的DRL库，告诉大家看哪些指标来判断一个DRL库好不好。（ReplayBuffer、RL训练数据的吞吐、算法依赖关系、是否区分on-policy和off-policy、多进程的控制中心等）</p> <p data-pid="58Fkdizc"><b>如果你觉得伯克利的Ray RLlib 学习成本高，那么推荐你使用「小雅：ElegantRL」</b></p> <ul> <li data-pid="_gxHIHSF"> <b>“小”</b>：只需安装 PyTorch 和 Matplotlib 用于 网络训练 以及 画图协助调参，整个库只有4个 py 文件，并且tutorial版将一直维持在1000行代码以内，保持与最新版使用相同的变量名与代码结构，把学习成本降到最低。</li> <li data-pid="wjRSgmhR"> <b>“雅”</b>：代码优雅 可读性高，耦合度低 可移植性高。我们整理出DRL算法的基本模板，让它们都继承自一个基类。只要照着模板编写新算法，就能自动支持多进程训练。</li> <li data-pid="_7IrPR4o"> <b>高性能</b>：ElegantRL在支持多进程训练后，她的最新版最要支持<b>多GPU训练</b>了。RL与DL的不同导致它的分布式不能照搬DL的多GPU训练模式。所以无法直接使用PyTorch 或TensorFlow 这些深度学习框架自带的多GPU训练模块。</li> </ul> <figure data-size="normal"><img src="https://picx.zhimg.com/v2-5df216944977980fd28405611d80f248_720w.jpg?source=d16d100b" data-size="normal" data-rawwidth="496" data-rawheight="415" class="origin_image zh-lightbox-thumb" width="496" data-original="https://picx.zhimg.com/v2-5df216944977980fd28405611d80f248_720w.jpg?source=d16d100b" style="max-width: 100%;" referrerpolicy="no-referrer"><figcaption>ElegantRL的Logo：圆π（强化学习求π策略），整体是Elegant的E，配色是 Columbia Blue</figcaption></figure><p data-pid="UpKONU_6">我觉得伯克利的Ray RLlib 不好上手，其他DRL库性能不够好，因此我们想寻求一个折中的方案，这是我们开源<b>「小雅：ElegantRL」的初衷：希望有比Ray RLlib更容易使用的DRL库</b>。我们接下来会开源它的分布式版本。</p> <a href="http://link.zhihu.com/?target=https%3A//github.com/AI4Finance-LLC/ElegantRL" data-draft-node="block" data-draft-type="link-card" class=" wrap external" target="_blank" rel="nofollow noreferrer">Github ElegantRL</a><p data-pid="LDKPtP9y"><b>微信群</b>（二维码更新于 2022-04-27 11:30:47）</p> <figure data-size="small"><img src="https://picx.zhimg.com/v2-e8c8955c39780408f6a4adadab2a1659_720w.jpg?source=d16d100b" data-caption="" data-size="small" data-rawwidth="720" data-rawheight="865" class="origin_image zh-lightbox-thumb" width="720" data-original="https://picx.zhimg.com/v2-e8c8955c39780408f6a4adadab2a1659_720w.jpg?source=d16d100b" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p data-pid="sZ2nEwEr"><b>本文由</b> <a class="member_mention" href="http://www.zhihu.com/people/0b139554c20b9220ef9f413e61a62a38" data-hash="0b139554c20b9220ef9f413e61a62a38" data-hovercard="p$b$0b139554c20b9220ef9f413e61a62a38" rel="external nofollow noopener" target="_blank">@luke</a> <b> 翻译自 Medium: </b><a href="http://link.zhihu.com/?target=https%3A//elegantrl.medium.com/elegantrl-a-lightweight-and-stable-deep-reinforcement-learning-library-95cef5f3460b" class=" wrap external" target="_blank" rel="nofollow noreferrer">ElegantRL: a lightweight and stable deep reinforcement learning library</a> </p> <p data-pid="7oEzwh6R">一句话概括强化学习（RL）：Agent不停地与环境互动，通过反复尝试的方式进行学习，在一定的不确定性下做出决策，最终达到exploration (尝试新的可能) 与exploitation (利用旧知识) 之间的平衡。</p> <p data-pid="sCnwhdZB">目录</p> <div class="highlight"><pre><code class="language-text"><span></span>1. ElegantRL的特点
2. ElegantRL的缺点（改进的方向）
3. 总述：文件结构和函数
4. 网络类的集合 net.py
5. DRL算法的构建 agent.py
6. 训练流程 run.py
7. 效果展示 BipedalWalker-v3
8. 回复Trinkle 的长评论
</code></pre></div> <h2><b>1. ElegantRL的特点</b></h2> <p data-pid="RfdCigVN">深度强化学习（DRL）在解决对人类有挑战性的多个现实问题中展示了巨大的潜力，例如自动驾驶，游戏竞技，自然语言处理（NLP）以及金融交易等。各种各样的深度强化学习算法及应用正在不断涌现。ElegantRL能够帮助研究人员和从业者更便捷地“设计、开发和部署”深度强化学习技术。<b>ElegantRL的 elegant 体现在以下几个方面: </b></p> <ul> <li data-pid="lD_ox77l"> <b>轻量：</b>代码量低于1000行。</li> <li data-pid="Tm3nrz6o"> <b>高效：</b>性能向Ray RLlib靠拢。</li> <li data-pid="5LWnpGLN"> <b>稳定：</b>比Stable Baseline 3更加稳定。</li> </ul> <p data-pid="ynbTOfbL">ElegantRL支持离散动作空间以及连续动作空间下的常用DRL算法。并且我们提供了十分友好的教程。</p> <h2>ElegantRL的缺点<b>（改进的方向）</b> </h2> <p data-pid="DL1pFVNF">我们已经在本地实现了一些新功能，如分布式、新的算法。我们发现：在考虑分布式的整体设计的同时又要保持代码可读性比较困难。因此我们没有全部更新到最新版里，以后会陆续整理并发布到这里 <a href="http://link.zhihu.com/?target=https%3A//github.com/AI4Finance-LLC/ElegantRL/tree/master/elegantrl/AgentZoo" class=" wrap external" target="_blank" rel="nofollow noreferrer">ElegantRL/AgentZoo</a> ，试用一段时间后，会移到最新版的文件夹中。</p> <ul> <li data-pid="I1-DBCQM"> <b>多GPU、甚至分布式：</b>目前的版本和Stable Baseline 一样只支持multi-workers，还不支持多GPU训练（其实多GPU和分布式都做了，毕竟如果不做到多GPU甚至分布式训练，怎能说自己“性能向Ray RLlib靠拢”呢？）</li> <li data-pid="Xs2ByvFL"> <b>算法少、缺少多智能体算法：</b>我们清楚还有很多重要的DRL算法需要实现，例如 <a href="https://zhuanlan.zhihu.com/p/342919579" class="internal" rel="external nofollow noopener" target="_blank">如何选择深度强化学习算法？MuZero/SAC/PPO/TD3/DDPG/DQN/等</a>。可能最近会更新 V-MPO、MuZero 和 QR-DQN。特别要指出的是：我们还没有足够的经验去挑选出优秀的MARL算法，希望你们能向我们推荐如 QMix、MAPPO这类坚实的工作。</li> </ul> <p data-pid="bG7l2KIr">在ElegantRL中，我们基于Actor-Critic框架搭建深度强化学习算法，其中每一个Agent（即DRL算法）由Actor网络和Critic网络组成。利用我们完整简洁的代码结构，用户可以非常轻松地开发自己的Agent。代码已上传至 <a href="http://link.zhihu.com/?target=https%3A//github.com/AI4Finance-LLC/ElegantRL" class=" wrap external" target="_blank" rel="nofollow noreferrer">Github ElegantRL</a>。</p> <a href="http://link.zhihu.com/?target=https%3A//github.com/AI4Finance-LLC/ElegantRL" data-draft-node="block" data-draft-type="link-card" data-image="https://picx.zhimg.com/v2-2b1960251766ce0eac4070a4b24bf2ff_l.jpg?source=d16d100b" data-image-width="400" data-image-height="400" class=" wrap external" target="_blank" rel="nofollow noreferrer">AI4Finance-LLC/ElegantRL</a><h2><b>3. 总述：文件结构和函数</b></h2> <p data-pid="1oCdOb2h"><b>ElegantRL的“小”最直观的体现就是：整个库只有3个文件</b>，net.py, agent.py, run.py。再加上一个env.py 用于存放与训练环境有关的代码。我们很开心能在Tutorial版用小于1000行的代码对一个完整的DRL库进行实现，这对想要入门深度强化学习的人能有莫大的帮助。</p> <p data-pid="j96eTfJg">请注意，Tutorial版的ElegantRL 只是用来学习的。如果想要把ElegantRL当成生产工具，就需要使用最新版的ElegantRL，他的文件结构和函数保持了与 Tutorial版的统一。我相信这个库能对强化学习这个领域有贡献。</p> <blockquote data-pid="a9GhO_F-">我们组建了一个QQ群“深度强化学习ElegantRL”（群号 1163106809），用于交流深度强化学习（<b>这个群不用来讨论ElegantRL</b>）。从在2020-10月组建到今天已经600+人了。在前期，群友们为提高Tutorial版的代码可读性提出了很多宝贵的建议。今天这个群已经成为讨论深度强化学习的净土，我只希望能帮到更多对强化学习感兴趣的人。</blockquote> <figure data-size="normal"><img src="https://picx.zhimg.com/v2-47bbe86532c4697c78576158b451485a_720w.jpg?source=d16d100b" data-size="normal" data-rawwidth="1302" data-rawheight="466" class="origin_image zh-lightbox-thumb" width="1302" data-original="https://picx.zhimg.com/v2-47bbe86532c4697c78576158b451485a_720w.jpg?source=d16d100b" style="max-width: 100%;" referrerpolicy="no-referrer"><figcaption>图1. Agent.py中的Agent使用Net.py中的网络，在Run.py中通过与Env.py中的环境互动来训练</figcaption></figure><p data-pid="NwabON2i">ElegantRL的文件结构如图1所示：</p> <ul> <li data-pid="gvNVwGYE">Env.py：包含与Agent互动的环境</li> <ul><ul> <li data-pid="jl_KpBDC">包括PreprocessEnv类，可用于对gym的环境进行改动。</li> <li data-pid="qB98oe9N">包括自主开发的股票交易环境作为用户自定义环境的例子。</li> </ul></ul> <li data-pid="yvffwuFx">Net.py：包含三种类型的网络，每个类型均包括一个网络基类，以便于针对不同算法进行继承与派生。</li> <ul><ul> <li data-pid="JnMMxlQh">Q网络</li> <li data-pid="qTZZ2VNH">Actor网络</li> <li data-pid="NFnfZiH7">Critic网络</li> </ul></ul> <li data-pid="hfpzc5VH">Agent.py：包含不同用于算法的Agent。</li> <li data-pid="PpYFLTlF">Run.py：提供用于训练和测试的基本函数。</li> <ul><ul> <li data-pid="fxbiojWc">参数初始化</li> <li data-pid="WiWJVaBE">训练</li> <li data-pid="OF6-3xdD">评测</li> </ul></ul> </ul> <p data-pid="Ehj_3R8r">从一个较高的层面描述这些文件之间的关系：首先，初始化Env.py文件中的环境和Agent.py文件中的Agent。Agent 既有包含在Net.py 文件中的网络，又与Env.py 文件中的环境进行互动。在Run.py 中进行的每一步训练中，Agent都会与环境进行互动，产生transitions 并将其存入回放缓存（Replay Buffer）。之后，Agent从Replay Buffer 中获取数据来更新它的网络。每一步更新之后，都会<b>有一个评测器来评测并保存表现好的Agent</b>。</p> <blockquote data-pid="amZKJfOQ">问：强化学习算法需要训练多久？训练到什么程度就算收敛了？（经常有人在群里问）<br>答：与深度学习不同，不总是训练时间越长，智能体的性能越好。复杂情况下，不一定能观察到强化学习收敛。<b>因此，强化学习并不是保存训练时间最长的，而是保存性能最好的model。</b>其他DRL库缺少评测智能体性能的模块，这让我不满意。因此，ElegantRL里面有一个评测器 Evaluator 用来画出 learning curve，然后自动保存保存表现好的Agent。不用担心这会拖慢训练速度，因为我们开启了一个独立于训练进程的子进程利用于运行评测器。评测器还会画出其他指标，帮助我们调整超参数，或者修改自定义的环境或者算法。</blockquote> <h2><b>4. 网络类的集合 net.py</b></h2> <p data-pid="w5Z-_yut">net.py 文件存放了算法库会使用到的神经网络。我们将网络分为了三类：</p> <div class="highlight"><pre><code class="language-python3"><span></span><span class="sd">'''Q 网络'''</span>
<span class="k">class</span> <span class="nc">QNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="k">class</span> <span class="nc">QNetDuel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span> 
<span class="k">class</span> <span class="nc">QNetTwin</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="k">class</span> <span class="nc">QNetTwinDuel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>


<span class="sd">'''Policy 网络(Actor)'''</span>
<span class="k">class</span> <span class="nc">Actor</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="k">class</span> <span class="nc">ActorPPO</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="k">class</span> <span class="nc">ActorSAC</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

<span class="sd">'''Value 网络(Critic)'''</span>
<span class="k">class</span> <span class="nc">Critic</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="k">class</span> <span class="nc">CriticAdv</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span> 
<span class="k">class</span> <span class="nc">CriticTwin</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</code></pre></div> <p data-pid="9G3R3uUw">这为使用者微调算法提供了极大的便利。例如：把图片作为state输入神经网络时，我只需要到 net.py 将全连接层修改为卷积层即可。这种模式把深度神经网络与强化学习隔开，提高了代码的可读性。debug的时候，你会感激我：把net.py单独列出来，并把计算张量的计算加到网络里。类似的很多细节是我们从2019年总结至今的成果。<b>因为，在之前我们一直没有正式发布「小雅：ElegantRL」，为的就是争取更多机会对整个架构进行调整。</b>小雅不像其他库一样有很重的历史包袱。</p> <h2><b>5. DRL算法的构建 agent.py</b></h2> <p data-pid="MfaSxowS">agent.py存放了不同的DRL算法。在这一部分，我们将分别描述DQN系列算法和DDPG系列算法。ElegantRL中每一个DRL算法的Agent都继承自它的基类。在实现分布式的过程中，我们不能拖着太长的尾巴（太多的DRL算法），因此更多的DRL算法还不能马上与大家见面。我们已经实现了部分在这篇文章中提及算法：<a href="https://zhuanlan.zhihu.com/p/342919579" class="internal" rel="external nofollow noopener" target="_blank"> 如何选择深度强化学习算法？MuZero/SAC/PPO/TD3/DDPG/DQN/等</a> ，未来还会有更多。</p> <p data-pid="mRTViObs"><b>DQN系列中的Agent类:</b></p> <div class="highlight"><pre><code class="language-python3"><span></span><span class="k">class</span> <span class="nc">AgentDQN</span><span class="p">:</span>
<span class="k">class</span> <span class="nc">AgentDuelingDQN</span><span class="p">(</span><span class="n">AgentDQN</span><span class="p">):</span>
<span class="k">class</span> <span class="nc">AgentDoubleDQN</span><span class="p">(</span><span class="n">AgentDQN</span><span class="p">):</span>
<span class="k">class</span> <span class="nc">AgentD3QN</span><span class="p">(</span><span class="n">AgentDoubleDQN</span><span class="p">):</span>
</code></pre></div> <figure data-size="normal"><img src="https://picx.zhimg.com/v2-58a33787b03321fab5ee0e7d7136b155_720w.jpg?source=d16d100b" data-size="normal" data-rawwidth="3058" data-rawheight="784" class="origin_image zh-lightbox-thumb" width="3058" data-original="https://picx.zhimg.com/v2-58a33787b03321fab5ee0e7d7136b155_720w.jpg?source=d16d100b" style="max-width: 100%;" referrerpolicy="no-referrer"><figcaption>图 2. DQN系列的继承层级关系</figcaption></figure><p data-pid="iXK_LUc_">如图2所示，ElengtRL中DQN系列的继承层级关系为：</p> <ul> <li data-pid="IwJ64S7S">AgentDQN：标准的DQN Agent。</li> <li data-pid="UZMC5YX1">AgentDoubleDQN：为了减少过高估计而包含两个Q网络的Double-DQN，继承自AgentDQN。</li> <li data-pid="b-CHO5WN">AgentDuelingDQN：采用不同的Q值计算方式的DQN Agent，继承自AgentDQN。</li> <li data-pid="jRK6zpbm">AgentD3QN：AgentDoubleDQN和AgentDuelingDQN的结合，继承自 AgentDoubleDQN。</li> </ul> <p data-pid="4929fyfJ"><b>DDPG系列中的Agent类:</b></p> <div class="highlight"><pre><code class="language-python3"><span></span><span class="k">class</span> <span class="nc">AgentBase</span><span class="p">:</span>
<span class="k">class</span> <span class="nc">AgentDDPG</span><span class="p">(</span><span class="n">AgentBase</span><span class="p">):</span>
<span class="k">class</span> <span class="nc">AgentTD3</span><span class="p">(</span><span class="n">AgentDDPG</span><span class="p">):</span>
<span class="k">class</span> <span class="nc">AgentSAC</span><span class="p">(</span><span class="n">AgentBase</span><span class="p">):</span>
<span class="k">class</span> <span class="nc">AgentPPO</span><span class="p">(</span><span class="n">AgentBase</span><span class="p">):</span>
<span class="k">class</span> <span class="nc">AgentModSAC</span><span class="p">(</span><span class="n">AgentSAC</span><span class="p">):</span>     <span class="c1"># Modified </span>
<span class="k">class</span> <span class="nc">AgentInterSAC</span><span class="p">(</span><span class="n">AgentBase</span><span class="p">):</span>  <span class="c1"># InterXXX表示parameter sharing</span>
<span class="k">class</span> <span class="nc">AgentInterPPO</span><span class="p">(</span><span class="n">AgentBase</span><span class="p">):</span>  <span class="c1"># InterXXX表示parameter sharing</span>
<span class="o">...</span>
</code></pre></div> <figure data-size="normal"><img src="https://pica.zhimg.com/v2-c2db1197f38e858fae2a714af2424d5d_720w.jpg?source=d16d100b" data-size="normal" data-rawwidth="700" data-rawheight="266" class="origin_image zh-lightbox-thumb" width="700" data-original="https://picx.zhimg.com/v2-c2db1197f38e858fae2a714af2424d5d_720w.jpg?source=d16d100b" style="max-width: 100%;" referrerpolicy="no-referrer"><figcaption>图 3. DDPG系列的继承层级关系</figcaption></figure><p data-pid="GZYnEdDa">如图3所示，ElegantRL中DDPG系列的继承层级关系为</p> <ul> <li data-pid="J8oovPSs">AgentBase：所有A-C框架下Agent的基类，包括所有Agent共有的参数。</li> <li data-pid="XN3aDkdF">AgentDDPG：DDPG Agent，继承自AgentBase。</li> <li data-pid="7NmgaZx0">AgentTD3：采用新的更新方式的TD3 Agent，继承自AgentDDPG</li> </ul> <p data-pid="X0Q_TKJ_">在构建DRL算法的Agent时，采用的这种层级结构极大地提升了ElegantRL的轻量性和高效性。当然，用户也可以按照类似的方式构建自己的Agent。</p> <p data-pid="W1jvHBVg">从根本上来说，每一个Agent都由两大基本功能构成。从数据流的角度可描述为图4的形式：</p> <ul> <li data-pid="HPeFvUk_">探索环境 explore_env： Agent利用与环境互动，在此过程中产生用于训练网络的数据。</li> <li data-pid="-N7YUkgv">更新网络 update_net： Agent从回放缓存 Replay Buffer 中获取数据（一批transitions），并利用这些数据来更新网络<b>。</b> </li> </ul> <figure data-size="normal"><img src="https://picx.zhimg.com/v2-c437c7a028f4a86f603efb38f893cb8f_720w.jpg?source=d16d100b" data-size="normal" data-rawwidth="1881" data-rawheight="323" class="origin_image zh-lightbox-thumb" width="1881" data-original="https://pica.zhimg.com/v2-c437c7a028f4a86f603efb38f893cb8f_720w.jpg?source=d16d100b" style="max-width: 100%;" referrerpolicy="no-referrer"><figcaption>图4. Agent的数据流</figcaption></figure><p data-pid="dj3uuupk"><b>经验回放缓存：</b></p> <div class="highlight"><pre><code class="language-text"><span></span>class ReplayBuffer:
class ReplayBufferMP:
</code></pre></div> <p data-pid="nXNbGMPd">经验回放缓存将极大地影响的DRL库的训练速度。我们专门对 ReplayBuffer 进行了优化，详见 <a href="https://zhuanlan.zhihu.com/p/103605702" class="internal" rel="external nofollow noopener" target="_blank">DRL的经验回放(Experiment Replay Buffer) 用Numpy实现让它更快一点</a> 。</p> <h2><b>6. 训练流程 run.py</b></h2> <p data-pid="J0biJtga">训练一个DRL Agent包含两大步:</p> <ul><li data-pid="Huo7ZlKj">初始化：</li></ul> <div class="highlight"><pre><code class="language-text"><span></span>args = Arguments(): 加载默认的超参数
env = PreprocessEnv()：创建（gym）环境。
agent = agent.XXX：基于算法创建Agent。
evaluator = Evaluator()：用于评测并保存模型。
buffer = ReplayBuffer()：回放缓存。
</code></pre></div> <ul><li data-pid="gFpp4njG">通过一个while循环来控制训练过程，这个while循环只有当达到某些特定条件时才会终止，比如获得目标分数，达到最大步数或人为终止：</li></ul> <div class="highlight"><pre><code class="language-text"><span></span>agent.explore_env(...)：
Agent在规定的步数内探索环境，产生transitions并将其保存至回放缓存（Replay Buffer）。

agent.update_net(...)：
Agent根据回放缓存（Replay Buffer）中的一批数据来更新网络参数。

evaluator.evaluate_save(...)：
评测Agent的表现，并保存具有最高得分的模型参数。 
</code></pre></div> <h2><b>7. 效果展示 BipedalWalker-v3</b></h2> <p data-pid="H3AVk8YG">BipedalWalker-v3是机器人研究中的一个经典任务：训练一个双足机器人尽快向前移动，同时消耗更少能量。由于它是一项连续动作空间下的简单任务，多数深度强化学习算法都能达到目标分数。我们提供这个环境供使用者快速体验算法。</p> <blockquote data-pid="t6aSG3sT">经过与群友的磨合，选来选去，在大陆，只有 BipedalWalker LunarLander 这些基于Box2D引擎的env 最容易安装。MuJoCo 需要收费，PyBullet 的一些环境需要训练超过半小时，且对winOS支持不好，OpenAI gym 的一些toy env 太简单只需要训练几秒钟。</blockquote> <p data-pid="AEypc2K-">另外，在此我们想要特别说明，每个DRL算法都有它的适用场景，并且要在合适的超参数设定下使用高质量的代码才能展现出它的实力。例如在此任务下，SAC算法在超参数合适、且代码实现高质量的情况下可以通关；而PPO算法需要很大的采样数量（甚至要接近显存极限）才能通关。</p> <p data-pid="UlfEG52s">接下来我们将展示如何利用ElegantRL一步一步训练能通关BipedalWalker-v3的深度强化学习算法。以下代码，你甚至可以在 <a href="http://link.zhihu.com/?target=https%3A//github.com/AI4Finance-LLC/ElegantRL" class=" wrap external" target="_blank" rel="nofollow noreferrer">Github ElegantRL</a> 的根目录找到三个文件并直接运行。</p> <div class="highlight"><pre><code class="language-text"><span></span>BipedalWalker_Example.ipynb  # 运行在Colab 的代码
Example_Demo.ipynb           # 登陆Colab有困难的，可以在本地运行这段代码
Example_SingleFilePPO.py     # 我们甚至提供了单个文件运行PPO算法，更容易上手
</code></pre></div> <p data-pid="eeWUw3yW"><b>步骤1：安装ElegantRL</b></p> <div class="highlight"><pre><code class="language-text"><span></span>pip install git+https://github.com/AI4Finance-LLC/ElegantRL.git
</code></pre></div> <p data-pid="qTgDBKmO"><b>步骤2：导入相关的库</b></p> <ul> <li data-pid="MacbKs9c">ElegantRL</li> <li data-pid="_GNmhibm">OpenAI Gym:用于开发和比较不同强化学习算法的工具</li> <li data-pid="2suKJ0FN">PyBullet Gym: OpenAI Gym的MuJoCo环境的开源实现</li> </ul> <div class="highlight"><pre><code class="language-python3"><span></span><span class="kn">from</span> <span class="nn">elegantrl.run</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">elegantrl.agent</span> <span class="kn">import</span> <span class="n">AgentPPO</span>
<span class="kn">from</span> <span class="nn">elegantrl.env</span> <span class="kn">import</span> <span class="n">PreprocessEnv</span>
<span class="kn">import</span> <span class="nn">gym</span>
<span class="n">gym</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">set_level</span><span class="p">(</span><span class="mi">40</span><span class="p">)</span> <span class="c1"># Block warning</span>
</code></pre></div> <p data-pid="KaKwkA4r"><b>步骤3：指定DRL算法 AgentXXX 和环境 EnvXXX</b></p> <ul> <li data-pid="WK4thbTr">args.agent：首先选定DRL算法，用户可以选择agent.py中的Agent</li> <li data-pid="k4hEiMem">args.env: 创建并修饰环境，用户在env.py中既可以创建自定义的环境，也可以修饰OpenAI Gym和PyBullet Gym的环境</li> </ul> <div class="highlight"><pre><code class="language-text"><span></span>args = Arguments(if_on_policy=False)
args.agent = AgentPPO() # AgentSAC(), AgentTD3(), AgentDDPG()
args.env = PreprocessEnv(env=gym.make(‘BipedalWalker-v3’))
args.reward_scale = 2 ** -1 # 不同算法实现在该任务上的收益区间: -200 &lt; -150 &lt; 300 &lt; 334
</code></pre></div> <p data-pid="q8G1JzlB"><b>步骤4：训练并评测Agent</b></p> <p data-pid="cvHbqVPx">训练和评测过程都在这个函数中实现train_and_evaluate_mp(args)。其中包含了DRL中的两个对象:</p> <div class="highlight"><pre><code class="language-python3"><span></span><span class="n">AgentXXX</span><span class="p">()</span>
<span class="n">EnvXXXXX</span><span class="p">()</span>
</code></pre></div> <p data-pid="LP5VPNaH"><b>其中包括用于训练的参数：</b></p> <div class="highlight"><pre><code class="language-python3"><span></span><span class="n">批次数据数量</span><span class="err">（</span><span class="n">batch_size</span><span class="err">），</span>
<span class="n">目标步数</span><span class="err">（</span><span class="n">target_step</span><span class="err">），</span>
<span class="n">收益大小比例调整</span><span class="err">（</span><span class="n">reward_scale</span><span class="err">），</span>
<span class="n">折扣率</span><span class="err">（</span><span class="n">gamma</span><span class="err">），</span> <span class="n">等等</span><span class="err">。</span><span class="n">另外还包括用于评测的参数</span><span class="err">：</span>
<span class="n">最大训练步数</span><span class="err">（</span><span class="n">break_step</span><span class="err">），</span>
<span class="n">随机种子</span><span class="err">（</span><span class="n">random_seed</span><span class="err">），</span><span class="n">等等</span><span class="err">。</span>
</code></pre></div> <p data-pid="FwZk37H-"> 一旦满足某个条件，训练过程将会自动终止。然后画出 learning curve 等数据帮助使用者调整超参数、算法以及环境。</p> <div class="highlight"><pre><code class="language-text"><span></span>train_and_evaluate_mp(args)
</code></pre></div> <p data-pid="-QC6cgFD"><b>步骤5：测试结果</b></p> <p data-pid="raafEEyt"><b>我们提供的一段代码，调用 env.render()把训练的结果渲染出来，并合成视频：</b></p> <div class="highlight"><pre><code class="language-python3"><span></span><span class="n">get_video_to_watch_env_render</span><span class="p">()</span>

<span class="o">...</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1024</span><span class="p">):</span>
    <span class="n">frame</span> <span class="o">=</span> <span class="n">gym_env</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="s1">'rgb_array'</span><span class="p">)</span>
    <span class="n">cv2</span><span class="o">.</span><span class="n">imwrite</span><span class="p">(</span><span class="sa">f</span><span class="s1">'</span><span class="si">{save_dir}</span><span class="s1">/</span><span class="si">{i:06}</span><span class="s1">.png'</span><span class="p">,</span> <span class="n">frame</span><span class="p">)</span>
    
    <span class="n">states</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">((</span><span class="n">state</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">actions</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="n">states</span><span class="p">)</span>
    <span class="n">action</span> <span class="o">=</span> <span class="n">actions</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span> <span class="k">if</span> <span class="n">done</span> <span class="k">else</span> <span class="n">next_state</span>
<span class="o">...</span>
</code></pre></div> <figure data-size="small"><img src="https://pic1.zhimg.com/v2-f421f33ab5ff8441c120756908114b42_720w.jpg?source=d16d100b" data-size="small" data-rawwidth="600" data-rawheight="400" class="origin_image zh-lightbox-thumb" width="600" data-original="https://pic1.zhimg.com/v2-f421f33ab5ff8441c120756908114b42_720w.jpg?source=d16d100b" style="max-width: 100%;" referrerpolicy="no-referrer"><figcaption>训练前的 双足机器人，它使用 随机动作</figcaption></figure><figure data-size="small"><img src="https://picx.zhimg.com/v2-e31f570faac6ce8c9f7274fd5ddd10b3_720w.jpg?source=d16d100b" data-size="small" data-rawwidth="600" data-rawheight="400" class="origin_image zh-lightbox-thumb" width="600" data-original="https://pica.zhimg.com/v2-e31f570faac6ce8c9f7274fd5ddd10b3_720w.jpg?source=d16d100b" style="max-width: 100%;" referrerpolicy="no-referrer"><figcaption>训练后的 双足机器人，PPO算法下得到了比较谨慎的步态</figcaption></figure><p data-pid="ecPMUqRd">我们提供了 <a href="http://link.zhihu.com/?target=https%3A//github.com/AI4Finance-LLC/ElegantRL/blob/master/BipedalWalker_Example.ipynb" class=" wrap external" target="_blank" rel="nofollow noreferrer">Colab</a> 的代码，帮助你运行双足机器人 BipedalWalker-v3 的demo，也许你能解锁出这个机器人的其他步态。如果你认为这个例子难度太低，该任务的进阶版BipedalWalkerHardCore-v3 是一个困难任务，环境中将会随机出现阻挡前进的台阶、沟壑以及大小箱子。由于它要求智能体在随机因素大的环境下训练（Dynamically Varying Environments 或者叫 非平稳环境 Non-Stationary Environments），因而只有少量的强化学习算法实现能达到目标分数。在<a href="http://link.zhihu.com/?target=https%3A//github.com/openai/gym/wiki/Leaderboard%25EF%25BC%2589" class=" wrap external" target="_blank" rel="nofollow noreferrer">gym 的 Leaderboard</a>搜索这个环境的名字，可以看到有很多没有标注通关步数的算法 。</p> <p data-pid="TlxstHMc"><b>ElegantRL 就能通关这个很难的环境</b>，选对了算法，用对了超参数的情况下，单GPU训练超过半天就能通关，有兴趣的可以挑战一下（用SAC、ModSAC 可以通关，而PPO比较难）。视频见：<a href="http://link.zhihu.com/?target=https%3A//www.bilibili.com/video/BV1wi4y187tC" class=" wrap external" target="_blank" rel="nofollow noreferrer">通关双足机器人硬核版 BipedalWalkerHardcore-v3_B站</a> </p> <figure data-size="normal"><img src="https://pic1.zhimg.com/v2-bcf11ae9f3455109e7c395d444aaa563_720w.jpg?source=d16d100b" data-caption="" data-size="normal" data-rawwidth="637" data-rawheight="479" class="origin_image zh-lightbox-thumb" width="637" data-original="https://picx.zhimg.com/v2-bcf11ae9f3455109e7c395d444aaa563_720w.jpg?source=d16d100b" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p data-pid="EeGC3EaN">很惭愧，只为强化学习领域做了一点微小的共享。如果github能给个星星，也许我们会利用更多空余时间尽快更新把其他DRL算法以及分布式，<b>有推荐的算法可以在写评论区，特别是MARL算法，谢谢了。</b></p> <a href="http://link.zhihu.com/?target=https%3A//github.com/AI4Finance-LLC/ElegantRL" data-draft-node="block" data-draft-type="link-card" data-image="https://picx.zhimg.com/v2-2b1960251766ce0eac4070a4b24bf2ff_l.jpg?source=d16d100b" data-image-width="400" data-image-height="400" class=" wrap external" target="_blank" rel="nofollow noreferrer">AI4Finance-LLC/ElegantRL</a><p data-pid="iog71VwA">万一github访问不了，那么还有码云的代码库（不能保证是最新版）</p> <a href="http://link.zhihu.com/?target=https%3A//gitee.com/yonv/ElegantRL" data-draft-node="block" data-draft-type="link-card" class=" external" target="_blank" rel="nofollow noreferrer"><span class="invisible">https://</span><span class="visible">gitee.com/yonv/ElegantR</span><span class="invisible">L</span><span class="ellipsis"></span></a><hr> <h2>8. 回复<a class="member_mention" href="http://www.zhihu.com/people/0a3cffd4427c90a63911fcff9c9c99b6" data-hash="0a3cffd4427c90a63911fcff9c9c99b6" data-hovercard="p$b$0a3cffd4427c90a63911fcff9c9c99b6" rel="external nofollow noopener" target="_blank">@Trinkle</a> 的长评论</h2> <p data-pid="WTEZ0VFo">我在下面的回复是我们内部讨论后给出的，与评论区的我个人的回复有所不同。</p> <blockquote data-pid="JyDihMtk">1. 一个正常的这种开源项目至少是需要自动化的单元测试的</blockquote> <p data-pid="v2mAsjrk">我们在本地有测试单元测试模块。我们准备在更新多GPU分布式之后，上传测试模块。</p> <blockquote data-pid="G-mVUiEC">2.1 单文件行数如果超过1000行是不利于后续开发和维护的，</blockquote> <p data-pid="_NkrRTv6">我们认同“单文件行数如果超过1000行是不利于后续开发和维护的”。我认为这个表述应该是“<b>单个类行数如果超过1000行</b>是不利于后续开发和维护的”。两个相同类型的类放在同个文件里是正常的做法。</p> <blockquote data-pid="8XgpN0yC">2.2 而且repo里面存在大量的**核心**代码复制，通常是一系列bug的来源；</blockquote> <p data-pid="AXiRqMRQ">这里讨论的是耦合与解耦合。把重复出现的代码写成一个函数是软件工程中的常用做法，这可以使代码简洁，并减少代码量，代价是：提高了代码的耦合度。<b>适当地进行这种操作是有益的，但是把控不好这个度，便会引入更加复杂的依赖关系。</b>权衡之后，我们写出了现版本的ElegantRL。我们不接受这个修改建议。</p> <div class="highlight"><pre><code class="language-python3"><span></span><span class="bp">self</span><span class="o">.</span><span class="n">cri_optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
<span class="n">obj_critic</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="bp">self</span><span class="o">.</span><span class="n">cri_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
<span class="bp">self</span><span class="o">.</span><span class="n">soft_update</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cri_target</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cri</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">soft_update_tau</span><span class="p">)</span>

<span class="c1"># 把大量重复出现的代码写成一个函数：</span>
<span class="k">def</span> <span class="nf">foo</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">obj_critic</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">cri_optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">obj_critic</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">cri_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">soft_update</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cri_target</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cri</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">soft_update_tau</span><span class="p">)</span>
</code></pre></div> <blockquote data-pid="eSUPD6cz">2.3 single file来完整地处理一整个算法流程，那么和stable-baselines3又有什么差别呢…？</blockquote> <p data-pid="dXvic_jw"><b>我们不是 single file。</b></p> <blockquote data-pid="rSauslUs">3.1 replay buffer有mp版本很不错，但是似乎只支持on-policy的系列算法，</blockquote> <p data-pid="Ldc0DZTc"><b>我们不只支持on-policy的系列算法</b>，还支持 off-policy，并且已经留出拓展空间，方便以后支持 MCTS系列（Model-based planning）。</p> <blockquote data-pid="OE_krIDo">3.2 而且replay buffer把state、action、reward写死了（用torch.as_tensor来flatten成array）这样其实挺不好的（其实这就是openai的做法），不利于后续拓展更多的环境</blockquote> <p data-pid="36puKmbO">我们已经调研过现存的DRL算法，详见<a href="https://zhuanlan.zhihu.com/p/342919579" class="internal" rel="external nofollow noopener" target="_blank">如何选择深度强化学习算法？MuZero/SAC/PPO/TD3/DDPG/DQN/等（已完成）</a> ，实现DRL库时，我们让state、action、reward的储存模式分出这三类：on-policy、off-policy 以及 MCTS（Model-based planning），<b>这样处理能让replay buffer足够灵活，有利于后续拓展更多的环境</b>：明显地提高了数据吞吐速度，从而直接节省训练时间，详见<a href="https://zhuanlan.zhihu.com/p/103605702" class="internal" rel="external nofollow noopener" target="_blank">DRL的经验回放(Experiment Replay Buffer) 用Numpy 实现让它更快一点</a> 。这里走了与天授不同的技术路线，让用户选择吧。</p> <blockquote data-pid="atm_9MFR">4. 实际上你会发现agent.update_*里面有很多相同的代码你写了好几遍，如果之后你想搞分布式训练的话（比如多个环境的交互）这一段所有的逻辑都得重写，天授的做法是专门拆出来一个Collector能够模块化地处理这件事</blockquote> <p data-pid="28-iwBLW">我们认为你们会遵守基本的沟通礼节（2.1/3.1/6.2），所以我们刚才必须阐明我们的立场。我现在讲一句，<b>你们没有资格在ElegantRL面前说：“你们从自身经验出发 给ElegantRL的分布式提建议”。直到2021年3月，天授没有实现分布式</b>。（我们过几周就会把本地的多GPU版本上传上去，然后再过几个月就会上传本地的分布式版本了）。若天授有升级为分布式的打算，它这一段所有的逻辑都得重写。这里提及的与 2.2 讨论的是同一件事情，详见上文“<b>但是把控不好这个度，便会引入更加复杂的依赖关系</b>。”</p> <div class="highlight"><pre><code class="language-text"><span></span>上面这段话的出处是：
我们认为你们会遵守基本的外交礼节，所以我们刚才必须阐明我们的立场。
我现在讲一句，你们没有资格在中国面前说：“你们从实力的地位出发同中国谈话”。——杨洁篪（阿拉斯加中美对话）

我们认为你们会遵守基本的沟通礼节，指的是：
2.3 single file来完整地处理一整个算法流程（我们不是single file）
3.1 似乎只支持on-policy的系列算法（我们不只支持on-policy系列）
6.2 （并不通用的）外部支持其实不应该作为核心代码出现在一个库中 （env.py 不是核心代码）
</code></pre></div> <blockquote data-pid="eTabBWxD">5. 没有对done（Timelimit.truncate）的特殊处理</blockquote> <p data-pid="4v-kcbyw"><b>是的，这是我们做得不好</b>，我们后续会处理它。谢谢这个一针见血的建议。</p> <blockquote data-pid="6kws1Nwr">6.1 缺少基本的benchmark result，比如Atari和Mujoco（因为其实很多搞rL的人写论文基本上跑的除了自己弄的toy env之外就跑这几个benchmark）——事实上天授已经有对应的benchmark结果，比如可以在20分钟内用裸的DDQN来训练出一个PongNoFrameskip-v4（20+ reward），以及用1M的env step来超过spinning up提供的3M的benchmark结果（DDPG/TD3/SAC），详见<a href="http://link.zhihu.com/?target=https%3A//github.com/thu-ml/tianshou/tree/master/examples" class=" wrap external" target="_blank" rel="nofollow noreferrer">thu-ml/tianshou</a> </blockquote> <p data-pid="w5quzAY1">是的， 我们缺少基本的benchmark result。因为目前开源的DRL库中，支持多GPU的数量不多，所以我们希望在分布多GPU版本后发布后，再添加。我们不会使用收费的MuJoCo完成任何测试，这与开源精神相悖。<b>我们暂时不打算和 spinning up 这种教学代码比较性能</b>。我们将会和 实现了多GPU训练的库 比较（至少也和 实现了multi-workers的库 比较）。</p> <blockquote data-pid="mf3-TbDT">6.2 还有像比如FinanceStockEnv这种（并不通用的）外部支持其实不应该作为核心代码出现在一个库中、不支持其他额外的功能比如RNN（我看过你的文章说RNN不好这我同意）、stack obs之类的小问题就不详细展开了</blockquote> <p data-pid="Qj1usR7-"><b>FinanceStockEnv所在的env.py 不是核心代码</b>。选择FinanceStockEnv的理由：它只是作为一个例子，给自定义环境的用户一个可参照的Demo。它不依赖任何物理引擎，非常简单。此外，在DRL里通用的环境应该是什么？如果有更好的建议，我们会把通用的环境加入非核心代码 env.py</p> <blockquote data-pid="rbSP6w5X">-1. 请不要天天嘲讽天授，在嘲讽之前请先比较一下双方的优缺点。我承认之前天授问题确实还挺多的，但是经过我们慢慢地推进，大部分问题已经得到落实，并且它有完善的社区反馈和跟进（新issue第一次回复时间平均小于30分钟，所有commit以PR的形式走并且有code review机制），这也就是它的github star至今还在不断上涨的原因</blockquote> <p data-pid="LI2YkLCU">好的，我已经将<a href="https://zhuanlan.zhihu.com/p/358560363/http%3C/b%3Es://zhuanlan.zhihu.com/p/127792558" class="internal" rel="external nofollow noopener" target="_blank">「何须天授？彼可取而代也」用于入门的强化学习库 model-free RL in PyTorch</a><b> ，这篇文章的链接从文章开头移除，只保留了</b><a href="https://www.zhihu.com/question/384871366" class="internal" rel="external nofollow noopener" target="_blank">知乎：如何看待清华大学深度强化学习框架“天授”</a><b>。我们将会在更新多GPU版本后，参考PyTorch 以及 stable baselines 的Pull Requests 机制。在更新多GPU版本后再完善Pull Requests 机制很重要，这将保障Pull Requests 的代码不用在更新到分布式版本后由经历一轮大改，减轻历史包袱。</b></p> <hr> <blockquote data-pid="yaaBz-dr">曾伊言问：目前天授距离你的1400行版本是否渐行渐远？一开始我们做这些库不就是看到别的库代码又长又乱才写的(不包括rllib，它长但不乱)？ 我有一个群，里面有大量初学者，也有人私信我抱怨过看不懂那些复杂的依赖关系，你现在对天授的掌控力如何？<br>曾伊言问：我现在对elegantRL还有掌控力，因此我费尽心力推出了 tutorial版，单文件也是出于这方面的考虑。只要单文件内都是同类型的类，那么debug反而会简单。<br><a class="member_mention" href="http://www.zhihu.com/people/0a3cffd4427c90a63911fcff9c9c99b6" data-hash="0a3cffd4427c90a63911fcff9c9c99b6" data-hovercard="p$b$0a3cffd4427c90a63911fcff9c9c99b6" rel="external nofollow noopener" target="_blank">@Trinkle</a> 答：1.1.1 我确实想过这个问题，目前核心代码差不多是之前的两倍，但是实现的功能也差不多是之前的两倍，所以感觉这个应该还能接受……？如果不用其他功能的话可以不看对应的代码，不影响使用；并且外部的接口还是和原来几乎一致的。<br><a class="member_mention" href="http://www.zhihu.com/people/0a3cffd4427c90a63911fcff9c9c99b6" data-hash="0a3cffd4427c90a63911fcff9c9c99b6" data-hovercard="p$b$0a3cffd4427c90a63911fcff9c9c99b6" rel="external nofollow noopener" target="_blank">@Trinkle</a> 答：1.1.2“复杂的依赖关系”这一点我自认为现在并不复杂，至少和其他已有的popular RL repo比起来不复杂，当然和去年的天授比起来会复杂一些，但是想要支持一些新功能就得加一定的代码，这个是个tradeoff……你写到后面可能就会有感触了</blockquote> <p data-pid="-_A3tEoB">是的，功能增加将带来代码量的上升。你提及“并且<b>外部的接口还是和原来几乎一致</b>”，我前文说的“<b>历史包袱</b>”指的就是这个。当原先的接口千锤百炼，那么它就可以保持稳定 。回复同前文。</p> <blockquote data-pid="Ked1Xg3b">曾伊言问： -1.2 我之前总结过各类DRL算法，确认了: DRL算法只需要分三类(on-policy off-policy MCTS), 因而replay buffer 可以用灵活性去换性能。类似的api设计，这些需要一个懂整个DRL训练流程的人去掌控，这样的人很少，天授从一个人设计到多个人设计之后，这方面做得如何？升级api是大动筋骨，你应该发挥你的影响力，给这一块来一轮升级才行。还有很多想说的，我会写在其他地方<br><a class="member_mention" href="http://www.zhihu.com/people/0a3cffd4427c90a63911fcff9c9c99b6" data-hash="0a3cffd4427c90a63911fcff9c9c99b6" data-hovercard="p$b$0a3cffd4427c90a63911fcff9c9c99b6" rel="external nofollow noopener" target="_blank">@Trinkle</a> 答：1.1.3 以及个人建议是最好遵循一下标准的软件工程的方法（公开单元测试、拆分多文件之类的），已有的规范还是挺重要的。debug的容易程度不是一个人说的算的，而是所有（各种）用户的实际感受，不是只有初学者。（当然初学者友好也是挺重要的，我们额外处理了各种corner case和报错信息为的就是能够让他们用起来比较丝滑）<br><a class="member_mention" href="http://www.zhihu.com/people/0a3cffd4427c90a63911fcff9c9c99b6" data-hash="0a3cffd4427c90a63911fcff9c9c99b6" data-hovercard="p$b$0a3cffd4427c90a63911fcff9c9c99b6" rel="external nofollow noopener" target="_blank">@Trinkle</a> 答：1.1.4 我还是有掌控力的，所有的pr都需要通过我的审核</blockquote> <p data-pid="bNDvIDDw">好的，我们更新多GPU版本之后才会发布对应的模块，这与天授的安排不同。回复同前文。</p> <blockquote data-pid="hTdVY1eR"> <a class="member_mention" href="http://www.zhihu.com/people/0a3cffd4427c90a63911fcff9c9c99b6" data-hash="0a3cffd4427c90a63911fcff9c9c99b6" data-hovercard="p$b$0a3cffd4427c90a63911fcff9c9c99b6" rel="external nofollow noopener" target="_blank">@Trinkle</a> 答：1.2 buffer api已经全部升级过一次了，在寒假的时候，半个月之前发布的新版本就是为了这个。主要是另一个学弟和我一起修改，不会有一些完整性的缺失</blockquote> <p data-pid="VlM-8iBO">是的，这些改动都可以在Github上看到。</p> <p class="post-meta"> 1 min read   ·   March 21, 2021   ·   RL in Robotics (Zhihu) </p> <p class="post-tags"> <a href="/blog/2021"> <i class="fa-solid fa-calendar fa-sm"></i> 2021 </a> </p> </li> <li> <h3> <a class="post-title" href="https://zhuanlan.zhihu.com/p/358668613" target="_blank" rel="external nofollow noopener">PR Efficient Ⅰ：机器人中的数据高效强化学习</a> <svg width="2rem" height="2rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </h3> <p></p> <blockquote data-pid="8WGy5D3a">本文使用 <a href="https://zhuanlan.zhihu.com/p/106057556" class="internal" rel="external nofollow noopener" target="_blank">Zhihu On VSCode</a> 创作并发布<br>已经有四个多月没有写专栏了，甚至知乎也曾被我暂时卸载（知乎最近引入的一些内容十分分散精力、耽误学习）。这期间做了很多事，完成了华为实验室的项目，投了论文（关于Graph Relational Reasoning for Target-driven Navigation），顺便将它包装成了毕业论文的题目，博士申请也差不多有结果了。所以现在终于又有时间静下心学习新知识了，Cheers!</blockquote> <p data-pid="yVqTtZoq">尽管强化学习最近取得了许多成果，但我们仍然面临很多实际挑战，其中之一就是数据效率低下：在现实世界中的问题（例如，机器人）中，由于时间或硬件限制，并不能像在仿真器中（例如，video game learning）进行百万次实验。因此，在现实世界问题中，强化学习的数据高效性（data-efficiency）就显得尤为重要。就我所知，在这方面研究较为深刻的学者当属 UCL 的 Marc Deisenroth （很可惜实力不够，没申到他的博士）。Deisenroth 的成名作就是在机器人强化学习中经常使用的基线 <b>PILCO</b>，我之前也有专门的学习笔记。他对于利用 Gaussian Processes (GPs) 以及 Bayesian Model 等 Probabilistic Machine Learning 方法加速强化学习的学习过程、提高 sample/data efficiency 很有研究。</p> <p data-pid="4ey39Q1F">Probabilistic Robotics 系列的 Efficient 分章就从 Deisenroth 关于 data-efficient Reinforcement Learning 的 Talk 入手，开始阅读学习data-efficient领域的论文和最新研究成果。</p> <p data-pid="fv9RJHz3">Deisenroth 的 talk 链接：</p> <p data-pid="8dqbNWAo"><a href="http://link.zhihu.com/?target=https%3A//deisenroth.cc/talk/2019-05-13-vw/" class=" wrap external" target="_blank" rel="nofollow noreferrer">Talk link</a></p> <h2>强化学习在机器人领域的问题</h2> <p data-pid="OOEi1Ggy"><b>我们为什么需要 data-efficient RL？</b></p> <p data-pid="SSymnmAY">这就关乎这个方向是否有意义了。可能很多做理论研究的老师和同学会觉得，完全可以通过构建仿真器的方式，使现实问题可以在simulator中并行学习。至于Sim2Real问题，大家也更多地在考虑如何将仿真器做的更逼真，以及如何在仿真器内尽可能多得模拟现实世界的参数变化，从而缩小gap。（最近也会深入学习 Transfer Learning，毕竟几个月没能静下来学习了，如果有什么新进展大家可以提醒一下）然而，Deisenroth 指出 autonomous systems 应该是 No human in the loop 的，也就意味着理论上我们期望机器人能够 Learn directly from data。我的理解是，构建仿真器的这一环节，也迫使 human 投入了大量的精力，尤其是非标机器人，因此我们希望机器人能够直接通过自主的、与现实环境的交互学习策略，这很明显是 autonomous systems 的理想形式。</p> <p data-pid="E_yRaOFU"><b>什么是 data-efficient RL？</b></p> <p data-pid="edKgwfec">简单来说就是，机器人能够 learn from small dataset, or from few samples。（类似于 DL 中的 Few-shot learning）</p> <p data-pid="qU4RsOAr"><b>如何做到 data-efficient RL？</b></p> <p data-pid="Cj3Uqm4d">机器人问题无非是三种：<b>Modeling, Predicting, Decision making</b>。</p> <ul> <li data-pid="KsPoJMxv">Modeling 的能力影响了机器人对环境的理解程度，这个环境既包括 External 的环境，也包含机器人自身的运动学、动力学模型；</li> <li data-pid="JeJUF_pQ">Predicting 代表着给定一个控制序列，我们能够预测 long-term 的机器人行为；</li> <li data-pid="BGd9nRRp">Decision making 就是 control problem，也是RL关心的核心问题。</li> </ul> <p data-pid="Vll9OKiD">这其中会有很多的 Uncertainty：sensor noise, unknown processes, limited knowledge (due to the sparse data)，以上这些问题的解决方案就在于 Probabilistic Machine Learning。</p> <p data-pid="n7UkFuzZ">基于以上分析，Deisenroth 在Talk中给出了三种解决方案。</p> <h2>Model-based Reinforcement Learning</h2> <p><br></p> <figure data-size="normal"><img src="https://pica.zhimg.com/v2-d95999c1e60f40110ae3931c66be980e_720w.png?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p><br></p> <p data-pid="UuK3ZjNQ">RL 与最优控制的主要区别在于，最优控制中 transition function 是已知的，而 RL 中是未知的，且需要从 data 中学习。</p> <p data-pid="UwRG5-nQ">因此，对于可以处理不确定性的 Probabilistic model-based RL，可以分为以下几个步骤。<br></p> <figure data-size="normal"><img src="https://pica.zhimg.com/v2-7756af70a4413ed5c0812341513262e4_720w.png?source=d16d100b" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"><figcaption>Marc P. Deisenroth, Dieter Fox, Carl E. Rasmussen, Gaussian Processes for Data-Efficient Learning in Robotics and Control, IEEE Transactions on Pattern Analysis and Machine Intelligence, volume 37, pp. 408–423, 2015</figcaption></figure><p><br></p> <h3>Model Learning</h3> <p data-pid="07TC36Wc">其中最关键的就是Probabilistic model的学习。一旦学习得到的 model 有很大的 <b>errors</b>，势必会严重影响 Prediction 和 Decision Making 环节的准确性。这也是我在之前 MBRL 系列笔记中强调的 model-based 算法的关键问题。</p> <p><br></p> <figure data-size="normal"><img src="https://pic1.zhimg.com/v2-89182c79771e02176315bd1f3c1284ed_720w.png?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p data-pid="ZgWwareE"><br>因此，需要使用 Gaussian Process 来学习 model。关于 GPs 的知识就不在这里展开了，详见</p> <h3>Prediction</h3> <figure data-size="normal"><img src="https://pica.zhimg.com/v2-9697e3adf09cd9f4f42729fb1477f1c2_720w.png?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p data-pid="jyxrGFmk">通过对 GPs 预测模型以及 state 与 control 的分布的积分，我们可以得到红色的分布。然而这个分布是没法计算的，因此使用 moment matching (MM) 将这个分布近似为一个高斯分布。</p> <h3>Decision Making</h3> <figure data-size="normal"><img src="https://picx.zhimg.com/v2-be70ed3903aa0b6d4d5b40af02da55d6_720w.png?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p data-pid="13n1ziF_">根据 Prediction 的结果，我们就可以通过对序列时刻的分布积分得到 long-term loss，从而更新 policy。</p> <p data-pid="ZQkbIBNo">可以关注下图列出的论文</p> <figure data-size="normal"><img src="https://picx.zhimg.com/v2-6865ffa4d99f7e364f8538395c443fda_720w.png?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><h2>Safe Exploration</h2> <p data-pid="sibVzXkf">这一节是关于现实环境机器人自主探索中需要具备安全探索的能力，因为real-world robotics的实验一旦crash代价很大。<br></p> <figure data-size="normal"><img src="https://pic1.zhimg.com/v2-ca340309222fa8d9749d1b39e6fe3303_720w.png?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p data-pid="VlRmbvcT">具体的做法就是将 MPC 引入到 RL 中，预测状态分布，并同时判断其安全性。</p> <figure data-size="normal"><img src="https://picx.zhimg.com/v2-9c1b5f3aadac90ee43b76cd7629508b1_720w.png?source=d16d100b" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"><figcaption>Sanket Kamthe, Marc P. Deisenroth, Data-Efficient Reinforcement Learning with Probabilistic Model Predictive Control, Proceedings of the International the Conference on Artificial Intelligence and Statistics (AISTATS), 2018</figcaption></figure><figure data-size="normal"><img src="https://picx.zhimg.com/v2-b072c13436d85d78e81d3b20c48a9df9_720w.png?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><h2>Meta Reinforcement Learning</h2> <p data-pid="28dgW3Zz">针对不同机器人（参数不同）之间的 knowledge transfer，可以使用 Meta-learning 的思路，使学习得到的控制经验可以在不同类型的机器人之间共享。</p> <figure data-size="normal"><img src="https://pica.zhimg.com/v2-ffa698d9e56ea0e0684ac0b2787c6e00_720w.png?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p data-pid="yJnE6x73">具体地，就是将机器人不同的参数配置作为隐变量加入到参数学习之中，使 GPs 能够得到动力学的全局特性，从而在新的机器人上快速的推断出相应的 GPs model。</p> <figure data-size="normal"><img src="https://pic1.zhimg.com/v2-5cf4c21157656481d135cb955fdff317_720w.png?source=d16d100b" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"><figcaption>Steindór Sæmundsson, Katja Hofmann, Marc P. Deisenroth, Meta Reinforcement Learning with Latent Variable Gaussian Processes, Proceedings of the International the Conference on Uncertainty in Artificial Intelligence (UAI), 2018</figcaption></figure><h2>Conclusion</h2> <figure data-size="normal"><img src="https://picx.zhimg.com/v2-21d8e9fc15f81d2f7de96a08a91ff0c8_720w.png?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p></p> <p class="post-meta"> 1 min read   ·   March 20, 2021   ·   RL in Robotics (Zhihu) </p> <p class="post-tags"> <a href="/blog/2021"> <i class="fa-solid fa-calendar fa-sm"></i> 2021 </a> </p> </li> <li> <h3> <a class="post-title" href="https://zhuanlan.zhihu.com/p/338404758" target="_blank" rel="external nofollow noopener">Awesome Target-driven Navigation 机器人目标驱动导航资源汇总</a> <svg width="2rem" height="2rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </h3> <p></p> <blockquote data-pid="DdkXOcnN"><p data-pid="axwiGBB1">本文使用 <a href="https://zhuanlan.zhihu.com/p/106057556" class="internal" rel="external nofollow noopener" target="_blank">Zhihu On VSCode</a> 创作并发布</p></blockquote> <hr> <p> </p> <p data-pid="lYbO362W">知乎没法显示Summary部分的折叠和表格内部跳转，还不支持目录，格式真的太丑了，<strong>再次请求支持完整版的 Markdown + HTML！！</strong><a href="https://www.zhihu.com/people/zhihuadmin" class="internal" rel="external nofollow noopener" target="_blank">@知乎小管家</a></p> <p data-pid="rSCVneEL">欢迎移步 Github 观看正常版，<strong>记得 Fork &amp; Star 哦！</strong></p> <p data-pid="tAahNWLn"><a href="http://link.zhihu.com/?target=https%3A//github.com/Skylark0924/Awesome-Target-driven-Navigation" class=" wrap external" target="_blank" rel="nofollow noreferrer">Awesome-Target-driven-Navigation/Skylark0924</a></p> <hr> <p> </p> <p></p> <figure data-size="normal"><img src="https://picx.zhimg.com/v2-4a1ad14ca89b7bc01bdaec68fd2ba086_720w.jpg?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p></p> <p data-pid="ZMuaXaM-"><img src="https://www.zhihu.com/equation?tex=%5E%5Cdagger" alt="^\dagger" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer"> For methods designed for single room navigation, I use the result about a subset of targets whose optimal trajectory length is greater than 5 (<img src="https://www.zhihu.com/equation?tex=L%5Cge+5" alt="L\ge 5" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer">).</p> <h2>ImageGoal Navigation</h2> <ol> <li data-pid="JREWcYzu"> <p data-pid="VZh1YEpu"><strong>Target-driven Visual Navigation in Indoor Scenes using Deep Reinforcement Learning (t-SNE)</strong></p> <p data-pid="gY7AXJvs"><em>Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph J. Lim, Abhinav Gupta, Li Fei-Fei, Ali Farhadi</em> <br> ICRA, 2017. [<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/abs/1609.05143" class=" wrap external" target="_blank" rel="nofollow noreferrer">Paper]</a> [<a href="http://link.zhihu.com/?target=https%3A//prior.allenai.org/projects/target-driven-visual-navigation" class=" wrap external" target="_blank" rel="nofollow noreferrer">Website]</a></p> </li> <li data-pid="vfmGbbqU"> <p data-pid="lkYYv78I"><strong>Semi-Parametric Topological Memory for Navigation (SPTM)</strong></p> <p data-pid="oHi2oa0C"><em>Nikolay Savinov*, Alexey Dosovitskiy*, Vladlen Koltun</em> <br> ICLR, 2018. [<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1803.00653.pdf" class=" wrap external" target="_blank" rel="nofollow noreferrer">Paper]</a> [<a href="http://link.zhihu.com/?target=https%3A//github.com/nsavinov/SPTM" class=" wrap external" target="_blank" rel="nofollow noreferrer">Code]</a> [<a href="http://link.zhihu.com/?target=https%3A//sites.google.com/view/SPTM" class=" wrap external" target="_blank" rel="nofollow noreferrer">Website]</a></p> </li> <li data-pid="87bnaUyI"> <p data-pid="cAflBgkp"><strong>Neural Topological SLAM for Visual Navigation (NTS)</strong></p> <p data-pid="ZsjGeuQn"><em>Devendra Singh Chaplot, Ruslan Salakhutdinov, Abhinav Gupta, Saurabh Gupta</em> <br> CVPR, 2020. [<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2005.12256.pdf" class=" wrap external" target="_blank" rel="nofollow noreferrer">Paper]</a> [<a href="http://link.zhihu.com/?target=https%3A//devendrachaplot.github.io/projects/Neural-Topological-SLAM" class=" wrap external" target="_blank" rel="nofollow noreferrer">Website]</a></p> </li> <li data-pid="yQo6JHKS"> <p data-pid="wH4HVfiI"><strong>Learning Your Way Without Map or Compass: Panoramic Target Driven Visual Navigation (PTVN)</strong></p> <p data-pid="3XnpWcRv"><em>David Watkins-Valls, Jingxi Xu, Nicholas Waytowich, Peter Allen</em><br> [<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1909.09295.pdf" class=" wrap external" target="_blank" rel="nofollow noreferrer">Paper</a>]</p> </li> </ol> <h2>LabelGoal Navigation</h2> <ol> <li data-pid="aTS05t9p"> <p data-pid="k2tbUhmc"><strong>Cognitive Mapping and Planning for Visual Navigation (CMP)</strong></p> <p data-pid="hAQZ608J"><em>Saurabh Gupta, Varun Tolani, James Davidson, Sergey Levine, Rahul Sukthankar, Jitendra Malik</em> <br> CVPR, 2017. [<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/abs/1702.03920" class=" wrap external" target="_blank" rel="nofollow noreferrer">Paper]</a></p> Summary<p data-pid="BwII5OdP">Two key ideas:</p> <ul> <li data-pid="xRkJTlbS">a unified joint architecture for mapping and planning, such that the mapping is driven by the needs of the task;</li> <li data-pid="jHWL6bZ0">a spatial memory with the ability to plan given an incomplete set of observations about the world.</li> </ul> <p data-pid="9wNkUkgN">CMP constructs a <strong>top-down belief map</strong> of the world and applies a <strong>differentiable neural net planner</strong> to produce the next action at each time step.</p> <p data-pid="cNxGFqiA"><strong>Network architecture</strong></p> <p></p> <figure data-size="normal"><img src="https://picx.zhimg.com/v2-43552d5cb2a856237915379ef05291cd_720w.jpg?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p></p> <p data-pid="-16SVSWB"><strong>Architecture of the mapper</strong></p> <p></p> <figure data-size="normal"><img src="https://pic1.zhimg.com/v2-d69520e9008cc8c8ba2e9d302d2e0618_720w.jpg?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p></p> <p data-pid="l1Upq76_"><strong>Architecture of the hierarchical planner</strong></p> <p></p> <figure data-size="normal"><img src="https://pic1.zhimg.com/v2-d07a07b61fe7c345d860729f385adb9f_720w.jpg?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p></p> </li> <li data-pid="fIt3_Cl3"> <p data-pid="S-tHHiod"><strong>Visual Semantic Navigation using Scene Priors (Scene Priors)</strong></p> <p data-pid="Ga9uW0A8"><em>Wei Yang, Xiaolong Wang, Ali Farhadi, Abhinav Gupta, Roozbeh Mottaghi</em> <br> ICLR, 2019. [<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/abs/1810.06543" class=" wrap external" target="_blank" rel="nofollow noreferrer">Paper]</a></p> Summary<p data-pid="y3iGhWTZ">They address navigation to <strong>novel objects</strong> or navigating in <strong>unseen scenes</strong> using <strong>scene priors</strong>, like human does.</p> <p data-pid="l7h4TUp1"><strong>Scene Priors</strong></p> <p></p> <figure data-size="normal"><img src="https://pic1.zhimg.com/v2-85dd34e75e15aa6dd8340df9b394a40a_720w.jpg?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p></p> <p data-pid="z1k8DPMQ"><strong>Architecture</strong></p> <p></p> <figure data-size="normal"><img src="https://picx.zhimg.com/v2-f9aa32fe76ee17e120791f3dc8d95c3b_720w.jpg?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p></p> <p data-pid="gOD5_XPn"><strong>GCN for relation graph embedding</strong></p> <p></p> <figure data-size="normal"><img src="https://pic1.zhimg.com/v2-b3c5349393c469c2ebfeb55d9976cc31_720w.jpg?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p></p> </li> <li data-pid="SFDBPZ1c"> <p data-pid="mVYCDlEX"><strong>Visual Representations for Semantic Target Driven Navigation (VR)</strong></p> <p data-pid="suewwRde"><em>Arsalan Mousavian, Alexander Toshev, Marek Fiser, Jana Kosecka, Ayzaan Wahid, James Davidson</em> <br> ICRA, 2019. [<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1805.06066.pdf" class=" wrap external" target="_blank" rel="nofollow noreferrer">Paper]</a> [<a href="http://link.zhihu.com/?target=https%3A//github.com/arsalan-mousavian/Navigation" class=" wrap external" target="_blank" rel="nofollow noreferrer">Code]</a></p> Summary<p data-pid="uaUXjm7E">This paper focuses on finding a good <strong>visual representation.</strong></p> <p></p> <figure data-size="normal"><img src="https://picx.zhimg.com/v2-673402901ca447c11dc3e97c6f8acd29_720w.jpg?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p></p> </li> <li data-pid="rmLEdMg_"> <p data-pid="oQpuoql-"><strong>Learning to Learn How to Learn: Self-Adaptive Visual Navigation using Meta-Learning (SAVN)</strong></p> <p data-pid="PSF7K9yn"><em>Mitchell Wortsman, Kiana Ehsani, Mohammad Rastegari, Ali Farhadi, Roozbeh Mottaghi</em> <br> CVPR, 2019. [<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/abs/1812.00971" class=" wrap external" target="_blank" rel="nofollow noreferrer">Paper]</a> [<a href="http://link.zhihu.com/?target=https%3A//github.com/allenai/savn" class=" wrap external" target="_blank" rel="nofollow noreferrer">Code]</a> [<a href="http://link.zhihu.com/?target=https%3A//prior.allenai.org/projects/savn" class=" wrap external" target="_blank" rel="nofollow noreferrer">Website]</a></p> Summary<p data-pid="8ZDJ-v9t">This paper uses <strong>Meta-reinforcement learning</strong> to construct an interaction loss for self-adaptive visual navigation.</p> <p></p> <figure data-size="normal"><img src="https://picx.zhimg.com/v2-4e26da50005e28427cb2569d4b8f2667_720w.jpg?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p></p> </li> <li data-pid="vTQEbN6T"> <p data-pid="KXNKoCiI"><strong>Bayesian Relational Memory for Semantic Visual Navigation (BRM)</strong></p> <p data-pid="ojCCbRgo"><em>Yi Wu, Yuxin Wu, Aviv Tamar, Stuart Russell, Georgia Gkioxari, Yuandong Tian</em> <br> ICCV, 2019. [<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/abs/1909.04306" class=" wrap external" target="_blank" rel="nofollow noreferrer">Paper]</a> [<a href="http://link.zhihu.com/?target=https%3A//github.com/jxwuyi/HouseNavAgent" class=" wrap external" target="_blank" rel="nofollow noreferrer">Code]</a></p> Summary<p data-pid="V51OqCMS">Construct a probabilistic relation graph to learn the relationship or a topological memory of house layout.</p> <p></p> <figure data-size="normal"><img src="https://pica.zhimg.com/v2-096989e15a9789eed2c50e2a00aff508_720w.jpg?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p></p> </li> <li data-pid="L8tVMKVo"> <p data-pid="6pTOVNpV"><strong>Situational Fusion of Visual Representation for Visual Navigation (SF)</strong></p> <p data-pid="4hnGmEyR"><em>William B. Shen, Danfei Xu, Yuke Zhu, Leonidas J. Guibas, Li Fei-Fei, Silvio Savarese</em> <br> ICCV, 2019. [<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/abs/1908.09073" class=" wrap external" target="_blank" rel="nofollow noreferrer">Paper]</a></p> Summary<p data-pid="_xigB3oD">This paper aims at fusing multiple visual representations, such as Semantic Segment, Depth Perception, Object Class, Room Layout and Scene Class.</p> <p data-pid="eIoAhWdx">They develop an action-level representation fusion scheme, which predicts an action candidate from each representation and adaptively consolidate these action candidates into the final action.</p> <p></p> <figure data-size="normal"><img src="https://picx.zhimg.com/v2-0a7a4ad75466ef027e0409f000e662ca_720w.jpg?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p></p> <p></p> <figure data-size="normal"><img src="https://pica.zhimg.com/v2-1a442d44e9eb37dd851cd6fe4a1d1160_720w.jpg?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p></p> </li> <li data-pid="DNlB901H"> <p data-pid="cQv8-RsC"><strong>Learning Object Relation Graph and Tentative Policy for Visual Navigation (ORG)</strong></p> <p data-pid="4IxK_UYU"><em>Heming Du, Xin Yu, Liang Zheng</em> <br> ECCV, 2020. [<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/abs/2007.11018" class=" wrap external" target="_blank" rel="nofollow noreferrer">Paper]</a></p> Summary<p data-pid="7Unqf9Hl">Aiming to learn informative visual representation and robust navigation policy, this paper proposes three complementary techniques, <strong>object relation graph</strong> (ORG), <strong>trial-driven imitation learning</strong> (IL), and a memory-augmented <strong>tentative policy network</strong> (TPN).</p> <ul> <li data-pid="tDVM5xRK">ORG improves visual representation learning by integrating object relationships;</li> <li data-pid="x5Rd-8R1">Both Trial-driven IL and TPN underlie robust navigation policy, in- structing the agent to escape from deadlock states, such as looping or being stuck;</li> <li data-pid="Iq7xW8Vh">IL is used in training, TPN for testing.</li> </ul> <p></p> <figure data-size="normal"><img src="https://picx.zhimg.com/v2-693ed6de217c1fe839c99605fc602fab_720w.jpg?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p></p> </li> <li data-pid="UlOjJaQV"> <p data-pid="HWRhk8ZG"><strong>Target driven visual navigation exploiting object relationships (MJOLNIR)</strong></p> <p data-pid="zpi0pniY"><em>Yiding Qiu, Anwesan Pal, Henrik I. Christensen</em> <br> arxiv, 2020. [<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/abs/2003.06749" class=" wrap external" target="_blank" rel="nofollow noreferrer">Paper]</a></p> Summary<p data-pid="4axYRJfd">They present Memory-utilized Joint hierarchical Object Learning for Navigation in Indoor Rooms (MJOLNIR)1, a target-driven visual navigation algorithm, which considers the inherent relationship between target objects, along with the more salient parent objects occurring in its surrounding.</p> <p data-pid="9VwM9wiz"><strong>MJOLNIR architecture</strong></p> <figure data-size="normal"><img src="https://picx.zhimg.com/v2-33081b0bf51da76462fe1f49044d9929_720w.jpg?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p></p> <p data-pid="RoIEbILV"><strong>The novel CGN architecture</strong></p> <figure data-size="normal"><img src="https://pica.zhimg.com/v2-1d805e391f0af5c1962b2acee12d8878_720w.jpg?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p></p> </li> <li data-pid="Il_TGrYa"> <p data-pid="zAy4KV7d"><strong>Improving Target-driven Visual Navigation with Attention on 3D Spatial Relationships (Attention 3D)</strong></p> <p data-pid="ySpbebS7"><em>Yunlian Lv, Ning Xie, Yimin Shi, Zijiao Wang, and Heng Tao Shen</em> <br> arxiv, 2020. [<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/abs/2005.02153" class=" wrap external" target="_blank" rel="nofollow noreferrer">Paper]</a></p> Summary<p data-pid="Khz3pKbA">To address the <strong>generalization and automatic obstacle avoidance</strong> issues, we incorporate two designs into classic DRL framework: attention on 3D knowledge graph (KG) and target skill extension (TSE) module.</p> <ul> <li data-pid="wmTb5Ot7">visual features and 3D spatial representations to learn navigation policy;</li> <li data-pid="anamNPab">TSE module is used to generate sub-targets which allow agent to learn from failures.</li> </ul> <p data-pid="aiKkZ9uu"><strong>Framework</strong></p> <p></p> <figure data-size="normal"><img src="https://picx.zhimg.com/v2-dce76c5ad0153abafd662d820c271eb6_720w.jpg?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p></p> <p data-pid="lxZyqUd0"><strong>3D spatial representations</strong></p> <p></p> <figure data-size="normal"><img src="https://picx.zhimg.com/v2-73025be7a1ebb683f612e03778706e96_720w.jpg?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p></p> </li> <li data-pid="8PXgRkpV"> <p data-pid="sSm9fbGV"><strong>Semantic Visual Navigation by Watching YouTube Videos (YouTube)</strong></p> <p data-pid="uB00zIKY"><em>Matthew Chang, Arjun Gupta, Saurabh Gupta</em> <br> arXiv, 2020. [<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2006.10034.pdf" class=" wrap external" target="_blank" rel="nofollow noreferrer">Paper]</a> [<a href="http://link.zhihu.com/?target=https%3A//matthewchang.github.io/value-learning-from-videos/" class=" wrap external" target="_blank" rel="nofollow noreferrer">Website]</a></p> Summary<p data-pid="_3SrZuI_">This paper <strong>learns and leverages such semantic cues</strong> for navigating to objects of interest in novel environments, <strong>by simply watching YouTube videos</strong>. They believe that these priors can improve efficiency for navigation in novel environments.</p> <p></p> <figure data-size="normal"><img src="https://pic1.zhimg.com/v2-a70ca29ea708b48b47735fcdd1b0c813_720w.jpg?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p></p> </li> <li data-pid="faoNGdQm"> <p data-pid="641LnGKf"><strong>Object Goal Navigation using Goal-Oriented Semantic Exploration (SemExp)</strong></p> <p data-pid="pSGjT0Ox"><em>Devendra Singh Chaplot, Dhiraj Gandhi, Abhinav Gupta*, Ruslan Salakhutdinov</em> <br> arXiv, 2020. [<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2007.00643.pdf" class=" wrap external" target="_blank" rel="nofollow noreferrer">Paper]</a> [<a href="http://link.zhihu.com/?target=https%3A//devendrachaplot.github.io/projects/semantic-exploration" class=" wrap external" target="_blank" rel="nofollow noreferrer">Website]</a></p> <p data-pid="RadJC4lp"><strong>Win CVPR2020 Habitat ObjectNav Challenge</strong></p> Summary<p data-pid="DzWm2mAg">They propose a modular system called, ‘<strong>Goal- Oriented Semantic Exploration (SemExp)</strong>’ which builds an episodic semantic map and uses it to explore the environment efficiently based on the goal object category.</p> <ul> <li data-pid="nmVrFy44">It builds top-down metric maps, which adds extra channels to encode semantic categories explicitly;</li> <li data-pid="bNRtrMNn">Instead of using a coverage maximizing goal-agnostic exploration policy based only on obstacle maps, we train a goal-oriented semantic exploration policy which learns semantic priors for efficient navigation.</li> </ul> <p data-pid="tG_wgH3H"><strong>Framework</strong></p> <p></p> <figure data-size="normal"><img src="https://picx.zhimg.com/v2-b1e9eed7277390841b0f8e5c22bd54c9_720w.jpg?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p></p> <p data-pid="BM7YFAh6"><strong>Semantic Mapping</strong></p> <p></p> <figure data-size="normal"><img src="https://picx.zhimg.com/v2-c397c2ee6e0e18eb5b3f93d31a25d7f3_720w.jpg?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p></p> </li> <li data-pid="vBahvBa-"> <p data-pid="SoCgw8EH"><strong>ObjectNav Revisited: On Evaluation of Embodied Agents Navigating to Objects</strong></p> <p data-pid="7PWHaI-S"><em>Dhruv Batra, Aaron Gokaslan, Aniruddha Kembhavi, Oleksandr Maksymets, Roozbeh Mottaghi, Manolis Savva, Alexander Toshev, Erik Wijmans</em> <br> arXiv, 2020. [<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/abs/2006.13171" class=" wrap external" target="_blank" rel="nofollow noreferrer">Paper]</a></p> Summary<p data-pid="0TusPxTF"><strong>This paper is not a research paper.</strong> They summarize the ObjectNav task and introduce popular datasets (Matterport3D, AI2-THOR) and Challenges (Habitat 2020 Challenge Habitat, RoboTHOR 2020 Challenge RoboTHOR).</p> </li> </ol> <p class="post-meta"> 1 min read   ·   December 21, 2020   ·   RL in Robotics (Zhihu) </p> <p class="post-tags"> <a href="/blog/2020"> <i class="fa-solid fa-calendar fa-sm"></i> 2020 </a> </p> </li> <li> <h3> <a class="post-title" href="https://zhuanlan.zhihu.com/p/100210903" target="_blank" rel="external nofollow noopener">Pycharm本地编程，远程执行，如何本地调用服务端图形界面</a> <svg width="2rem" height="2rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </h3> <p></p> <p data-pid="cTyYJnEj">问题描述：现在所有的程序都会在Linux服务器上运行，但是一般会在Win主机上编写，因为开发效率比较高，所以一般会出现一个问题，本地Win上通过Pycharm编译代码，在远端Linux上使用Python的解释器进行执行的时候，怎么在本地出现远端的服务器的图形化界面呢？</p> <p data-pid="7MQQhJ6Q">问题的解决：Pycharm+xming</p> <ol><li data-pid="xGqL5vGB">Pycharm可以去官网直接下载，社区或者付费版都可以</li></ol> <a href="http://link.zhihu.com/?target=http%3A//www.jetbrains.com/" data-draft-node="block" data-draft-type="link-card" data-image="https://picx.zhimg.com/v2-59e81b8d97279ce803fb7ecb70e8d3ec_qhd.jpg?source=d16d100b" data-image-width="1280" data-image-height="800" class=" wrap external" target="_blank" rel="nofollow noreferrer">JetBrains: Developer Tools for Professionals and Teams</a><p data-pid="kzg1LeQZ">2. Xming下载安装，下载链接链接：<a href="http://link.zhihu.com/?target=https%3A//pan.baidu.com/s/1zDu4rXlLnv7dqRiW9eKqsQ" class=" external" target="_blank" rel="nofollow noreferrer"><span class="invisible">https://</span><span class="visible">pan.baidu.com/s/1zDu4rX</span><span class="invisible">lLnv7dqRiW9eKqsQ</span><span class="ellipsis"></span></a> </p> <p data-pid="d_AEnmhD">提取码：dh76 </p> <p data-pid="58gAXb1s">Xming的安装</p> <figure data-size="normal"><img src="https://pic1.zhimg.com/v2-eff304cceae0298c14e10c602e932889_720w.jpg?source=d16d100b" data-caption="" data-size="normal" data-rawwidth="663" data-rawheight="471" class="origin_image zh-lightbox-thumb" width="663" data-original="https://picx.zhimg.com/v2-eff304cceae0298c14e10c602e932889_720w.jpg?source=d16d100b" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-331f5465c19cdc62fb3df05539314e2e_720w.jpg?source=d16d100b" data-caption="" data-size="normal" data-rawwidth="660" data-rawheight="472" class="origin_image zh-lightbox-thumb" width="660" data-original="https://pic1.zhimg.com/v2-331f5465c19cdc62fb3df05539314e2e_720w.jpg?source=d16d100b" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p data-pid="HUBsMxmM">其他配置保持默认即可，安装完成可以打开XLaunch</p> <figure data-size="normal"><img src="https://pic1.zhimg.com/v2-f3d06945818d87d4bccbf52dd9b3fa1d_720w.jpg?source=d16d100b" data-caption="" data-size="normal" data-rawwidth="393" data-rawheight="256" class="content_image" width="393" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-71a0d6bfbb49f8b9be84194ab3157721_720w.jpg?source=d16d100b" data-caption="" data-size="normal" data-rawwidth="637" data-rawheight="515" class="origin_image zh-lightbox-thumb" width="637" data-original="https://picx.zhimg.com/v2-71a0d6bfbb49f8b9be84194ab3157721_720w.jpg?source=d16d100b" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p data-pid="auhAFBAy">保证是0即可</p> <p data-pid="LMTnqZYU">3. Pycharm的设置</p> <p data-pid="Hy-qVFrk">获取Linux服务器的环境变量</p> <figure data-size="normal"><img src="https://picx.zhimg.com/v2-f4cb1f31993ff7df286567004c49b1cd_720w.jpg?source=d16d100b" data-caption="" data-size="normal" data-rawwidth="996" data-rawheight="649" class="origin_image zh-lightbox-thumb" width="996" data-original="https://pica.zhimg.com/v2-f4cb1f31993ff7df286567004c49b1cd_720w.jpg?source=d16d100b" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p data-pid="kezoFFXo">设置Pycharm</p> <figure data-size="normal"><img src="https://picx.zhimg.com/v2-6f202ff7dd08efc53c855c15ca8d00d9_720w.jpg?source=d16d100b" data-caption="" data-size="normal" data-rawwidth="479" data-rawheight="493" class="origin_image zh-lightbox-thumb" width="479" data-original="https://picx.zhimg.com/v2-6f202ff7dd08efc53c855c15ca8d00d9_720w.jpg?source=d16d100b" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><figure data-size="normal"><img src="https://picx.zhimg.com/v2-413655feab5ef52a01c466adc6c1a9b9_720w.jpg?source=d16d100b" data-caption="" data-size="normal" data-rawwidth="1345" data-rawheight="853" class="origin_image zh-lightbox-thumb" width="1345" data-original="https://pic1.zhimg.com/v2-413655feab5ef52a01c466adc6c1a9b9_720w.jpg?source=d16d100b" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><figure data-size="normal"><img src="https://pica.zhimg.com/v2-5b04df2b404491b54be3bfc4ac985201_720w.jpg?source=d16d100b" data-caption="" data-size="normal" data-rawwidth="1269" data-rawheight="886" class="origin_image zh-lightbox-thumb" width="1269" data-original="https://pic1.zhimg.com/v2-5b04df2b404491b54be3bfc4ac985201_720w.jpg?source=d16d100b" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-e8e4d0e5c64f35e8d5362efb4d17c6e7_720w.jpg?source=d16d100b" data-caption="" data-size="normal" data-rawwidth="853" data-rawheight="690" class="origin_image zh-lightbox-thumb" width="853" data-original="https://pic1.zhimg.com/v2-e8e4d0e5c64f35e8d5362efb4d17c6e7_720w.jpg?source=d16d100b" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p data-pid="vrEmO1cb">这样，就可以直接在Pycharm中执行脚本，弹出服务器端的图形界面了。</p> <p class="post-meta"> 1 min read   ·   December 30, 2019   ·   RL in Robotics (Zhihu) </p> <p class="post-tags"> <a href="/blog/2019"> <i class="fa-solid fa-calendar fa-sm"></i> 2019 </a> </p> </li> </ul> <nav aria-label="Blog page naviation"> <ul class="pagination pagination-lg justify-content-center"> <li class="page-item "> <a class="page-link" href="/blog/page/4/" tabindex="-1" aria-disabled="4">Newer</a> </li> <li class="page-item "> <a class="page-link" href="/blog/index.html" title="Blog">1</a> </li> <li class="page-item "> <a class="page-link" href="/blog/page/2/index.html" title="Blog - page 2">2</a> </li> <li class="page-item "> <a class="page-link" href="/blog/page/3/index.html" title="Blog - page 3">3</a> </li> <li class="page-item "> <a class="page-link" href="/blog/page/4/index.html" title="Blog - page 4">4</a> </li> <li class="page-item active"> <a class="page-link" href="/blog/page/5/index.html" title="Blog - page 5">5</a> </li> <li class="page-item disabled"> <a class="page-link" href="">Older</a> </li> </ul> </nav> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Junjia LIU. Last updated: March 30, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?5d75c11f89cd96294bf5e6dd1ee1bb30"></script> <script defer src="/assets/js/common.js?fcfacfb8c6281f5e68d5a7d348186eb1"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>