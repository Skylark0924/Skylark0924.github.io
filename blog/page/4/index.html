<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Blog - page 4 | Junjia LIU </title> <meta name="author" content="Junjia LIU"> <meta name="description" content=""> <meta name="keywords" content="Robotics, Embodied AI, Humanoid Robots"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://skylark0924.github.io/blog/page/4/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Junjia LIU </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <div class="header-bar"> <h1>Junjia LIU's blog</h1> <h2></h2> </div> <div class="tag-category-list"> <ul class="p-0 m-0"> <li> <i class="fa-solid fa-hashtag fa-sm"></i> <a href="/blog/tag/formatting">formatting</a> </li> <p>•</p> <li> <i class="fa-solid fa-hashtag fa-sm"></i> <a href="/blog/tag/images">images</a> </li> <p>•</p> <li> <i class="fa-solid fa-hashtag fa-sm"></i> <a href="/blog/tag/links">links</a> </li> <p>•</p> <li> <i class="fa-solid fa-hashtag fa-sm"></i> <a href="/blog/tag/math">math</a> </li> <p>•</p> <li> <i class="fa-solid fa-hashtag fa-sm"></i> <a href="/blog/tag/code">code</a> </li> <p>•</p> <li> <i class="fa-solid fa-tag fa-sm"></i> <a href="/blog/category/blockquotes">blockquotes</a> </li> </ul> </div> <ul class="post-list"> <li> <h3> <a class="post-title" href="https://zhuanlan.zhihu.com/p/368668837" target="_blank" rel="external nofollow noopener">Tools 7：Python字体、背景、绘图颜色设置以及强迫症中文对齐</a> <svg width="2rem" height="2rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </h3> <p></p> <blockquote data-pid="UHZSdb-K">本文使用 <a href="https://zhuanlan.zhihu.com/p/106057556" class="internal" rel="external nofollow noopener" target="_blank">Zhihu On VSCode</a> 创作并发布<br>工欲善其事，必先利其器</blockquote> <p data-pid="jcbaRKPy">本文是为了备忘，整理一下常用的python颜色显示小工具。</p> <h2>Python字体、背景颜色</h2> <p data-pid="yp38_D8S">字体、背景颜色<b>转发自：<a href="http://link.zhihu.com/?target=https%3A//blog.csdn.net/ever_peng/article/details/91492491" class=" external" target="_blank" rel="nofollow noreferrer"><span class="invisible">https://</span><span class="visible">blog.csdn.net/ever_peng</span><span class="invisible">/article/details/91492491</span><span class="ellipsis"></span></a></b></p> <p data-pid="N2v5f1qX">试了上面链接之后发现，输出的颜色背景不对齐。作为一个强迫症，这是不被允许的。试过了<code>%.10s</code>和<code>%-10s</code>以及<code>str({}).ljust(10)</code>都一样，最后发现问题出在中文字符上，补齐时一个中文算一个字符，打印出来却成了两个字符。所以，我又找到了下面的专治中文对齐的妙招：</p> <p data-pid="MwIRkaLS"><a href="http://link.zhihu.com/?target=https%3A//blog.csdn.net/weixin_42280517/article/details/80814677" class=" external" target="_blank" rel="nofollow noreferrer"><span class="invisible">https://</span><span class="visible">blog.csdn.net/weixin_42</span><span class="invisible">280517/article/details/80814677</span><span class="ellipsis"></span></a></p> <p data-pid="JJjtTJwY">顺便，把代码改成了方便复制的形式。</p> <div class="highlight"><pre><code class="language-python"><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\033</span><span class="s2">[1;30m</span><span class="si">{}</span><span class="se">\033</span><span class="s2">[0m"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s1">' 字体颜色：白色'</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\033</span><span class="s2">[1;31m</span><span class="si">{}</span><span class="se">\033</span><span class="s2">[0m"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s1">' 字体颜色：红色'</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\033</span><span class="s2">[1;32m</span><span class="si">{}</span><span class="se">\033</span><span class="s2">[0m"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s1">' 字体颜色：深黄色'</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\033</span><span class="s2">[1;33m</span><span class="si">{}</span><span class="se">\033</span><span class="s2">[0m"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s1">' 字体颜色：浅黄色'</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\033</span><span class="s2">[1;34m</span><span class="si">{}</span><span class="se">\033</span><span class="s2">[0m"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s1">' 字体颜色：蓝色'</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\033</span><span class="s2">[1;35m</span><span class="si">{}</span><span class="se">\033</span><span class="s2">[0m"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s1">' 字体颜色：淡紫色'</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\033</span><span class="s2">[1;36m</span><span class="si">{}</span><span class="se">\033</span><span class="s2">[0m"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s1">' 字体颜色：青色'</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\033</span><span class="s2">[1;37m</span><span class="si">{}</span><span class="se">\033</span><span class="s2">[0m"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s1">' 字体颜色：灰色'</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\033</span><span class="s2">[1;38m</span><span class="si">{}</span><span class="se">\033</span><span class="s2">[0m"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s1">' 字体颜色：浅灰色'</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'{0:</span><span class="si">{1}</span><span class="s1">&lt;9} </span><span class="se">\033</span><span class="s1">[1;40m</span><span class="si">{2}</span><span class="se">\033</span><span class="s1">[0m</span><span class="se">\n</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s1">'背景颜色：白色'</span><span class="p">,</span> <span class="nb">chr</span><span class="p">(</span><span class="mi">12288</span><span class="p">),</span> <span class="s1">'    '</span><span class="p">),</span> <span class="n">end</span><span class="o">=</span><span class="s1">''</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'{0:</span><span class="si">{1}</span><span class="s1">&lt;9} </span><span class="se">\033</span><span class="s1">[1;41m</span><span class="si">{2}</span><span class="se">\033</span><span class="s1">[0m</span><span class="se">\n</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s1">'背景颜色：红色'</span><span class="p">,</span> <span class="nb">chr</span><span class="p">(</span><span class="mi">12288</span><span class="p">),</span> <span class="s1">'    '</span><span class="p">),</span> <span class="n">end</span><span class="o">=</span><span class="s1">''</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'{0:</span><span class="si">{1}</span><span class="s1">&lt;9} </span><span class="se">\033</span><span class="s1">[1;42m</span><span class="si">{2}</span><span class="se">\033</span><span class="s1">[0m</span><span class="se">\n</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s1">'背景颜色：深黄色'</span><span class="p">,</span> <span class="nb">chr</span><span class="p">(</span><span class="mi">12288</span><span class="p">),</span> <span class="s1">'    '</span><span class="p">),</span> <span class="n">end</span><span class="o">=</span><span class="s1">''</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'{0:</span><span class="si">{1}</span><span class="s1">&lt;9} </span><span class="se">\033</span><span class="s1">[1;43m</span><span class="si">{2}</span><span class="se">\033</span><span class="s1">[0m</span><span class="se">\n</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s1">'背景颜色：浅黄色'</span><span class="p">,</span> <span class="nb">chr</span><span class="p">(</span><span class="mi">12288</span><span class="p">),</span> <span class="s1">'    '</span><span class="p">),</span> <span class="n">end</span><span class="o">=</span><span class="s1">''</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'{0:</span><span class="si">{1}</span><span class="s1">&lt;9} </span><span class="se">\033</span><span class="s1">[1;44m</span><span class="si">{2}</span><span class="se">\033</span><span class="s1">[0m</span><span class="se">\n</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s1">'背景颜色：蓝色'</span><span class="p">,</span> <span class="nb">chr</span><span class="p">(</span><span class="mi">12288</span><span class="p">),</span> <span class="s1">'    '</span><span class="p">),</span> <span class="n">end</span><span class="o">=</span><span class="s1">''</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'{0:</span><span class="si">{1}</span><span class="s1">&lt;9} </span><span class="se">\033</span><span class="s1">[1;45m</span><span class="si">{2}</span><span class="se">\033</span><span class="s1">[0m</span><span class="se">\n</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s1">'背景颜色：淡紫色'</span><span class="p">,</span> <span class="nb">chr</span><span class="p">(</span><span class="mi">12288</span><span class="p">),</span> <span class="s1">'    '</span><span class="p">),</span> <span class="n">end</span><span class="o">=</span><span class="s1">''</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'{0:</span><span class="si">{1}</span><span class="s1">&lt;9} </span><span class="se">\033</span><span class="s1">[1;46m</span><span class="si">{2}</span><span class="se">\033</span><span class="s1">[0m</span><span class="se">\n</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s1">'背景颜色：青色'</span><span class="p">,</span> <span class="nb">chr</span><span class="p">(</span><span class="mi">12288</span><span class="p">),</span> <span class="s1">'    '</span><span class="p">),</span> <span class="n">end</span><span class="o">=</span><span class="s1">''</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'{0:</span><span class="si">{1}</span><span class="s1">&lt;9} </span><span class="se">\033</span><span class="s1">[1;47m</span><span class="si">{2}</span><span class="se">\033</span><span class="s1">[0m</span><span class="se">\n</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s1">'背景颜色：灰色'</span><span class="p">,</span> <span class="nb">chr</span><span class="p">(</span><span class="mi">12288</span><span class="p">),</span> <span class="s1">'    '</span><span class="p">),</span> <span class="n">end</span><span class="o">=</span><span class="s1">''</span><span class="p">)</span>
</code></pre></div> <p data-pid="RgyhIMbI">运行结果：</p> <p><br></p> <figure data-size="normal"><img src="https://picx.zhimg.com/v2-84523348c41ffa8d32fea942e4dc1653_720w.png?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p><br></p> <h2>Matplotlib颜色表</h2> <p data-pid="ssQQkIXw"><b>转发自：<a href="http://link.zhihu.com/?target=https%3A//finthon.com/matplotlib-color-list/" class=" external" target="_blank" rel="nofollow noreferrer"><span class="invisible">https://</span><span class="visible">finthon.com/matplotlib-</span><span class="invisible">color-list/</span><span class="ellipsis"></span></a></b></p> <p><br></p> <figure data-size="normal"><img src="https://picx.zhimg.com/v2-b1f257b4163bb0b8733b4ef25a151e5d_720w.png?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p></p> <p class="post-meta"> 1 min read   ·   April 28, 2021   ·   RL in Robotics (Zhihu) </p> <p class="post-tags"> <a href="/blog/2021"> <i class="fa-solid fa-calendar fa-sm"></i> 2021 </a> </p> </li> <li> <h3> <a class="post-title" href="https://zhuanlan.zhihu.com/p/360596249" target="_blank" rel="external nofollow noopener">PR Efficient Ⅶ：表征学习对 Efficient RL 影响的理论研究</a> <svg width="2rem" height="2rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </h3> <p></p> <blockquote data-pid="5eDKgj5u">本文使用 <a href="https://zhuanlan.zhihu.com/p/106057556" class="internal" rel="external nofollow noopener" target="_blank">Zhihu On VSCode</a> 创作并发布<br>封面自 <a href="http://link.zhihu.com/?target=https%3A//www.google.com.hk/url%3Fsa%3Di%26url%3Dhttp%253A%252F%252Fwww.allwhitebackground.com%252Fblue-minimalist-wallpapers.html%252Fdownload%252F29937%26psig%3DAOvVaw234IM3ZqsztPUI2kQMh9vw%26ust%3D1617032639790000%26source%3Dimages%26cd%3Dvfe%26ved%3D0CAIQjRxqFwoTCKCE6pOq0-8CFQAAAAAdAAAAABAJ" class=" wrap external" target="_blank" rel="nofollow noreferrer">Link</a><br><b>每天一篇 Efficient，离 robot learning 落地更进一步。<br>想要专栏作家勋章，大家快关注专栏 RL in Robotics帮帮我~</b> </blockquote> <p data-pid="J0HlllBd">本文来自 ICLR 2020，<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1910.03016.pdf" class=" wrap external" target="_blank" rel="nofollow noreferrer">Is a Good Representation Sufficient for Sample Efficient Reinforcement Learning?</a>.<br></p> <figure data-size="normal"><img src="https://picx.zhimg.com/v2-d8750086a7cdca378075342d3eeee2ee_720w.png?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p><br></p> <p data-pid="WyRByBV5">Recent work 仅针对 model-based 中 learned model error 进行研究，却很少有研究 Efficient RL 的必要条件。本文发现，从统计学的角度来看，对满足 sample-efficient RL的表征比传统的近似观点中要求<b>更加严格</b>。本文的主要结果为 RL 提供了清晰的门槛，表明在构成良好的函数逼近（就表征的维数而言）方面存在严格的限制。这些下限突显出，<b>除非其函数逼近的质量超过某些严格的阈值，否则一个良好的表征不足以实现 Efficient RL。</b> 本文试图了解当我们能够获得准确的（紧凑的）参数表征时，是否有可能进行 efficient 的学习？</p> <h2>Proof</h2> <p data-pid="nKDdIsQO">由于我理论基础不太好，具体证明就不写了（/捂脸笑，人菜还想分析paper说的就是我），推荐参照 <a href="https://www.zhihu.com/people/zhang-chu-heng" class="internal" rel="external nofollow noopener" target="_blank">@张楚珩</a> 的文章：</p> <p><br></p> <a href="https://zhuanlan.zhihu.com/p/100213425" data-draft-node="block" data-draft-type="link-card" class="internal" rel="external nofollow noopener" target="_blank">【强化学习 101】Representation Lower Bound</a><p><br></p> <h2>Results</h2> <ol> <li data-pid="goUIfxd4">对 value-based learning，本文证明即使所有策略的 <img src="https://www.zhihu.com/equation?tex=Q" alt="Q" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer"> 函数都可以由给定表征的线性函数近似且具有近似误差 <img src="https://www.zhihu.com/equation?tex=%5Cdelta%3D%5COmega%5Cleft%28%5Csqrt%7B%5Cdfrac%7BH%7D%7Bd%7D%7D%5Cright%29" alt="\delta=\Omega\left(\sqrt{\dfrac{H}{d}}\right)" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer"> ，其中 <img src="https://www.zhihu.com/equation?tex=d" alt="d" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer"> 是表征维度，<img src="https://www.zhihu.com/equation?tex=H" alt="H" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer"> 是 planning horizon，agent 仍然需要采样指数级的样本才能找到接近最优的策略；</li> <li data-pid="VV3OLzpm">对 model-based learning，本文证明即使过渡矩阵和奖励函数可以由给定表征的线性函数逼近而具有近似误差 <img src="https://www.zhihu.com/equation?tex=%5Cdelta%3D%5COmega%5Cleft%28%5Csqrt%7B%5Cdfrac%7BH%7D%7Bd%7D%7D%5Cright%29" alt="\delta=\Omega\left(\sqrt{\dfrac{H}{d}}\right)" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer">，agent 仍然需要采样指数级的样本才能找到接近最优的策略；</li> <li data-pid="QCpJNKLD">对 policy-based learning，本文证明即使可以通过给定表征的线性函数以严格的 positive margin 完美预测最佳策略，agent 仍然需要采样指数级的样本才能找到接近最优的策略。</li> </ol> <p data-pid="6w04JvsM">这些下限即使在确定性系统，甚至已知 transition model 的情况中也是如此。本文的结果突出了以下见解：</p> <ol> <li data-pid="lN5WPQ__">对于最坏情况下表征的逼近质量，存在严格的阈值下限；</li> <li data-pid="uIrG4_1u">我们发现 efficient 的问题不是由于传统观念下的 exploration 导致的，未知的奖励函数足以使问题变得棘手；</li> <li data-pid="kgoH5nRd">我们的下限不是由于 agent 无法执行 efficient 的监督学习而引起的，因为如果数据分布固定，我们的假设确实允许多项式样本复杂度的上限；</li> <li data-pid="6Lda0oSn">最大的困难来自于 distribution mismatch。</li> </ol> <h2>Separations</h2> <p data-pid="fciJg8TF">本文还对比分析了不同的算法设定</p> <p data-pid="hFuICY87">Perfect representation vs. good-but-not-perfect representation</p> <p data-pid="UfCzRH4C">结论：<b>更好的表征形式具有 provable exponential benefit。</b></p> <p data-pid="16h0Ztnq">Value-based learning vs. policy-based learning</p> <p data-pid="bTIHuppI">结论：<b>表征预测 Q 函数的能力比预测最优策略的能力强得多。</b></p> <p data-pid="SxirPgwv">Supervised learning vs. reinforcement learning</p> <p data-pid="dEGbgDzr">结论：<b>样本复杂度对 planning horizon H 的依赖性是指数级的。</b></p> <p data-pid="0Z3SFPYe">Imitation learning vs. reinforcement learning</p> <p data-pid="xZSjMapD">结论：<b>使用函数拟合时，policy-based RL 比 IL 差一个数量级。</b></p> <p class="post-meta"> 1 min read   ·   March 28, 2021   ·   RL in Robotics (Zhihu) </p> <p class="post-tags"> <a href="/blog/2021"> <i class="fa-solid fa-calendar fa-sm"></i> 2021 </a> </p> </li> <li> <h3> <a class="post-title" href="https://zhuanlan.zhihu.com/p/360526111" target="_blank" rel="external nofollow noopener">PR Efficient Ⅴ：自预测表征，让RL agent高效地理解世界</a> <svg width="2rem" height="2rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </h3> <p></p> <blockquote data-pid="aUI-HYiX">本文使用 <a href="https://zhuanlan.zhihu.com/p/106057556" class="internal" rel="external nofollow noopener" target="_blank">Zhihu On VSCode</a> 创作并发布<br><b>每天一篇 Efficient，离 robot learning 落地更进一步。</b><br><b>想要专栏作家勋章，大家快关注专栏 RL in Robotics帮帮我~</b> </blockquote> <p><br></p> <p data-pid="ziG37wKx">本文来自 ICLR 2021，初步满足了我对于<b>将 model-based RL 与 Representation Learning 结合</b>的想法。<br></p> <p data-pid="ntOremU2"><a href="http://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2007.05929.pdf" class=" wrap external" target="_blank" rel="nofollow noreferrer">Paper Link</a></p> <figure data-size="normal"><img src="https://picx.zhimg.com/v2-63566cf1f0f815aca37498280f0578fa_720w.png?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p><br></p> <h2>Motivation</h2> <p data-pid="OVbprQ-T">本文受近期 semi-supervised 和 self-supervised learning 领域的启发，通过将状态空间映射到 latent space，并使用自监督的方法从数据的自然结构中，即时生成无限地训练数据来提高数据效率。RL 算法可以利用这个 learned representation model 来预测未来多步的 latent state representation。</p> <h2>Method</h2> <figure data-size="normal"><img src="https://picx.zhimg.com/v2-28eb29152a35315a304ac19cbc1fb6ec_720w.png?source=d16d100b" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"><figcaption>Full SPR method</figcaption></figure><h3>Online and Target Encoder</h3> <p data-pid="hfNrhHq2">设计一个 <b>online encoder</b> <img src="https://www.zhihu.com/equation?tex=f_o" alt="f_o" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer">，将观测状态 <img src="https://www.zhihu.com/equation?tex=s_t" alt="s_t" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer"> 转换为表征 <img src="https://www.zhihu.com/equation?tex=z_t%5Ctriangleq+f_o%28s_t%29" alt="z_t\triangleq f_o(s_t)" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer">。该表征函数只用于当前时刻。</p> <p data-pid="oppTAFR4">真正用到未来状态表征预测的是 target encoder <img src="https://www.zhihu.com/equation?tex=f_m" alt="f_m" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer">，它不随梯度更新，其参数与 online encoder 的关系为：</p> <p data-pid="GPVomPGD"><img src="https://www.zhihu.com/equation?tex=%5Ctheta_m%5Cleftarrow+%5Ctau+%5Ctheta_m+%2B+%281-%5Ctau%29%5Ctheta_o%5C%5C" alt="\theta_m\leftarrow \tau \theta_m + (1-\tau)\theta_o\\" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer"></p> <h3>Transition Model</h3> <p data-pid="gaXhCm4D"><img src="https://www.zhihu.com/equation?tex=%5Chat%7Bz%7D_%7Bt%2Bk%2B1%7D+%5Ctriangleq+h%28%5Chat%7Bz%7D_%7Bt%2Bk%7D%2Ca_%7Bt%2Bk%7D%29%5C%5C" alt="\hat{z}_{t+k+1} \triangleq h(\hat{z}_{t+k},a_{t+k})\\" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer"></p> <h3>Projection Heads</h3> <p data-pid="2q-oWGs3">分别将 online 和 target encoder 得到的隐式表征投影到更小的 latent space，方便学习，并设置了一个 prediction head <img src="https://www.zhihu.com/equation?tex=q" alt="q" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer"> 函数来学习 online projection 和 target projection 之间的关系。<br></p> <figure data-size="normal"><img src="https://picx.zhimg.com/v2-92aeef3e9af5286413ab9c534fe0cfbe_720w.png?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><h3>Prediction Loss</h3> <p data-pid="Cru8bCQE">通过对 <img src="https://www.zhihu.com/equation?tex=t%5Csim+t%2BK" alt="t\sim t+K" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer"> 时刻预测值和观测值之间的余弦相似度求和，计算出SPR的未来预测损失。（这和上一篇文章的多步损失一样，用于限制 long-horizon 的 learned model error）<br></p> <figure data-size="normal"><img src="https://pica.zhimg.com/v2-a60e58e70b1abac80167b9f9a38d3d6d_720w.png?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p data-pid="B7jzwvzQ">Prediction Loss 是以 auxiliary loss 的形式参与到训练中的，具体来说就是 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D_%5Ctheta%5E%7Btotal%7D+%3D+%5Cmathcal%7BL%7D_%5Ctheta%5E%7BRL%7D%2B%5Clambda+%5Cmathcal%7BL%7D_%5Ctheta%5E%7BSPR%7D" alt="\mathcal{L}_\theta^{total} = \mathcal{L}_\theta^{RL}+\lambda \mathcal{L}_\theta^{SPR}" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer">。</p> <h3>Algorithm</h3> <figure data-size="normal"><img src="https://pica.zhimg.com/v2-574a00c3efa6be7f9afc3796ba016e54_720w.png?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><h2>Conclusion</h2> <p data-pid="lMDnt56G">其实本文的亮点在第三章 Related Work，<b>列举了大量 data-efficient RL 以及 RL 结合 state representation 的最新进展</b>，我在这里汇总一下：</p> <ol> <li data-pid="qLW7--pG"> <b>SiMPLe</b> (Kaiser et al., 2019)：学习 Atari 的像素级 transition model，以生成模拟训练数据，从而在100k帧设定下的几场比赛中取得了不错的成绩，但仍需要<b>花费数周</b>的训练时间；</li> <li data-pid="GQWF-FS-"> <b>Data-Efficient Rainbow (DER) and OTRainbow</b> (Hasselt et al. (2019) and Kielak (2020))：引入了针对样本效率进行了调整的Rainbow变体，它们可通过更少的计算获得相当或更高的性能；</li> <li data-pid="YQ_NuSoT">(Hafner et al., 2019; Lee et al., 2019; Hafner et al., 2020)：利用 <b>reconstruction loss</b> 进行训练的 latent-space model 来提高样本效率；</li> <li data-pid="KNyebncG"> <b>DrQ</b> (Yarats et al., 2021) and <b>RAD</b> (Laskin et al., 2020)：发现适度的 data augmentation 可以大大提高 RL 中的样本效率；</li> <li data-pid="H4YBp3ZL"> <b>CURL</b> (Srinivas et al., 2020)：提出将 image augmentation and a contrastive loss 相结合来执行RL的表征学习。但是，RAD的跟踪结果 (Laskin et al., 2020) 表明，CURL的大部分利好来自图像增强，而不是对比损失。（所以本文一直在强调其模型具有兼容数据增强的特性）</li> <li data-pid="30ObVzvY"> <b>CPC</b> (Oord et al., 2018), <b>CPC|Action</b> (Guo et al., 2018), <b>ST-DIM</b> (Anand et al., 2019) and <b>DRIML</b> (Mazoure et al., 2020)：优化强化学习环境中的各种 temporal contrastive losses；</li> <li data-pid="HHckP67_">Kipf et al. (2019)：提出通过训练基于<b>图神经网络</b>的<b>结构化 transition model</b> 来学习面向对象的 <b>contrastive representations</b>。</li> <li data-pid="TOYW2fwD"> <b>DeepMDP</b> (Gelada et al., 2019)：训练了具有未归一化的L2损失的 transition model，以预测未来状态的表征以及奖励预测目标函数；</li> <li data-pid="GV0_Yv-s"> <b>PBL</b> (Guo et al., 2020)：直接通过梯度下降训练的两个独立的 target networks，预测未来状态的表征。</li> </ol> <p data-pid="sx359lL4">这篇文章一个重大缺陷是，<b>自预测表征依赖于全局观测，使其应用场景受限于 video game</b>，仍不适合 real-world Robotics。</p> <p class="post-meta"> 1 min read   ·   March 28, 2021   ·   RL in Robotics (Zhihu) </p> <p class="post-tags"> <a href="/blog/2021"> <i class="fa-solid fa-calendar fa-sm"></i> 2021 </a> </p> </li> <li> <h3> <a class="post-title" href="https://zhuanlan.zhihu.com/p/360314680" target="_blank" rel="external nofollow noopener">机器人：我要在五分钟内自己学会走路~！</a> <svg width="2rem" height="2rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </h3> <p></p> <h2>PR Efficient Ⅳ：五分钟内让四足机器人自主学会行走</h2> <blockquote data-pid="04Q9_vZx">本文使用 <a href="https://zhuanlan.zhihu.com/p/106057556" class="internal" rel="external nofollow noopener" target="_blank">Zhihu On VSCode</a> 创作并发布<br><b>每天一篇 Efficient，离 robot learning 落地更进一步。</b><br><b>想要专栏作家勋章，大家快关注专栏 RL in Robotics帮帮我~</b> </blockquote> <p data-pid="fQW_JuuS">经我观察，一般做 Efficient RL 的文章，都可以同时实现 safe exploration。这两个需求之间是有相通性的，毕竟 efficient 主要是通过压缩state或action维度来实现，那么在这个过程中压缩掉一些 unsafe exploration 就很好理解了。Efficient 与 safe exploration 的结合使强化学习在 real-world 机器人中的落地更进一步。</p> <p data-pid="1fgM9NMb">今天看一篇2019年 Google Robotics 的 CoRL<br></p> <figure data-size="normal"><img src="https://picx.zhimg.com/v2-b2f9bbeb59ece6ff10298abec48373af_720w.png?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p><br></p> <p data-pid="90y06RbA">这篇文章的效果还是很惊艳的，从机器人随机动作采集数据开始，到机器人学会 walking 步态，一共只需要 4.5 min 的数据采集。就算是加上 rollout 和 experiment resets，也不过10分钟，36个 episodes (45,000 control steps)。</p> <p data-pid="nLc08-Fl">转载自：<a href="http://link.zhihu.com/?target=https%3A//www.youtube.com/watch%3Fv%3DoB9IXKmdGhc" class=" external" target="_blank" rel="nofollow noreferrer"><span class="invisible">https://www.</span><span class="visible">youtube.com/watch?</span><span class="invisible">v=oB9IXKmdGhc</span><span class="ellipsis"></span></a></p> <a href="https://www.zhihu.com/zvideo/1359134710791266304" data-draft-node="block" data-draft-type="link-card" rel="external nofollow noopener" target="_blank"></a><h2>Main Contribution</h2> <p data-pid="yj1pOGUx">我们都知道 model-based 相较于 model-free RL 在机器人上是更合理且更高效的做法，这里就不赘述了。</p> <p data-pid="QEpB1F5f">针对 model-based RL in real-world Robotics，本文主要解决下述<b>三个问题</b>：</p> <ol> <li data-pid="BE-ZeEEg">The learned model needs to be sufficiently <b>accurate</b> for long-horizon planning。即使短期内的小误差，经过 long horizon 的累积，对控制的影响都是致命的；</li> <li data-pid="04Spjrd2"> <b>Real-time</b> action planning at a high control frequency。实时性是机器人任务，尤其是底层控制器不得不面对的现实问题；</li> <li data-pid="zdDtV9gO"> <b>Safe</b> data collection for model learning is nontrivial。防止 mechanical failures/damages。</li> </ol> <p data-pid="1o_JuDm3">本文给出的对应<b>解决方案</b>：</p> <ol> <li data-pid="zI7G4xH1"> <b>Accurate</b>：使用 <b>multi-step loss</b> 来防止 long-horizon prediction 中的 error accumulation；</li> <li data-pid="FUTwcJf0"> <b>Real-time</b>：将 planning 和 controlling <b>并行化</b>，并借助 learned model 进行超前预测，弥补 planning 和 controlling 之间的时差，实现<b>异步控制</b>；</li> <li data-pid="QWQqoi_h"> <b>Safe</b>：使用 trajectory generator 确保 planned action 的<b>平滑性</b>。</li> </ol> <p data-pid="9QJSBiPc">下面我们详细展开一下这些贡献。</p> <h2>Multi-step loss</h2> <p data-pid="y90k8e5O"><b>为什么用 multi-step loss，而非 MVE/STEVE/MBPO 常用的 model ensemble 来消除 model error的影响？</b></p> <p data-pid="MXDx3xuY">虽然 ensemble 也不错，但是它增加了 planning time，影响了实时性。</p> <p data-pid="h6wt56mZ"><b>Multi-step loss 的形式</b>普通的单步损失：</p> <p><br></p> <figure data-size="normal"><img src="https://picx.zhimg.com/v2-8f625df91d94c23affe4c470278c08a7_720w.png?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p><br></p> <p data-pid="Y8tael7b">多步损失：<br></p> <figure data-size="normal"><img src="https://pic1.zhimg.com/v2-6a385eaf96364d5266437c06e8d9bd1d_720w.png?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p><br></p> <p data-pid="uhbqhKtR">其中，<img src="https://www.zhihu.com/equation?tex=f_%5Ctheta%28s_t%2C+a_t%29" alt="f_\theta(s_t, a_t)" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer"> 是用于拟合 model-based 惯用的状态差 <img src="https://www.zhihu.com/equation?tex=s_%7Bt%2B1%7D+-+s_t" alt="s_{t+1} - s_t" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer"> 的神经网络。至于其消除 long-horizon model error 的作用还是显而易见的。</p> <h2>Parallel and asynchronous</h2> <p><br></p> <figure data-size="normal"><img src="https://pic1.zhimg.com/v2-03cdaa25177d0664d543cfc2c96aee27_720w.png?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p><br></p> <p data-pid="Jc1sZdyN">并行必定会导致两个环节处理时长不匹配的问题。所以这里就借助了上一节的 learned model 让 planner 能够在 <img src="https://www.zhihu.com/equation?tex=t" alt="t" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer"> 时刻超前预测 <img src="https://www.zhihu.com/equation?tex=t%2BT" alt="t+T" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer"> 时刻的 planning，从而弥补了 planning 的滞后。</p> <h2>Smoothness</h2> <p><br></p> <figure data-size="normal"><img src="https://pic1.zhimg.com/v2-64f06a3fd62fdea9c6818cad7e404e37_720w.png?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p data-pid="Q01xKjhA"><br>这里的 Trajectory generators (TGs) 概念借用自 <a href="http://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1910.02812.pdf" class=" wrap external" target="_blank" rel="nofollow noreferrer">Policies Modulating Trajectory Generators - Google, CoRL 2018 </a>，我简单看了一下，这是一个可以为控制器结合 <b>memory</b> 和 <b>prior knowledge</b> 控制行为生成器，是机器人能够生成更符合人类期望的复杂行为或步态。</p> <p data-pid="VnTf9ZBB">除此之外，本文对输出的连续 actions 也做了平滑处理。<br></p> <figure data-size="normal"><img src="https://pica.zhimg.com/v2-51f00c01e7e9552042e1ca3b53e857b2_720w.png?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p><br></p> <h2>Experiments</h2> <h2>On-robot experiments</h2> <figure data-size="normal"><img src="https://picx.zhimg.com/v2-bb997438f9ffd78abb4915ba039d052b_720w.png?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><h3>与 model-free 的效率对比</h3> <p data-pid="CMUURi-T">学习效率较 model-free 方法高出一个数量级。不过这里并没有出现与 SOTA 的model-based 方法的比较，但是考虑到五分钟的训练就有这样的效果，还是不错的了。</p> <figure data-size="normal"><img src="https://picx.zhimg.com/v2-055ecfa2fbaf62f087e209043ba91b40_720w.png?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><h2>Conclusion</h2> <p data-pid="E5tBtQhk">从 efficient 的角度来看，效果还是不错的，也考虑并解决了很多<b>实际工程问题</b>，值得借鉴。</p> <p class="post-meta"> 1 min read   ·   March 27, 2021   ·   RL in Robotics (Zhihu) </p> <p class="post-tags"> <a href="/blog/2021"> <i class="fa-solid fa-calendar fa-sm"></i> 2021 </a> </p> </li> <li> <h3> <a class="post-title" href="https://zhuanlan.zhihu.com/p/359776893" target="_blank" rel="external nofollow noopener">PR Efficient Ⅲ：像训练狗狗一样高效地训练机器人</a> <svg width="2rem" height="2rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </h3> <p></p> <blockquote data-pid="LQ4PC4DU">本文使用 <a href="https://zhuanlan.zhihu.com/p/106057556" class="internal" rel="external nofollow noopener" target="_blank">Zhihu On VSCode</a> 创作并发布<br><br><b>每天一篇 Efficient，离 robot learning 落地更进一步。</b> </blockquote> <p><br></p> <figure data-size="normal"><img src="https://pic1.zhimg.com/v2-432fde524ebd2d7d77afdc67ebcc87e0_720w.png?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p><br></p> <p data-pid="ZvA8zbs-">这是一篇20年的 RA-L，题目要素过多，既有 efficient 又可以 multi-step 还能考虑到 sim2real，让人看了直呼 <b>Good</b>！</p> <blockquote data-pid="9Jx3k-Nz">Current Reinforcement Learning (RL) algorithms<br>struggle with long-horizon tasks where time can be wasted exploring dead ends and task progress may be easily reversed. We develop the SPOT framework, which explores within action safety zones, learns about unsafe regions without exploring them, and prioritizes experiences that reverse earlier progress to learn with remarkable efficiency</blockquote> <p data-pid="U7FlbUgP">摘要直接指明当前RL算法在探索死胡同或无用的状态上浪费了太多时间，且任务进度很容易被逆转 (progress reverse, 稍后解释)。因此，本文提出了 SPOT 框架，实现了以下三点：</p> <ol> <li data-pid="YgWO2q0b">在安全区内探索；</li> <li data-pid="mxEdcyPL">即使没有探索也能学习非安全区的知识；</li> <li data-pid="9bh6Byii">优先考虑可以逆转早期学习进度的经验。</li> </ol> <p data-pid="_9N5bdTb">从而实现高效的强化学习。下图展示了他们实机测试的结果，十分令人振奋！</p> <p><br></p> <figure data-size="normal"><img src="https://picx.zhimg.com/v2-fd2fb6d35c24dc9902755811e713dfff_720w.png?source=d16d100b" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"><figcaption>SPOT 针对的是方块堆叠任务以及将他们排成一排，将这两个任务的成功率都提升至 100%，效率提升 30%，使训练过程只需要 1-20k 步。且这些在仿真器中训练的模型可以在不进行 fine-tuning 的情况下迁移到 real world，并获得 100% 的成功率！</figcaption></figure><p><br></p> <h2>Idea</h2> <p data-pid="yRK83ih-"><b>什么是无用的探索？</b></p> <p data-pid="26Y0y_IF">具体来说，以人类的角度，上述任务中机器人在空气中乱抓明显是没意义的，然而 RL 算法们基本都在这种情况上浪费了大量的时间。</p> <p data-pid="LHu7eL_a"><b>如何避免无效探索？</b></p> <p data-pid="YGzt_1s7">本文提出的框架，结合了 <b>common sense constraints</b>，从而显著提高学习效率与最终任务效率。借鉴了一种叫做 <b>"Positive Conditioning"</b> 的人性化与高效的宠物训练方法。这里直接放个 wikipedia 的链接吧：<a href="http://link.zhihu.com/?target=https%3A//zh.wikipedia.org/wiki/%25E6%2593%258D%25E4%25BD%259C%25E5%2588%25B6%25E7%25B4%2584" class=" wrap external" target="_blank" rel="nofollow noreferrer">操作性条件反射</a></p> <blockquote data-pid="147wM2cL">Bionics 万岁！话说 RL 本身好像就是 Bionics啊，和这个操作性条件反射没啥区别吧。有趣的是，我发现生活中这些都属于操作性条件反射：</blockquote> <ol> <li data-pid="1S0XMeW_">在滑雪课上执行转弯后，你的教练喊道：“辛苦了！”</li> <li data-pid="yEOog22X">在工作中，你超出了本月的销售配额，因此你的老板给了您奖金。</li> <li data-pid="dZJEpg22">在心理学课程中，你观看有关人脑的视频并撰写关于所学知识的论文。你的老师会为你的工作提供20个额外的学分。</li> </ol> <p><br></p> <p data-pid="UgKLqqsk"><b>什么是 progress reverse?</b></p> <p data-pid="kvryFU9i">简单来说，这就是个汉诺塔，基础不牢，堆的越高错的越惨。Progress reverse 就是描述这样一个推倒重来的情景。<br></p> <figure data-size="normal"><img src="https://pica.zhimg.com/v2-421ba0e09889b78103129ce519b407bc_720w.png?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p><br></p> <h2>Approach</h2> <p><br></p> <figure data-size="normal"><img src="https://pica.zhimg.com/v2-2db633e08bd5ded442d3afaf55da95bc_720w.png?source=d16d100b" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"><figcaption>Main Framework</figcaption></figure><p data-pid="E8SLjOff"><br>作者认为在机器人应用中，Q-learning算法的动作空间和尝试成本很大，并且 efficient 也会有一定的 safe exploration 保证。此外，算法的性能很大程度取决于奖励函数的设置，因此本文先从 task-specific reward shaping 入手。</p> <h3>Reward shaping</h3> <p data-pid="ORt6z5yc">本文列出了几种奖励函数的设置，其中的 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7B1%7D" alt="\mathbf{1}" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer"> 是二进制的 indicator function：</p> <ol> <li data-pid="dvIiHgSv"> <img src="https://www.zhihu.com/equation?tex=R_%7Bbase%7D%28s_%7Bt%2B1%7D%2C+a_t%29%3DW%28%5Cphi_t%29%5Cmathbf%7B1%7D_a%5Bs_%7Bt%2B1%7D%2C+a_t%5D" alt="R_{base}(s_{t+1}, a_t)=W(\phi_t)\mathbf{1}_a[s_{t+1}, a_t]" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer"><br><img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7B1%7D_a%5Bs_%7Bt%2B1%7D%2C+a_t%5D" alt="\mathbf{1}_a[s_{t+1}, a_t]" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer">等于1代表 <img src="https://www.zhihu.com/equation?tex=a_t" alt="a_t" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer"> 在 sub-task <img src="https://www.zhihu.com/equation?tex=%5Cphi" alt="\phi" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer"> 中成功，反之为0；<img src="https://www.zhihu.com/equation?tex=W" alt="W" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer"> 是每个 sub-task 的权重。</li> <li data-pid="CHHXnxXh"> <img src="https://www.zhihu.com/equation?tex=R_%7BSR%7D%28s_%7Bt%2B1%7D%2C+a_t%29%3D%5Cmathbf%7B1%7D_%7BSR%7D%5Bs_t%2C+s_%7Bt%2B1%7D%5DR_%7Bbase%7D%28s_%7Bt%2B1%7D%2C+a_t%29" alt="R_{SR}(s_{t+1}, a_t)=\mathbf{1}_{SR}[s_t, s_{t+1}]R_{base}(s_{t+1}, a_t)" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer"><br>定义一个全局任务进度函数 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BP%7D%3A+S%5Crightarrow+%5Cmathbb%7BR%7D%5Cin+%5B0%2C1%5D" alt="\mathcal{P}: S\rightarrow \mathbb{R}\in [0,1]" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer">，当<img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BP%7D%28s_t%29%3D1" alt="\mathcal{P}(s_t)=1" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer">的时候，全局任务完成；<img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7B1%7D_%7BSR%7D%5Bs_t%2C+s_%7Bt%2B1%7D%5D" alt="\mathbf{1}_{SR}[s_t, s_{t+1}]" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer"> 在 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BP%7D%28s_%7Bt%2B1%7D%29%5Cge%5Cmathcal%7BP%7D%28s_t%29" alt="\mathcal{P}(s_{t+1})\ge\mathcal{P}(s_t)" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer">时为1。</li> <li data-pid="DZitCoqB"><img src="https://www.zhihu.com/equation?tex=R_%5Cmathcal%7BP%7D%3D%5Cmathcal%7BP%7D%28s_%7Bt%2B1%7D%29R_%7BSR%7D%28s_%7Bt%2B1%7D%2C+a_t%29" alt="R_\mathcal{P}=\mathcal{P}(s_{t+1})R_{SR}(s_{t+1}, a_t)" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer"></li> </ol> <blockquote data-pid="FQYOUBy-">解释一下为什么要 2 和 3 区分开，以及什么情况 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BP%7D%28s_%7Bt%2B1%7D%29%5Cge%5Cmathcal%7BP%7D%28s_t%29" alt="\mathcal{P}(s_{t+1})\ge\mathcal{P}(s_t)" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer"><br>本文将 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BP%7D" alt="\mathcal{P}" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer"> 设定为木块堆积高度与期望总高度的比值，摆成一排的任务也是如此。所以这是一个 <img src="https://www.zhihu.com/equation?tex=%5B0%2C1%5D" alt="[0,1]" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer"> 的离散值，而非二进制。</blockquote> <p data-pid="H9c6ggTb">以上三种奖励函数都可以及时获得，但是没有考虑早期的错误，所以本文又给出了下面这个贯穿整个训练过程的奖励函数。</p> <h3>Situation Removal: SPOT Trail Reward</h3> <p><br></p> <figure data-size="normal"><img src="https://pic1.zhimg.com/v2-52570c1b2a829e51f3989b9dd9f359fa_720w.png?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p><br></p> <p data-pid="-2HXB9AD">其中，<img src="https://www.zhihu.com/equation?tex=R_%2A" alt="R_*" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer"> 可以是 <img src="https://www.zhihu.com/equation?tex=R_%7BSR%7D%2C+R_%5Cmathcal%7B%7BP%7D%7D" alt="R_{SR}, R_\mathcal{{P}}" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer"> 的任意一种。<img src="https://www.zhihu.com/equation?tex=N" alt="N" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer"> 代表任务终止，<img src="https://www.zhihu.com/equation?tex=%5Cgamma+%3D+0.65" alt="\gamma = 0.65" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer">。<img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7BR%7D_%7Btrial%7D" alt="\mathbf{R}_{trial}" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer"> 的意义是，未来的奖励只会在子任务完成时回传。</p> <p data-pid="EPb0icrV">以下面的例子说明，<b>Situation removal 的零奖励切断了未来奖励的回传，致使算法聚焦在那些短且成功的序列上</b>。<br></p> <figure data-size="normal"><img src="https://picx.zhimg.com/v2-04320a29db3c26d87d46439bf53ffa17_720w.png?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p><br></p> <h3>SPOT-Q Learning and Dynamic Action Spaces</h3> <p data-pid="vWVjLdBG">这一节作者引入环境先验来加速学习。假设了一个<b>先知</b> <img src="https://www.zhihu.com/equation?tex=M%28s_t%2C+a%29%5Crightarrow+%7B0%2C1%7D" alt="M(s_t, a)\rightarrow {0,1}" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer">，当前 action 失败就返回0。根据这个先知，我们可以定义一个动态的动作空间（也就是利用先知缩小动作空间，去掉那些不值得探索的动作）:</p> <p data-pid="OVDYqL2Q"><img src="https://www.zhihu.com/equation?tex=M_t%28A%29+%3D+%5C%7Ba%5Cin+A%7CM%28s_t%2C+a%29%3D1%5C%7D%5C%5C" alt="M_t(A) = \{a\in A|M(s_t, a)=1\}\\" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer"></p> <p data-pid="NrW9MbvB">所以就有了压缩动作空间之后的 <b>SPOT-Q learning</b>：<br></p> <figure data-size="normal"><img src="https://pic1.zhimg.com/v2-db48a5088597d3b753ded2a63488ba69_720w.png?source=d16d100b" data-caption="" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"></figure><p><br></p> <p data-pid="7wZh2NSm">算法伪码如下<br></p> <figure data-size="normal"><img src="https://pic1.zhimg.com/v2-4cd0661539c90059958436b813c54a31_720w.png?source=d16d100b" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"><figcaption>使用了PER技术</figcaption></figure><p><br></p> <h2>Experiment</h2> <figure data-size="normal"><img src="https://pic1.zhimg.com/v2-72da9fa634ecf6e6744a81aa83cc9f2d_720w.png?source=d16d100b" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"><figcaption>Simulation 实验</figcaption></figure><p><br></p> <figure data-size="normal"><img src="https://picx.zhimg.com/v2-eb0cc1bf72801430cfdb51951d9b58d9_720w.png?source=d16d100b" data-size="normal" class="content_image" style="max-width: 100%;" referrerpolicy="no-referrer"><figcaption>Real world 实验</figcaption></figure><p><br></p> <h2>Conclusion</h2> <p data-pid="dIVAnl_-">SPOT 框架根据 zero-reward guidance, a masked action space, situation removal 这三方面提升了任务效果。</p> <p data-pid="CRw6G3tZ">正如文末指出的，本文的主要 weakness：</p> <ol> <li data-pid="aqevIRzc">需要有一个从 data 结合 Situation removal 的learning structure；</li> <li data-pid="1IsbSWcK">用于动作空间压缩的先知mask <img src="https://www.zhihu.com/equation?tex=M" alt="M" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer"> <b>竟然是手工设定的</b>，我读文章时候一直以为是类似于 model learning 的操作。</li> </ol> <p data-pid="TgOQQ587">总的来说，实验效果是好的，但是提出的算法看不出什么技术含量，毕竟 task-specific 的 reward shaping 只是 task-specific。不过，压缩动作空间来实现 efficient 的想法是值得借鉴的。</p> <p class="post-meta"> 1 min read   ·   March 25, 2021   ·   RL in Robotics (Zhihu) </p> <p class="post-tags"> <a href="/blog/2021"> <i class="fa-solid fa-calendar fa-sm"></i> 2021 </a> </p> </li> </ul> <nav aria-label="Blog page naviation"> <ul class="pagination pagination-lg justify-content-center"> <li class="page-item "> <a class="page-link" href="/blog/page/3/" tabindex="-1" aria-disabled="3">Newer</a> </li> <li class="page-item "> <a class="page-link" href="/blog/index.html" title="Blog">1</a> </li> <li class="page-item "> <a class="page-link" href="/blog/page/2/index.html" title="Blog - page 2">2</a> </li> <li class="page-item "> <a class="page-link" href="/blog/page/3/index.html" title="Blog - page 3">3</a> </li> <li class="page-item active"> <a class="page-link" href="/blog/page/4/index.html" title="Blog - page 4">4</a> </li> <li class="page-item "> <a class="page-link" href="/blog/page/5/index.html" title="Blog - page 5">5</a> </li> <li class="page-item "> <a class="page-link" href="/blog/page/5/">Older</a> </li> </ul> </nav> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Junjia LIU. Last updated: March 30, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?5d75c11f89cd96294bf5e6dd1ee1bb30"></script> <script defer src="/assets/js/common.js?fcfacfb8c6281f5e68d5a7d348186eb1"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>