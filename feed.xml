<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://skylark0924.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://skylark0924.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-03-30T04:51:41+00:00</updated><id>https://skylark0924.github.io/feed.xml</id><title type="html">Junjia LIU</title><entry><title type="html">Rofunc-The Full Process Python Package for Robot Learning from Demonstration and Robot Manipulation</title><link href="https://skylark0924.github.io/blog/2024/Rofunc0026/" rel="alternate" type="text/html" title="Rofunc-The Full Process Python Package for Robot Learning from Demonstration and Robot Manipulation"/><published>2024-01-24T16:40:16+00:00</published><updated>2024-01-24T16:40:16+00:00</updated><id>https://skylark0924.github.io/blog/2024/Rofunc0026</id><content type="html" xml:base="https://skylark0924.github.io/blog/2024/Rofunc0026/"><![CDATA[<div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/doc/img/logo8-480.webp 480w,/assets/img/doc/img/logo8-800.webp 800w,/assets/img/doc/img/logo8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/doc/img/logo8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h1 id="rofunc-the-full-process-python-package-for-robot-learning-from-demonstration-and-robot-manipulation">Rofunc: The Full Process Python Package for Robot Learning from Demonstration and Robot Manipulation</h1> <p><a href="https://pypi.org/project/rofunc/"><img src="https://img.shields.io/github/v/release/Skylark0924/Rofunc" alt="Release"/></a> <img src="https://img.shields.io/github/license/Skylark0924/Rofunc?color=blue" alt="License"/> <img src="https://img.shields.io/github/downloads/skylark0924/Rofunc/total" alt=""/> <a href="https://github.com/Skylark0924/Rofunc/issues?q=is%3Aissue+is%3Aclosed"><img src="https://img.shields.io/github/issues-closed-raw/Skylark0924/Rofunc?color=brightgreen" alt=""/></a> <a href="https://github.com/Skylark0924/Rofunc/issues?q=is%3Aopen+is%3Aissue"><img src="https://img.shields.io/github/issues-raw/Skylark0924/Rofunc?color=orange" alt=""/></a> <a href="https://rofunc.readthedocs.io/en/latest/?badge=latest"><img src="https://readthedocs.org/projects/rofunc/badge/?version=latest" alt="Documentation Status"/></a> <a href="https://actions-badge.atrox.dev/Skylark0924/Rofunc/goto?ref=main"><img src="https://img.shields.io/endpoint.svg?url=https%3A%2F%2Factions-badge.atrox.dev%2FSkylark0924%2FRofunc%2Fbadge%3Fref%3Dmain&amp;style=flat" alt="Build Status"/></a></p> <blockquote> <p><strong>Repository address: <a href="https://github.com/Skylark0924/Rofunc">https://github.com/Skylark0924/Rofunc</a></strong> <br/> <strong>Documentation: <a href="https://rofunc.readthedocs.io/">https://rofunc.readthedocs.io/</a></strong></p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/doc/img/task_gif3/CURIQbSoftHandSynergyGraspSpatulaRofuncRLPPO-480.webp 480w,/assets/img/doc/img/task_gif3/CURIQbSoftHandSynergyGraspSpatulaRofuncRLPPO-800.webp 800w,/assets/img/doc/img/task_gif3/CURIQbSoftHandSynergyGraspSpatulaRofuncRLPPO-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/doc/img/task_gif3/CURIQbSoftHandSynergyGraspSpatulaRofuncRLPPO.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/doc/img/task_gif3/CURIQbSoftHandSynergyGraspPower_drillRofuncRLPPO-480.webp 480w,/assets/img/doc/img/task_gif3/CURIQbSoftHandSynergyGraspPower_drillRofuncRLPPO-800.webp 800w,/assets/img/doc/img/task_gif3/CURIQbSoftHandSynergyGraspPower_drillRofuncRLPPO-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/doc/img/task_gif3/CURIQbSoftHandSynergyGraspPower_drillRofuncRLPPO.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/doc/img/task_gif3/CURIQbSoftHandSynergyGraspPhillips_Screw_DriverRofuncRLPPO-480.webp 480w,/assets/img/doc/img/task_gif3/CURIQbSoftHandSynergyGraspPhillips_Screw_DriverRofuncRLPPO-800.webp 800w,/assets/img/doc/img/task_gif3/CURIQbSoftHandSynergyGraspPhillips_Screw_DriverRofuncRLPPO-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/doc/img/task_gif3/CURIQbSoftHandSynergyGraspPhillips_Screw_DriverRofuncRLPPO.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/doc/img/task_gif3/CURIQbSoftHandSynergyGraspLarge_clampRofuncRLPPO-480.webp 480w,/assets/img/doc/img/task_gif3/CURIQbSoftHandSynergyGraspLarge_clampRofuncRLPPO-800.webp 800w,/assets/img/doc/img/task_gif3/CURIQbSoftHandSynergyGraspLarge_clampRofuncRLPPO-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/doc/img/task_gif3/CURIQbSoftHandSynergyGraspLarge_clampRofuncRLPPO.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/doc/img/task_gif3/HumanoidFlipRofuncRLAMP-480.webp 480w,/assets/img/doc/img/task_gif3/HumanoidFlipRofuncRLAMP-800.webp 800w,/assets/img/doc/img/task_gif3/HumanoidFlipRofuncRLAMP-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/doc/img/task_gif3/HumanoidFlipRofuncRLAMP.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/doc/img/task_gif3/HumanoidDanceRofuncRLAMP-480.webp 480w,/assets/img/doc/img/task_gif3/HumanoidDanceRofuncRLAMP-800.webp 800w,/assets/img/doc/img/task_gif3/HumanoidDanceRofuncRLAMP-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/doc/img/task_gif3/HumanoidDanceRofuncRLAMP.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/doc/img/task_gif3/HumanoidRunRofuncRLAMP-480.webp 480w,/assets/img/doc/img/task_gif3/HumanoidRunRofuncRLAMP-800.webp 800w,/assets/img/doc/img/task_gif3/HumanoidRunRofuncRLAMP-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/doc/img/task_gif3/HumanoidRunRofuncRLAMP.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/doc/img/task_gif3/HumanoidASEHeadingSwordShieldRofuncRLASE-480.webp 480w,/assets/img/doc/img/task_gif3/HumanoidASEHeadingSwordShieldRofuncRLASE-800.webp 800w,/assets/img/doc/img/task_gif3/HumanoidASEHeadingSwordShieldRofuncRLASE-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/doc/img/task_gif3/HumanoidASEHeadingSwordShieldRofuncRLASE.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/doc/img/task_gif3/HumanoidASEStrikeSwordShieldRofuncRLASE-480.webp 480w,/assets/img/doc/img/task_gif3/HumanoidASEStrikeSwordShieldRofuncRLASE-800.webp 800w,/assets/img/doc/img/task_gif3/HumanoidASEStrikeSwordShieldRofuncRLASE-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/doc/img/task_gif3/HumanoidASEStrikeSwordShieldRofuncRLASE.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/doc/img/task_gif3/HumanoidASELocationSwordShieldRofuncRLASE-480.webp 480w,/assets/img/doc/img/task_gif3/HumanoidASELocationSwordShieldRofuncRLASE-800.webp 800w,/assets/img/doc/img/task_gif3/HumanoidASELocationSwordShieldRofuncRLASE-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/doc/img/task_gif3/HumanoidASELocationSwordShieldRofuncRLASE.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/doc/img/task_gif3/BiShadowHandLiftUnderarmRofuncRLPPO-480.webp 480w,/assets/img/doc/img/task_gif3/BiShadowHandLiftUnderarmRofuncRLPPO-800.webp 800w,/assets/img/doc/img/task_gif3/BiShadowHandLiftUnderarmRofuncRLPPO-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/doc/img/task_gif3/BiShadowHandLiftUnderarmRofuncRLPPO.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/doc/img/task_gif3/BiShadowHandDoorOpenOutwardRofuncRLPPO-480.webp 480w,/assets/img/doc/img/task_gif3/BiShadowHandDoorOpenOutwardRofuncRLPPO-800.webp 800w,/assets/img/doc/img/task_gif3/BiShadowHandDoorOpenOutwardRofuncRLPPO-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/doc/img/task_gif3/BiShadowHandDoorOpenOutwardRofuncRLPPO.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/doc/img/task_gif3/BiShadowHandSwingCupRofuncRLPPO-480.webp 480w,/assets/img/doc/img/task_gif3/BiShadowHandSwingCupRofuncRLPPO-800.webp 800w,/assets/img/doc/img/task_gif3/BiShadowHandSwingCupRofuncRLPPO-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/doc/img/task_gif3/BiShadowHandSwingCupRofuncRLPPO.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div>]]></content><author><name></name></author><category term="code-project"/><category term="humanoid-robots"/><category term="reinforcement-learning"/><category term="skill-learning"/><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">HuggingFace Transformers使用教程</title><link href="https://skylark0924.github.io/blog/2023/huggingface-transformers/" rel="alternate" type="text/html" title="HuggingFace Transformers使用教程"/><published>2023-05-26T10:41:02+00:00</published><updated>2023-05-26T10:41:02+00:00</updated><id>https://skylark0924.github.io/blog/2023/huggingface-transformers</id><content type="html" xml:base="https://skylark0924.github.io/blog/2023/huggingface-transformers/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[本教程基本是从Transformers的官方文档翻译而来，只是作为个人学习使用的总结。 Transformers 提供 API 和工具来轻松下载和训练最先进的预训练模型。使用预训练模型可以降低您的计算成本和碳足迹，并节省您从头开始训练模型所需的时间和资源。这些模型支持不同模式的常见任务，例如： 自然语言处理：文本分类、命名实体识别、问答、语言建模、摘要、翻译、多项选择和文本生成。 ️计算机视觉：图像分类、对象检测和分割。 ️音频：自动语音识别和音频分类。r/&gt; 多模态：表格问答、光学字符识别、扫描文档信息提取、视频分类、视觉问答。安装pip install transformers 安装数据集pip install dataset安装Acceleratepip install accelerate安装bitsandbytespip install bitsandbytes教程Pipelinepipeline是使用Transformers库最简单直接的方法，它会自动下载并缓存默认的pre-trained model和tokenizer以进行相应的任务推理。from transformers import pipeline classifier=pipeline("sentiment-analysis") # Inference classifier("We are very happy to show you the Transformers library.") # Output: [{'label': 'POSITIVE', 'score': 0.9998}] # Inference for more than one input results=classifier(["We are very happy to show you the Transformers library.", "We hope you don't hate it."]) for result in results: print(f"label: {result['label']}, with score: {round(result['score'], 4)}") # Output: label: POSITIVE, with score: 0.9998 # label: NEGATIVE, with score: 0.5309 pipelinez支持的任务有TaskDescriptionModalityPipeline identifierText classificationassign a label to a given sequence of textNLPpipeline(task=“sentiment-analysis”)Text generationgenerate text given a promptNLPpipeline(task=“text-generation”)Summarizationgenerate a summary of a sequence of text or documentNLPpipeline(task=“summarization”)Image classificationassign a label to an imageComputer visionpipeline(task=“image-classification”)Image segmentationassign a label to each individual pixel of an image (supports semantic, panoptic, and instance segmentation)Computer visionpipeline(task=“image-segmentation”)Object detectionpredict the bounding boxes and classes of objects in an imageComputer visionpipeline(task=“object-detection”)Audio classificationassign a label to some audio dataAudiopipeline(task=“audio-classification”)Automatic speech recognitiontranscribe speech into textAudiopipeline(task=“automatic-speech-recognition”)Visual question answeringanswer a question about the image, given an image and a questionMultimodalpipeline(task=“vqa”)Document question answeringanswer a question about a document, given an image and a questionMultimodalpipeline(task=“document-question-answering”)Image captioninggenerate a caption for a given imageMultimodalpipeline(task=“image-to-text”)Pipeline还支持自定义pre-trained model和tokenizerfrom transformers import AutoTokenizer, AutoModelForSequenceClassification model_name="nlptown/bert-base-multilingual-uncased-sentiment" model=AutoModelForSequenceClassification.from_pretrained(model_name) tokenizer=AutoTokenizer.from_pretrained(model_name) classifier=pipeline("sentiment-analysis", model=model, tokenizer=tokenizer) classifier("Nous sommes très heureux de vous présenter la bibliothèque Transformers.") # Output: [{'label': '5 stars', 'score': 0.7273}] pipeline参数Devicesgenerator = pipeline(model="openai/whisper-large", device=0) # pip install accelerate # 允许Accelerate自动确定如何在多个GPU上加载和存储模型权重。 generator=pipeline(model="openai/whisper-large", device_map="auto") Batch size不一定比单个推理快generator = pipeline(model="openai/whisper-large", device=0, batch_size=2) audio_filenames=[f"audio_{i}.flac" for i in range(10)] texts=generator(audio_filenames) Task specific parameters# wav2vec2-large-960h-lv60-self的特定参数return_timestamps generator=pipeline(model="facebook/wav2vec2-large-960h-lv60-self", return_timestamps="word") generator("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac") 在自定义数据集上使用piplinedef data(): for i in range(1000): yield f"My example {i}" pipe=pipeline(model="gpt2", device=0) generated_characters=0 for out in pipe(data()): generated_characters += len(out[0]["generated_text"]) # KeyDataset is a util that will just output the item we're interested in. from transformers.pipelines.pt_utils import KeyDataset from datasets import load_dataset pipe=pipeline(model="hf-internal-testing/tiny-random-wav2vec2", device=0) dataset=load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation[:10]") for out in pipe(KeyDataset(dataset, "audio")): print(out) 视觉pipelinefrom transformers import pipeline vision_classifier=pipeline(model="google/vit-base-patch16-224") preds=vision_classifier( images="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg" ) preds=[{"score": round(pred["score"], 4), "label": pred["label"]} for pred in preds] # Output: [{'score': 0.4335, 'label': 'lynx, catamount'}, {'score': 0.0348, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.0324, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.0239, 'label': 'Egyptian cat'}, {'score': 0.0229, 'label': 'tiger cat'}] 文本pipelinefrom transformers import pipeline # This model is a `zero-shot-classification` model. # It will classify text, except you are free to choose any label you might imagine classifier=pipeline(model="facebook/bart-large-mnli") classifier( "I have a problem with my iphone that needs to be resolved asap!!", candidate_labels=["urgent", "not urgent", "phone", "tablet", "computer"], ) # Output: {'sequence': 'I have a problem with my iphone that needs to be resolved asap!!', 'labels': ['urgent', 'phone', 'computer', 'not urgent', 'tablet'], 'scores': [0.504, 0.479, 0.013, 0.003, 0.002]} 多模态pipelinefrom transformers import pipeline vqa=pipeline(model="impira/layoutlm-document-qa") vqa( image="https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png", question="What is the invoice number?", ) # Output: [{'score': 0.42515, 'answer': 'us-001', 'start': 16, 'end': 16}] 结合accelerate使用大模型import torch from transformers import pipeline pipe=pipeline(model="facebook/opt-1.3b", torch_dtype=torch.bfloat16, device_map="auto") output=pipe("This is a cool example!", do_sample=True, top_p=0.95) import torch from transformers import pipeline pipe=pipeline(model="facebook/opt-1.3b", device_map="auto", model_kwargs={"load_in_8bit": True}) output=pipe("This is a cool example!", do_sample=True, top_p=0.95) AutoClassTransformers提供的AutoClass支持从名称或路径自动检索预训练模型的架构。数据预处理在数据集上训练模型之前，需要将其预处理为预期的模型输入格式。无论数据是文本、图像还是音频，都需要将它们转换成批次的张量。AutoTokenizer分词器负责将文本预处理为array，作为模型的输入。有多个规则管理标记化过程，包括如何拆分单词以及应在什么级别拆分单词（在标记器摘要中了解有关标记化的更多信息）。最重要的是要记住，您需要使用相同的模型名称实例化分词器，以确保您使用的是与预训练模型相同的分词规则。from transformers import AutoTokenizer model_name="nlptown/bert-base-multilingual-uncased-sentiment" tokenizer=AutoTokenizer.from_pretrained(model_name) # 用tokenizer来编码文本 encoded_input=tokenizer("Do not meddle in the affairs of wizards, for they are subtle and quick to anger.") print(encoded_input) # Output: {'input_ids': [101, 2079, 2025, 19960, 10362, 1999, 1996, 3821, 1997, 16657, 1010, 2005, 2027, 2024, 11259, 1998, 4248, 2000, 4963, 1012, 102], # 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], # 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]} # 解码文本 tokenizer.decode(encoded_input["input_ids"]) # Output： '[CLS] Do not meddle in the affairs of wizards, for they are subtle and quick to anger. [SEP]' # 多序列输入 batch_sentences=[ "But what about second breakfast?", "Don't think he knows about second breakfast, Pip.", "What about elevensies?", ] encoded_inputs=tokenizer(batch_sentences) print(encoded_inputs) # Output：{'input_ids': [[101, 1252, 1184, 1164, 1248, 6462, 136, 102], # [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102], # [101, 1327, 1164, 5450, 23434, 136, 102]], # 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0], # [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], # [0, 0, 0, 0, 0, 0, 0]], # 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], # [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], # [1, 1, 1, 1, 1, 1, 1]]} tokenizer返回一个字典，其中包含：input_ids：tokens的数字表征。attention_mask：表示应注意哪些tokens。token_type_ids：表示当存在多个序列时token属于哪个序列。Pad句子的长度并不总是相同，这可能是一个问题，因为模型输入的张量需要具有统一的形状。填充是一种通过向较短的句子添加特殊的padding token来确保张量是矩形的策略。batch_sentences = [ "But what about second breakfast?", "Don't think he knows about second breakfast, Pip.", "What about elevensies?", ] encoded_input=tokenizer(batch_sentences, padding=True) print(encoded_input) # Output: {'input_ids': [[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0], # [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102], # [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]], # 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], # [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], # [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], # 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], # [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], # [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]} Truncation另一方面，有时序列对于模型来说可能太长而无法处理。在这种情况下，您需要将序列截断为更短的长度。将参数设置truncation为True将序列截断为模型接受的最大长度：batch_sentences = [ "But what about second breakfast?", "Don't think he knows about second breakfast, Pip.", "What about elevensies?", ] encoded_input=tokenizer(batch_sentences, padding=True, truncation=True) print(encoded_input) # Output: {'input_ids': [[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0], # [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102], # [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]], # 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], # [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], # [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], # 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], # [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], # [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]} Build tensorsbatch_sentences=[ "But what about second breakfast?", "Don't think he knows about second breakfast, Pip.", "What about elevensies?", ] encoded_input=tokenizer(batch_sentences, padding=True, truncation=True, return_tensors="pt") print(encoded_input) # Output: {'input_ids': tensor([[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0], # [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102], # [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]]), # 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], # [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], # [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), # 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], # [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], # [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])} AutoImageProcessor 图像预处理from transformers import AutoImageProcessor from datasets import load_dataset from torchvision.transforms import RandomResizedCrop, ColorJitter, Compose # Load the food101 dataset, split参数可以指定加载数据集的大小 dataset=load_dataset("food101", split="train[:100]") image_processor=AutoImageProcessor.from_pretrained("google/vit-base-patch16-224") # 图像增强 size=( image_processor.size["shortest_edge"] if "shortest_edge" in image_processor.size else (image_processor.size["height"], image_processor.size["width"]) ) _transforms=Compose([RandomResizedCrop(size), ColorJitter(brightness=0.5, hue=0.5)]) def transforms(examples): images=[_transforms(img.convert("RGB")) for img in examples["image"]] examples["pixel_values"] = image_processor(images, do_resize=False, return_tensors="pt")["pixel_values"] return examples dataset.set_transform(transforms) Pad在某些情况下，例如，在微调 DETR 时，模型会在训练时应用规模增强。这可能会导致图像在批处理中具有不同的大小。您可以使用 DetrImageProcessor 中的 DetrImageProcessor.pad_and_create_pixel_mask（） 并定义自定义collate_fn以将图像批处理在一起。def collate_fn(batch): pixel_values=[item["pixel_values"] for item in batch] encoding=image_processor.pad_and_create_pixel_mask(pixel_values, return_tensors="pt") labels=[item["labels"] for item in batch] batch={} batch["pixel_values"] = encoding["pixel_values"] batch["pixel_mask"] = encoding["pixel_mask"] batch["labels"] = labels return batch AutoFeatureExtractor 语音预处理from transformers import AutoFeatureExtractor from datasets import load_dataset, Audio # Load the MInDS-14 dataset dataset=load_dataset("PolyAI/minds14", name="en-US", split="train") dataset[0]["audio"] # Output: {'array': array([ 0. , 0.00024414, -0.00024414, ..., -0.00024414, # 0. , 0. ], dtype=float32), # 'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav', 'sampling_rate': 8000} # 在本教程中，您将使用 Wav2Vec2 模型。看看模型卡，您将了解到Wav2Vec2是在16kHz采样语音音频上预先训练的。音频数据的采样率与用于预训练模型的数据集的采样率相匹配非常重要。如果数据的采样率不同，则需要对数据重新采样。 dataset=dataset.cast_column("audio", Audio(sampling_rate=16_000)) dataset[0]["audio"] # Output: {'array': array([ 2.3443763e-05, 2.1729663e-04, 2.2145823e-04, ..., # 3.8356509e-05, -7.3497440e-06, -2.1754686e-05], dtype=float32), # 'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav', 'sampling_rate': 16000} # 加载特征提取器以规范化和填充输入。填充文本数据时，为较短的序列添加 0。同样的想法适用于音频数据。特征提取器向数组添加一个 0 - 解释为静音。 feature_extractor=AutoFeatureExtractor.from_pretrained("facebook/wav2vec2-base") audio_input=[dataset[0]["audio"]["array"]] feature_extractor(audio_input, sampling_rate=16000) # 创建一个函数来预处理数据集，使音频样本的长度相同。指定最大样本长度，特征提取器将填充或截断序列以匹配它： def preprocess_function(examples): audio_arrays=[x["array"] for x in examples["audio"]] inputs=feature_extractor( audio_arrays, sampling_rate=16000, padding=True, max_length=100000, truncation=True, ) return inputs processed_dataset=preprocess_function(dataset[:5]) processed_dataset["input_values"][0].shape # Output: (100000,) processed_dataset["input_values"][1].shape # Output: (100000,) AutoProcessor 多模态预处理from transformers import AutoProcessor from datasets import load_dataset # Load the LJ Speech dataset (for automatic speech recognition (ASR)) lj_speech=load_dataset("lj_speech", split="train") # 删掉不关注的列 lj_speech=lj_speech.map(remove_columns=["file", "id", "normalized_text"]) lj_speech[0]["audio"] # Output：{'array': array([-7.3242188e-04, -7.6293945e-04, -6.4086914e-04, ..., # 7.3242188e-04, 2.1362305e-04, 6.1035156e-05], dtype=float32), # 'path': '/root/.cache/huggingface/datasets/downloads/extracted/917ece08c95cf0c4115e45294e3cd0dee724a1165b7fc11798369308a465bd26/LJSpeech-1.1/wavs/LJ001-0001.wav', 'sampling_rate': 22050} lj_speech[0]["text"] # Output：'Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition' processor=AutoProcessor.from_pretrained("facebook/wav2vec2-base-960h") # 创建一个函数来处理数组中包含的音频数据作为input_values，并将文本标记为标签。 def prepare_dataset(example): audio=example["audio"] example.update(processor(audio=audio["array"], text=example["text"], sampling_rate=16000)) return example # 调用函数处理一个样本 prepare_dataset(lj_speech[0]) AutoModel为任务选择正确的自动模型。对于文本（或序列）分类，您应该加载AutoModelForSequenceClassification：from transformers import AutoModelForSequenceClassification from torch import nn model_name="nlptown/bert-base-multilingual-uncased-sentiment" pt_model=AutoModelForSequenceClassification.from_pretrained(model_name) pt_outputs=pt_model(**pt_batch) pt_predictions=nn.functional.softmax(pt_outputs.logits, dim=-1) print(pt_predictions) # Output: tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725], # [0.2084, 0.1826, 0.1969, 0.1755, 0.2365]], grad_fn=&lt;SoftmaxBackward0&gt;) Save &amp; load modelsfrom transformers import AutoModelForSequenceClassification # Save the model pt_save_directory="./pt_save_pretrained" tokenizer.save_pretrained(pt_save_directory) pt_model.save_pretrained(pt_save_directory) # Load the model pt_model=AutoModelForSequenceClassification.from_pretrained("./pt_save_pretrained") # 支持加载不同架构（torch, tensorflow）学习的模型 tf_save_directory="./tf_save_pretrained" tokenizer=AutoTokenizer.from_pretrained(tf_save_directory) pt_model=AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True) 数据预处理文本，使用Tokenizer将文本转换为一系列标记，创建标记的数字表示，并将它们组装成张量。语音和音频，使用特征提取器从音频波形中提取序列特征并将其转换为张量。图像输入使用ImageProcessor将图像转换为张量。多模态输入，使用处理器来组合分词器和特征提取器或图像处理器。自定义模型构建您可以修改模型的配置类以更改模型的构建方式。该配置指定模型的属性，例如隐藏层或注意头的数量。当您从自定义配置类初始化模型时，您是从头开始的。模型属性是随机初始化的，您需要先训练模型，然后才能使用它获得有意义的结果。from transformers import AutoConfig, AutoModel my_config=AutoConfig.from_pretrained("distilbert-base-uncased", n_heads=12) my_model=AutoModel.from_config(my_config) https://huggingface.co/docs/transformers/v4.29.1/en/create_a_model训练所有模型都是标准模型torch.nn.Module，因此您可以在任何典型的训练循环中使用它们。虽然您可以编写自己的训练循环，但 Transformers为 PyTorch提供了一个Trainer类，其中包含基本的训练循环并为分布式训练、混合精度等特性添加了额外的功能。from transformers import AutoModelForSequenceClassification, TrainingArguments, AutoTokenizer， DataCollatorWithPadding, Trainer # A PreTrainedModel or a torch.nn.Module model=AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased") # TrainingArguments 包含可以更改的模型超参数，例如学习率、批量大小和要训练的 epoch 数。如果未指定任何训练参数，则使用默认值 training_args=TrainingArguments( output_dir="path/to/save/folder/", learning_rate=2e-5, per_device_train_batch_size=8, per_device_eval_batch_size=8, num_train_epochs=2, ) # 预处理类，如分词器、图像处理器、特征提取器或处理器 tokenizer=AutoTokenizer.from_pretrained("distilbert-base-uncased") # 加载数据集 dataset=load_dataset("rotten_tomatoes") # 创建一个函数来标记数据集，并用它处理整个数据集 def tokenize_dataset(dataset): return tokenizer(dataset["text"]) dataset=dataset.map(tokenize_dataset, batched=True) # DataCollatorWithPadding 用于从数据集创建一批示例 data_collator=DataCollatorWithPadding(tokenizer=tokenizer) # 开始训练 trainer=Trainer( model=model, args=training_args, train_dataset=dataset["train"], eval_dataset=dataset["test"], tokenizer=tokenizer, data_collator=data_collator, ) # doctest: +SKIP trainer.train() 对于使用序列到序列模型的任务（如翻译或摘要），请改用Seq2SeqTrainer和Seq2SeqTrainingArguments类。您可以通过子类化Trainer中的方法来自定义训练循环行为。这使您可以自定义损失函数、优化器和调度程序等功能。查看Trainer参考，了解哪些方法可以被子类化。自定义训练循环的另一种方法是使用回调。您可以使用回调与其他库集成并检查训练循环以报告进度或提前停止训练。回调不会修改训练循环本身的任何内容。要自定义损失函数之类的东西，您需要改为对Trainer进行子类化。Fine-tune预训练模型from datasets import load_dataset from transformers import AutoTokenizerf, AutoModelForSequenceClassification, TrainingArguments import numpy as np import evaluate # --- Prepare dataset --- dataset=load_dataset("yelp_review_full") dataset["train"][100] # {'label': 0, # 'text': 'My expectations for McDonalds are t rarely high. But for one to still fail so spectacularly...that takes something special!\\nThe cashier took my friends\'s order, then promptly ignored me. I had to force myself in front of a cashier who opened his register to wait on the person BEHIND me. I waited over five minutes for a gigantic order that included precisely one kid\'s meal. After watching two people who ordered after me be handed their food, I asked where mine was. The manager started yelling at the cashiers for \\"serving off their orders\\" when they didn\'t have their food. But neither cashier was anywhere near those controls, and the manager was the one serving food to customers and clearing the boards.\\nThe manager was rude when giving me my order. She didn\'t make sure that I had everything ON MY RECEIPT, and never even had the decency to apologize that I felt I was getting poor service.\\nI\'ve eaten at various McDonalds restaurants for over 30 years. I\'ve worked at more than one location. I expect bad days, bad moods, and the occasional mistake. But I have yet to have a decent experience at this store. It will remain a place I avoid unless someone in my party needs to avoid illness from low blood sugar. Perhaps I should go back to the racially biased service of Steak n Shake instead!'} tokenizer=AutoTokenizer.from_pretrained("bert-base-cased") def tokenize_function(examples): return tokenizer(examples["text"], padding="max_length", truncation=True) tokenized_datasets=dataset.map(tokenize_function, batched=True) # --- Train --- model=AutoModelForSequenceClassification.from_pretrained("bert-base-cased", num_labels=5) training_args=TrainingArguments(output_dir="test_trainer") metric=evaluate.load("accuracy") def compute_metrics(eval_pred): logits, labels=eval_pred predictions=np.argmax(logits, axis=-1) return metric.compute(predictions=predictions, references=labels) trainer=Trainer( model=model, args=training_args, train_dataset=small_train_dataset, eval_dataset=small_eval_dataset, compute_metrics=compute_metrics, ) trainer.train() 本文使用 Zhihu On VSCode 创作并发布]]></summary></entry><entry><title type="html">从零开始认识大模型：一篇非从业者指南</title><link href="https://skylark0924.github.io/blog/2023/.md/" rel="alternate" type="text/html" title="从零开始认识大模型：一篇非从业者指南"/><published>2023-05-18T13:47:44+00:00</published><updated>2023-05-18T13:47:44+00:00</updated><id>https://skylark0924.github.io/blog/2023/.md</id><content type="html" xml:base="https://skylark0924.github.io/blog/2023/.md/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[【LM 00】从零开始认识大模型：一篇非从业者指南什么是ChatGPT？什么是GPT-4？Bard和PaLM又是什么？它们有什么关系？它们对我们有什么影响？AI技术第一次这么密集地冲击着非从业者们，身处变革之中的我们或许会迷茫，但这本来就是工业革命之后的常态，我们要做的就是像九十年代学习计算机一样拥抱未来。在ChatGPT刚出的时候，我对未来的知识创作是抱着悲观态度的。我认为个人创作者会被ChatGPT挤兑，或者直接投降借助ChatGPT来创作。久而久之，知识会陷入到封闭的循环当中，知乎也会沦为一个ChatGPT版公众号的聚集地，所以有一段时间我卸载了知乎。但是不久之后，我就发现自己这样的想法就和1900年的保皇派一样，看似是看清了变革的方向，实际却是裹足不前。对于一个新事物，体系性的学习是理解和掌握它的必要途径，也是最便捷的方式。如果只是试图从公众号庞杂的文章中了解，无异于管中窥豹。大模型是什么首先，大模型这个词是建立在神经网络模型上的。神经网络是一种在过去二十年得到广泛研究的人工智能方法，我们试图使用这样的模型模拟人类的思维和决策。神经网络的本质是在每个人工神经元（图1a的圆形）上设定一些可以调节的参数，并能够根据这些参数对输入到该神经元的数据进行加权求和 和非线性变换 ，使其能够借助学习/优化算法实现从输入值 到输出值 的映射 。这个描述映射关系的函数 是图1a中众多小 们和加权求和函数叠加作用的结果，它在早些年（21世纪初）还是一门玄学，但是之后就有很多针对神经网络的可解释性工作，试图揭开这背后的隐秘。图1 神经网络这种映射关系的作用十分广泛，你可以用特定的数据（一些提前制作好的输入-输出对）作为它的训练数据，用学习算法来调节网络参数，使其实现类似于训练数据的映射关系。广义上来说，给它什么样的训练数据，就能通过神经网络建立什么样的映射关系。例如，动物的图片 -- 动物的类别中文 -- 对应的英文翻译语音 -- 对应的文字内容视频 -- 视频中的事件描述机器人的视觉反馈 -- 机器人下一步的动作听起来是十分美好的，仿佛我们可以借助神经网络构建万物之间的联系、帮助我们更好地理解世界。但是这其实并不容易，映射关系建立的好与坏通常受到以下几种条件的影响：模型架构训练数据的数量训练数据的质量学习算法计算硬件算力训练数据的数量和质量在这二十年间已经得到了长足的发展，很多TB级别的数据集被开源出来作为模型的测试基准。这不得不感谢东南亚及印度的廉价数据标注工人，在资本温柔的剥削下，他们做着这个世纪最伟大的事情。有了大量的数据以及英伟达一年翻一倍的硬件算力，学术界更聚焦在如何设计出一个美妙的模型架构。我们找到了很多好用的基础模型，也设计过很多眼花缭乱的网络架构，但最后发现还是大力出奇迹---好的基础模型的大量堆叠就能出现意想不到的效果。图1b展示了一个大一点的神经网络，它会被叫做深度神经网络，但还不值得被称为大模型。大模型的模型参数量达到了亿的级别，近期的研究成果基本在百亿到千亿的范围。当然这不会是终点，大模型的神经元数量和相关参数量必定会朝着远超人类大脑神经元数量的方向发展。这既体现了现有人工神经网络方法的低效性（当然是比不过地球亿万年的进化），也彰显着人类的雄心。相信至此，你已经对大模型中的大和模型都有了一定的了解，那么我们就可以回答一开始的问题了。ChatGPT是美国著名AI研究所/公司---OpenAI的一款AI在线问答产品。之所以能够火出圈，是因为实在效果太震撼了。你可以以一种全新的、交互式的方式获取之前需要搜索引擎查找的信息。同时，这些信息是由模型生成出来的，也就意味着它们在这之前是并不完全地存在于互联网（以及人类知识库）中的。它基于的技术就是一种叫做 Generative Pre-trained Transformer (GPT, 生成式预训练Transformer) 的模型。以上面的方式简单来讲，ChatGPT以一种更加玄学的方式构建了你的问题和你可接受的答案之间的映射。很明显，GPT-4就代表了GPT这种模型的第四版（更大地参数量、更多的模型优化）。Bard呢，是谷歌公司的竞品，也是在线问答，可以做表格也可以写文案，总体来说略逊于ChatGPT。它背后的技术也在不断地迭代，从LaMDA到最近刚发布的PaLM2。最强语言模型 PaLM2 亮相，Bard 能力跃升，它可以实现哪些功能？算是 ChatGPT 杀手吗？大模型的优势那么为什么模型越大，构建映射的能力就越强了呢？尽管这看起来是很理所当然的事情，也可以从生物演化的角度来解释，但是总归是一种无法描述的玄学感。近期的一些工作将模型大的优势解释为大模型所带来的涌现能力和思维链构建能力。涌现涌现还确实是个生物演化的概念。它描述的是一种现象，即复杂系统具有某些组成它的小系统所不具备的特性。图2 图源：http://slide.tech.sina.com.cn/d/slide_5_453_74691.html#p=1这种现象在自然界比较常见，例如，水分子在构建成雪花时普遍呈现出的六角型状态，生物能够由基本粒子构成却具备高级生物功能的神奇现象，以及生物群体能够自发形成秩序社会的复杂行为。这些都暗示着大模型的成功背后具备着某种自然界的神秘力量，一种我们暂时还没有能力解析的混沌系统。但我相信，随着大模型的发展，这种解析并不遥远。思维链大模型涌现能力的直接结果就是能够构建出思维链，这是一种推理能力。下图就是思维链这篇论文给出的例子图3 图源：Chain-of-Thought Prompting Elicits Reasoning in Large Language Models如果只是单纯的文本生成，大模型在简单的加减法上都会有很大的问题，因为这里隐含着逻辑推理的能力，并非单纯的文本。但是如果我们同时将问题的解题思路描述给大模型，就会得到有理有据的正确结论。通过这样的方式训练的大模型，就具备了将一个问题拆解成许多子问题的能力，从而提升了答案的可靠性。这种能力将在机器人操作中大放异彩，会成为促进机器人应用落地的重要一环。我之后会专门就机器人大模型和决策大模型的研究现状与光明前景写一篇文章，感兴趣的小伙伴可以关注我的知乎哦。参考文献Emergent Abilities of Large Language ModelsChain-of-Thought Prompting Elicits Reasoning in Large Language Models大语言模型(LLM)族谱上文一直在使用大模型一词，实际上大模型已在短短四年间发展壮大成为一个人工智能的主流研究方向。大模型的发展最初是伴随着自然语言处理技术的不断发展的，这是由于文本数据的数据量更大且更容易获取。所以目前大模型最大的分类还是大语言模型，近两年衍生出一些语言与其他形式融合的大模型，例如，文字生成音乐（MusicLM）、文字生成图像（DALL-E2, Midjourney）、文字图像生成机器人动作（RT-1）等。大模型包括但不限于以下几类：大语言模型视觉大模型多模态大模型决策大模型机器人大模型下图是我绘制的大语言模型从2019-2023的发展路径。目前大多数的大语言模型都是建立在一个叫做 Transformer 的基础模型之上的（下一篇会介绍）。而根据使用的 Transformer 的方式不同，LLM的构建就被分为三条研究路径：编码器-解码器结构、只使用解码器、只使用编码器。绿色标注的是开源可用的模型，部分模型后有相应的参数量（以B--十亿为单位）。大语言模型（实时更新）上图在实时更新中。参考文献Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and BeyondHolistic Evaluation of Language Models多模态大模型(MLM)族谱绿色为开源模型。部分基础模型例如ViT和MAE等并非视觉语言模型，而是单纯的视觉模型，这里列出是出于其重要性的考虑。多模态大模型（实时更新）上图在实时更新中。如何与大模型共生我们有一个无法摆脱的命运，那就是和技术共生。田园牧歌的生活只会逐渐成为奢望，我们在不断被迫接受着过量的信息和超出认知的技术革新，否则就会处于被革新的尴尬境地。那么大模型也一样，我们要寻求共生之道。我在这个回答中给出了一个理想主义的答案，即在大模型等技术的支撑下，我们能够从劳作中解脱出来，可以有更多的精力探寻人存在的价值。怎么看待人工智能与人类的关系？但这终究是个过分理想的叙事角度，不难想象，在不久的将来，我们会在高频迭代的AI技术的逼促下，更辛苦地劳作以赶上时代的发展（毕竟我就在加班看大模型论文），或者彻底屈服于其下做一个便宜稳定的数据源。所以看起来，路子只有一条，了解它并学会使用它，就像学会使用电脑为我们创造价值一样。现在还不晚，还来得及拥抱未来。本文使用 Zhihu On VSCode 创作并发布]]></summary></entry><entry><title type="html">ChatGPT版Siri: 基于ChatGPT和ROS的机器人语音问答</title><link href="https://skylark0924.github.io/blog/2023/chatgptsiri-chatgptros/" rel="alternate" type="text/html" title="ChatGPT版Siri: 基于ChatGPT和ROS的机器人语音问答"/><published>2023-03-18T14:10:02+00:00</published><updated>2023-03-18T14:10:02+00:00</updated><id>https://skylark0924.github.io/blog/2023/chatgptsiri-chatgptros</id><content type="html" xml:base="https://skylark0924.github.io/blog/2023/chatgptsiri-chatgptros/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[ChatGPT 的出现让很多有趣的想法成为了可能。我最近一直在思考 LLM 及其相关技术与 Robot Manipulation 相结合的可能，也尝试着做了一些工作，效果还是可以的。如果能够 accept，再给大家分享一下。今天主要介绍一个有趣的开源小组件，即利用 ChatGPT API 以及 AWS 的语音 API，在 ROS 上实现机器人与人类之间的交互式语音问答。开源代码库如下，详细的 API 获取和安装方式已经写在其中了，详见 Installation Manualhttps://github.com/Skylark0924/Rofunc-ros具体来说，这个功能由 Speech2text, Chat 和Text2speech 三部分组成。给大家画个简单的图Voice Q&amp;A 使用方式得益于ROS天然的多线程结构，我们可以以一行代码开启这个功能，并让各个模块持续长时间地保持监听。Englishroslaunch rofunc_ros voice_qa_en.launch Chineseroslaunch rofunc_ros voice_qa_cn.launch 子模块使用方式Speech2textEnglishroslaunch rofunc_ros speech2text_en.launch Chineseroslaunch rofunc_ros speech2text_cn.launch ChatOpen a terminal to run the launch fileroslaunch rofunc_ros start.launch Open another terminal to feed the questionrosrun rofunc_ros str_pub.py "Hello" Text2speechOpen a terminal to run the launch fileroslaunch rofunc_ros text2speech.launch Open another terminal to give the text rosrun rofunc_ros voicer.py 'Hello everyone! I am CURI, a humanoid robot designed by \ collaborative and versatile robots laboratory. Our lab focuses on the co-evolutionary\ development of human-centered robotics and AI technologies for advanced robots, such as \ human-like mobile manipulators, humanoid robots, to perform autonomous, assistive and \ collaborative tasks by learning and transferring the skills from humans.' 希望能为大家的机器人研究增加一些乐趣]]></summary></entry><entry><title type="html">Rofunc：迈向高冗余人型机器人的多模态模仿学习</title><link href="https://skylark0924.github.io/blog/2022/rofunc/" rel="alternate" type="text/html" title="Rofunc：迈向高冗余人型机器人的多模态模仿学习"/><published>2022-11-20T07:57:15+00:00</published><updated>2022-11-20T07:57:15+00:00</updated><id>https://skylark0924.github.io/blog/2022/rofunc</id><content type="html" xml:base="https://skylark0924.github.io/blog/2022/rofunc/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[读博一周年纪念专栏把 Rofunc 的 Github Repository 放个显眼的位置Skylark0924/Rofunc - Github欢迎 Star，欢迎 Issue，欢迎 Contributor，欢迎一切志同道合的小伙伴！！！序早就想开新专栏了，一直拖延到一周年零几个月。。。截至目前，旧的 RL in Robotics 专栏已累计98篇内容、5728个赞同。感谢各位对当年那个在迷雾中肆意探索的我给予的支持与鼓励，让我们缅怀一下这个即将完结的专栏：RL in Robotics新专栏会更关注于机器人本身，尤其是更具挑战性的高冗余人型/类人型机器人的操作和行为，而不会像之前一样沉迷于某种听起来仿佛很厉害、但可能没大用的新技术。这可能也是我读博一年来的心得体会吧，总算是学会了从问题出发，而不是拿着锤子找钉子。这里就要感谢一下我导陈翡老师啦，以及一直为我提供硬核技术指导的 Sylvain Calinon 老师。还有一点与之前不同的是，这次的专栏其实也是我们正在推进的开源机器人模仿学习库 Rofunc 的中文教程。在科研的过程中分心搞工具包开发的初衷也很简单。在深度学习领域，我们有 torch；在图学习领域，我们有 DGL；在强化学习领域，我们有 RLlib。为什么机器人领域就不能拥有这种易用的、普适的，且能够一览机器人任务从数据到部署全流程的工具包呢？技术的本质就应该是解蔽，而不是打着科研招牌的故弄玄虚。只有机器人门槛放低，才会吸引更多志同道合的朋友投入到机器人社区的建设中，才会最终迎来那姗姗来迟的机器人革命。那么，先上 Rofunc 的 Github Repository 吧！Skylark0924/Rofunc - Github欢迎 Star，欢迎 Issue，欢迎 Contributor，欢迎一切志同道合的小伙伴！！！概览机器人的模仿学习/示教学习并不是一个很新的话题，甚至读博这一年我也接触到了欧洲和美国两种不同的思想流派。像 Billard 教授和 Calinon 老师这样的欧洲研究者坚守在拖动示教以及由此衍生基于概率和流形的示教学习方法（可以参考一下 李淼老师的回答）。而美国呢，貌似更喜欢基于视频示教直接使用DL、RL等 NN-based 方法。这两种思路是各有优势，也是各有缺点的，优劣是需要根据具体任务来评判的。我们想做的，就是在 Rofunc 中同时引入这两种思路，为 peer researchers 提供一个模仿学习的 baseline 甚至是 benchmark package。除了 learning 模块外，我们还有更大的野心：提供从多模态数据采集与处理、示教学习到机器人规划与控制、以及具有多种类人机器人仿真器的全流程示教学习 pipeline。 对于高冗余度的类人型机器人来说，从人类演示中学习 (learning from human demonstration) 是获取新的、复杂的技能最自然且便捷的解决方案。我们提供了多视角视觉（ZED camera）、人类运动学（Xsens MTw Awinda）、物体运动学（Optitrack）以及人体生物力学（Delsys sEMG）的多传感器、多模态采集与处理方案，提供了基于优化的机器人规划与控制方法，也同样提供了基于 Isaac Gym 的多种机器人（Franka, CURI, Ubtech Walker, Diablo等）仿真器。目录持续更新中本文使用 Zhihu On VSCode 创作并发布]]></summary></entry><entry><title type="html">Rofunc：迈向高冗余人型机器人的多模态模仿学习</title><link href="https://skylark0924.github.io/blog/2022/rofunc/" rel="alternate" type="text/html" title="Rofunc：迈向高冗余人型机器人的多模态模仿学习"/><published>2022-11-20T07:57:15+00:00</published><updated>2022-11-20T07:57:15+00:00</updated><id>https://skylark0924.github.io/blog/2022/rofunc</id><content type="html" xml:base="https://skylark0924.github.io/blog/2022/rofunc/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[读博一周年纪念专栏把 Rofunc 的 Github Repository 放个显眼的位置Skylark0924/Rofunc - Github欢迎 Star，欢迎 Issue，欢迎 Contributor，欢迎一切志同道合的小伙伴！！！序早就想开新专栏了，一直拖延到一周年零几个月。。。截至目前，旧的 RL in Robotics 专栏已累计98篇内容、5728个赞同。感谢各位对当年那个在迷雾中肆意探索的我给予的支持与鼓励，让我们缅怀一下这个即将完结的专栏：RL in Robotics新专栏会更关注于机器人本身，尤其是更具挑战性的高冗余人型/类人型机器人的操作和行为，而不会像之前一样沉迷于某种听起来仿佛很厉害、但可能没大用的新技术。这可能也是我读博一年来的心得体会吧，总算是学会了从问题出发，而不是拿着锤子找钉子。这里就要感谢一下我导陈翡老师啦，以及一直为我提供硬核技术指导的 Sylvain Calinon 老师。还有一点与之前不同的是，这次的专栏其实也是我们正在推进的开源机器人模仿学习库 Rofunc 的中文教程。在科研的过程中分心搞工具包开发的初衷也很简单。在深度学习领域，我们有 torch；在图学习领域，我们有 DGL；在强化学习领域，我们有 RLlib。为什么机器人领域就不能拥有这种易用的、普适的，且能够一览机器人任务从数据到部署全流程的工具包呢？技术的本质就应该是解蔽，而不是打着科研招牌的故弄玄虚。只有机器人门槛放低，才会吸引更多志同道合的朋友投入到机器人社区的建设中，才会最终迎来那姗姗来迟的机器人革命。那么，先上 Rofunc 的 Github Repository 吧！Skylark0924/Rofunc - Github欢迎 Star，欢迎 Issue，欢迎 Contributor，欢迎一切志同道合的小伙伴！！！概览机器人的模仿学习/示教学习并不是一个很新的话题，甚至读博这一年我也接触到了欧洲和美国两种不同的思想流派。像 Billard 教授和 Calinon 老师这样的欧洲研究者坚守在拖动示教以及由此衍生基于概率和流形的示教学习方法（可以参考一下 李淼老师的回答）。而美国呢，貌似更喜欢基于视频示教直接使用DL、RL等 NN-based 方法。这两种思路是各有优势，也是各有缺点的，优劣是需要根据具体任务来评判的。我们想做的，就是在 Rofunc 中同时引入这两种思路，为 peer researchers 提供一个模仿学习的 baseline 甚至是 benchmark package。除了 learning 模块外，我们还有更大的野心：提供从多模态数据采集与处理、示教学习到机器人规划与控制、以及具有多种类人机器人仿真器的全流程示教学习 pipeline。 对于高冗余度的类人型机器人来说，从人类演示中学习 (learning from human demonstration) 是获取新的、复杂的技能最自然且便捷的解决方案。我们提供了多视角视觉（ZED camera）、人类运动学（Xsens MTw Awinda）、物体运动学（Optitrack）以及人体生物力学（Delsys sEMG）的多传感器、多模态采集与处理方案，提供了基于优化的机器人规划与控制方法，也同样提供了基于 Isaac Gym 的多种机器人（Franka, CURI, Ubtech Walker, Diablo等）仿真器。目录持续更新中本文使用 Zhihu On VSCode 创作并发布]]></summary></entry><entry><title type="html">自定义pip package打包——看这篇就够了</title><link href="https://skylark0924.github.io/blog/2022/pip-package/" rel="alternate" type="text/html" title="自定义pip package打包——看这篇就够了"/><published>2022-09-22T14:01:07+00:00</published><updated>2022-09-22T14:01:07+00:00</updated><id>https://skylark0924.github.io/blog/2022/pip-package</id><content type="html" xml:base="https://skylark0924.github.io/blog/2022/pip-package/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[项目结构首先，需要按照下列格式组织你的 packageproject （项目名称，随意，与package无关） |----package （这个才是包名） |----__init__.py （__init__包内每个文件夹下都要有） |----code.py （具体的代码） |----other （package外可以放一些doc, img之类的，不会被封装到package中） |----__init__.py （空着即可） |----setup.py （构建脚本） init.py 如何写？对于包内的 __init__.py文件的写法，以下面的包内文件结构为例|----rofunc （包名） |----devices （一级文件夹） |----xsens （二级文件夹） |----record.py （包含record函数） |----process.py （包含data_clean函数） |----__init__.py （三级__init__） |----optitrack |----record.py |----process.py |----__init__.py |----... |----__init__.py （二级__init__） |----lfd |----... |----__init__.py （一级__init__） 那么如果想要实现以下索引，该怎么写这几层的__init__.py呢？import rofunc as rf rf.xsens.data_clean(...) 需要在一级__init__.py中from __future__ import absolute_import from .devices import xsens, optitrack, ... 2. 二级__init__.py空着3. 三级__init__.pyfrom __future__ import absolute_import from .record import * from .process import * 具体在你的包中该如何组织多层__init__.py，实际上要看你对于功能调用格式的设计了。setup.py 如何写？# -*- coding:utf-8 -*- from distutils.core import setup from setuptools import find_packages setup(name='declare', version='0.1', packages=find_packages(where='src\\'), # 查找包的路径 package_dir={'': 'src'}, # 包的root路径映射到的实际路径 include_package_data=False, package_data={'data': []}, description='A python lib for xxxxx', long_description='', author='python developer', author_email='xxxxxxx@qq.com', url='http://www.xxxxx.com/', # homepage license='MIT', install_requires=['requests', 'selenium', 'baidu-aip', 'pillow', 'pywin32'], ) # name : 打包后包的文件名 # version : 版本号 # author : 作者 # author_email : 作者的邮箱 # py_modules : 要打包的.py文件 # packages: 打包的python文件夹 # include_package_data : 项目里会有一些非py文件,比如html和js等,这时候就要靠include_package_data 和 package_data 来指定了。package_data:一般写成{‘your_package_name’: [“files”]}, include_package_data还没完,还需要修改MANIFEST.in文件.MANIFEST.in文件的语法为: include xxx/xxx/xxx/.ini/(所有以.ini结尾的文件,也可以直接指定文件名) # license : 支持的开源协议 # description : 对项目简短的一个形容 # ext_modules : 是一个包含Extension实例的列表,Extension的定义也有一些参数。 # ext_package : 定义extension的相对路径 # requires : 定义依赖哪些模块 # provides : 定义可以为哪些模块提供依赖 # data_files :指定其他的一些文件(如配置文件),规定了哪些文件被安装到哪些目录中。如果目录名是相对路径,则是相对于sys.prefix或sys.exec_prefix的路径。如果没有提供模板,会被添加到MANIFEST文件中。 实例from setuptools import setup, find_packages setup( name="rofunc", version="0.0.0.9", description='The Full Process Python Package for Robot Learning from Demonstration', author="Junjia Liu", author_email="jjliu@mae.cuhk.edu.hk", url='https://github.com/Skylark0924/Rofunc', packages=find_packages(), install_requires=['matplotlib', 'pandas', 'tqdm', 'pillow', 'pytransform3d', 'tensorflow', 'numpy==1.21.6', 'nestle', 'pbdlib @ https://github.com/Skylark0924/Rofunc/releases/download/v0.0.0.7.1/pbdlib-0.1-py3-none-any.whl'], python_requires="&gt;=3.6,&lt;3.11", keywords=['robotics', 'learning from demonstration'], license='MIT', classifiers=[ "Programming Language :: Python :: 3", "License :: OSI Approved :: MIT License", "Operating System :: OS Independent", ], ) 这里重点介绍一下 install_requires 的一些冷知识：如果依赖的包没有发布在PYPI，又不想/不能封在自己的包内，那么可以参照我对于 Sylvain Calinon 老师的pbdlib包的处理方法。pbdlib包是一个托管在 Gitlab 上的示教/模仿学习python库，原地址为 https://gitlab.idiap.ch/rli/pbdlib-python 。可以发现，这个库里是有setup.py的，但是只被用来做 git clone 之后的本地安装。我的解决方案是：本地打包（参照下一节的内容）：git clone ...python setup.py bdist_wheel sdist你会发现，主目录下会生成build和dist两个文件夹，dist下可以找被我们称作“轮子”的，对该库封装的压缩包pbdlib-0.1-py3-none-any.whl将其作为Release文件，上传到github的某个版本的Release中（当然，你自己有个人网站也是可以的，能够完整下载就行）在install_requires中，以包名+url的形式加入依赖'pbdlib @ https://github.com/Skylark0924/Rofunc/releases/download/v0.0.0.7.1/pbdlib-0.1-py3-none-any.whl'即可实现用户只需pip install rofunc，自动安装pbdlib依赖库。很遗憾的是，尽管这样的写法允许我们在本地安装的时候直接安装这些未上传到PYPI的依赖，但是由于规则限制，拥有这种直接链接的package无法被推送到pypi。会收到如下报错：HTTPError: 400 Client Error: '[your requirements]' is an invalid value for Version. Error: Can't use PEP 440 local versions. See https://packaging.python.org/specifications/core-metadata for url: https://test.pypi.org/legacy/： 详情请参见 issue 430，以及这个链接。包内数据Setuptools有关于这部分的教程，但是你会发现需要指定文件类型，而且每个文件夹下都要有一个__init__.py。这么麻烦的方式肯定不适合把整个文件夹的数据打包起来。于是，我就发现可以通过在project目录下再写一个 MANIFEST.in文件来实现。通过直接指定想要添加的数据路径就ok了recursive-include rofunc/data/ * recursive-include rofunc/simulator/assets/ * 此外，还需要在setup.py文件中加一行来指向MANIFEST.in文件include_package_data=True ReferencePython setup.py和MANIFEST.in文件The MANIFEST.in template打包、安装与推送打包python setup.py bdist_wheel # 打包为whl文件 python setup.py sdist # 打包为tar.gz文件 python setup.py bdist_wheel sdist # 也可以一起写，省事 本地安装执行完之后会在当前目录生成 dist 文件夹，文件夹内部是编译好的 python 包，whl 后缀结尾cd dist 执行安装命令pip install rofunc-0.0.0.9-py3-none-any.whl 卸载pip uninstall rofunc 值得注意的是，在开发阶段，应该以本地安装-&gt;加功能-&gt;测试-&gt;打包-&gt;本地安装为主，等到完成了一定的功能并测试ok之后，再上传PYPI。这是因为 PYPI上，要求版本号的唯一性（即使删除了也没用）。所以如果不想在网站上搞出很多个版本的话，建议考虑稳妥之后再上传。自动化shell脚本为简化测试流程，我还给大家提供了上述流程的 shell 脚本写法，在 project 目录下，创建 scripts 文件夹，并在其中创建 update.sh 脚本，内容为#!/bin/bash -i alias pip3p="/home/ubuntu/anaconda3/envs/plast/bin/pip" alias pyana3p="/home/ubuntu/anaconda3/envs/plast/bin/python3.7" # 以上两行针对使用别名指定python版本的同学，大家按需采用 cd ../dist pip3p uninstall -y rofunc cd .. pyana3p setup.py bdist_wheel sdist cd ./dist pip3p install rofunc-0.0.0.9-py3-none-any.whl 由此，即可通过简单的一行命令，实现本地打包及安装sh update.sh 上传 PYPI先在 pypi 上注册一个账号 https://pypi.org/# 上传需要安装twine pip install twine twine upload dist/* # 输入刚注册的用户名密码就能上传。 使用token自动登录PYPI账号每次都要输入用户名密码才能上传，想想就麻烦，可以用token自动化这个过程创建token\home\[usename]目录下创建~/.pypirc文件并添加[pypi] username=__token__ password=（创建的token） 验证 pypi 上传成功pip uninstall &lt;package_name&gt; # 卸载本地包 pip install &lt;package_name&gt; # 安装PYPI包 本文使用 Zhihu On VSCode 创作并发布]]></summary></entry><entry><title type="html">量子计算应用Ⅱ：量子隐形传输</title><link href="https://skylark0924.github.io/blog/2022/.md/" rel="alternate" type="text/html" title="量子计算应用Ⅱ：量子隐形传输"/><published>2022-03-12T14:27:27+00:00</published><updated>2022-03-12T14:27:27+00:00</updated><id>https://skylark0924.github.io/blog/2022/.md</id><content type="html" xml:base="https://skylark0924.github.io/blog/2022/.md/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[我记得在一次散步中，爱因斯坦突然停下来，看向我，问我是否真的相信月亮只有在我看着它时才存在。然后这次步行的其余时间，我们都在讨论物理学家应该用存在这个词来表达什么意思。(I recall that during one walk Einstein suddenly stopped, turned to me and asked whether I really believed that the moon exists only when I look at it. The rest of this walk was devoted to a discussion of what a physicist should mean by the term 'to exist'.)—— Abraham Pais参考教材：老师的ppt和 Quantum Computation and Quantum Information量子计算应用Ⅱ：量子隐形传输定义实现例子定义量子隐形传态 (Quantum Teleportation) 是发送方 Alice 和接收方 Bob 在没有量子通信信道连接的情况下，借助共享的EPR对同时仅通过发送两个经典比特，移动量子状态的一项技术。[1]实现首先，Alice 和 Bob 共享一堆 EPR 对，以 为例；同时，Alice还有一个量子比特 ，这也是 Alice 想要发送给 Bob 的；Alice 对量子比特 和EPR对中的一个量子比特进行操作，并将测量结果发给 Bob；Bob根据Alice的测量结果对他拥有EPR对中的一个量子比特进行一些操作即可恢复出。[1]具体的解释见下面的量子电路例子。例子电路的初始态为其中，.Alice 通过 CNOT 门发送她的量子位。 将 CNOT 应用于前两个量子位：然后对第一个量子比特应用 Hadamard gate第一项状态为 的 Alice 量子位和状态为 的 Bob 量子位，这是 的原始状态。如果 Alice 执行测量并获得结果 00，则 为 Bob 的投影，也即是说 Bob 的系统将处于 状态。通过投影，我们可以根据 Alice 的测量结果，获得 Bob 的 post-measurement state[2]：根据 Alice 的测量结果，Bob 的量子位最终将处于这四种可能状态之一。一旦Bob了解了测量结果，Bob 就可以通过应用适当的量子门来修复他的状态 .如果测量值为 00，则 Bob 无需执行任何操作；如果测量值为 01，则 Bob 可以通过应用 X 门来修复其状态；如果测量值为 10，则 Bob 可以通过应用 Z 门来修复其状态；如果测量值为 11，则 Bob 可以通过先应用 X，然后应用 Z 门来修复他的状态。综上，Alice 可以借助量子纠缠，并通过经典通信通道将她的测量结果传输给 Bob，从而实现一量子比特信息的传送。经典通道受到光速的限制，因此量子隐形传态不能以比光速更快的速度完成。本文使用 Zhihu On VSCode 创作并发布]]></summary></entry><entry><title type="html">量子计算入门Ⅳ：量子纠缠</title><link href="https://skylark0924.github.io/blog/2022/.md/" rel="alternate" type="text/html" title="量子计算入门Ⅳ：量子纠缠"/><published>2022-03-12T13:11:07+00:00</published><updated>2022-03-12T13:11:07+00:00</updated><id>https://skylark0924.github.io/blog/2022/.md</id><content type="html" xml:base="https://skylark0924.github.io/blog/2022/.md/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[量子计算入门Ⅳ：量子纠缠我记得在一次散步中，爱因斯坦突然停下来，看向我，问我是否真的相信月亮只有在我看着它时才存在。然后这次步行的其余时间，我们都在讨论物理学家应该用存在这个词来表达什么意思。(I recall that during one walk Einstein suddenly stopped, turned to me and asked whether I really believed that the moon exists only when I look at it. The rest of this walk was devoted to a discussion of what a physicist should mean by the term 'to exist'.)—— Abraham Pais参考教材：老师的ppt和 Quantum Computation and Quantum Information封面图引用自 Evading the uncertainty principle in quantum physics量子计算入门Ⅳ：量子纠缠什么是量子纠缠物理定义数学定义EPR 佯谬施密特分解证明例子施密特秩Bell state证明贝尔态的量子电路实现贝尔态的测量Evolution应用超密编码量子隐形传态什么是量子纠缠物理定义当几个粒子在彼此相互作用后，由于各个粒子所拥有的特性已综合成为整体性质，无法单独描述各个粒子的性质，只能描述整体系统的性质，则称这现象为量子纠缠（quantum entanglement）[1]1。量子纠缠是一种纯粹发生于量子系统的现象；在经典力学里，找不到类似的现象。对于两个相互纠缠的粒子分别测量其物理性质，像位置、动量、自旋、偏振等，则会发现量子状态之间的关联性。例子：亚原子粒子衰变成一对纠缠的其他粒子。假设一个零自旋粒子衰变为两个以相反方向移动分离的粒子。由于这种衰变之前和之后的总自旋必须为零（角动量守恒），因此每当测量第一个粒子在某个轴上旋转时，当在同一轴上测量时，总是发现另一个粒子是向下旋转的。这称为自旋反相关 (spin anti-correlated) 现象。如果测量每个自旋的先验概率相等，则称该对处于单线态 (singlet state)。[1]数学定义假设一个复合系统是由两个子系统A、B所组成，这两个子系统A、B的希尔伯特空间分别为 。其复合态表示为 ，如果 ，则 是 中的状态，是 中的状态。如果 不可分离的，则它是纠缠态，也就是说，如果它不能写成 ，则子系统A、B相互纠缠。EPR 佯谬EPR 佯谬，全称爱因斯坦-波多尔斯基-罗森佯谬 (Einstein-Podolsky-Rosen paradox)，是在论文 Can Quantum-Mechanical Description of Physical Reality Be Considered Complete? 中，以佯谬形式提出的针对量子力学哥本哈根诠释的早期重要批评。EPR论文建立于貌似合理的假设──定域论与实在论，合称为定域实在论。定域论：只允许在某区域发生的事件以不超过光速的传递方式影响其它区域。即，不允许鬼魅般的超距作用。实在论：粒子在被测量之前必须具有确定的位置和动量值。即，即使无人赏月，月亮依旧存在。施密特分解首先，假设两个希尔伯特空间 ，其正交基分别为 ，那么任意复合态 可以表示为而施密特分解 (Schmidt decomposition) 是指，该状态可以被分解为两组正交态 ，其中维度 其中，0, \sum^m_{j=1}\lambda_j^2=1" eeimg="1" style="max-width: 100%;" referrerpolicy="no-referrer">证明设 矩阵 .对其进行奇异值分解 ，其中 为 的酉矩阵， 为 的酉矩阵， 为 矩阵（对角线为奇异值，其余为0）。假设 有 个非零奇异值 ，因为 由 进行了正则化，也就意味着 因为 令 ，那么例子如果 的维数为 2（量子比特），而 的维数为 n，则 的维数为 。然而， 中的一般状态总是可以写成不超过两个项的总和因为 施密特秩施密特分解中的 为施密特秩 (Schmidt rank)，其值等于矩阵 的秩。当 时，该复合态可分 (separable)， 时，复合态为纠缠态。Bell state量子纠缠态的冯·诺依曼熵为零。贝尔态 (Bell state) 是最简单的一种二量子比特系统的量子纠缠态，包含四个特定的最大纠缠量子态，并形成两个量子位的希尔伯特空间的正交基。最大纠缠态是一种量子态，它对每个二分都有最大冯·诺依曼熵 [2]。证明对于一个复合态是否是纠缠态，就可以用施密特分解得到的施密特秩来判断。假设Bell态之一 可以被写作两个单量子态的张量积二者的张量积为那么应该有很明显，要满足上式就没法使 ，即不存在这样的两个单态可以将贝尔态分解为二者的张量积。贝尔态的量子电路实现贝尔态的测量Evolution对两个贝尔态进行相同的酉变换，不会改变其状态应用假设子系统 ，以及对应的观察者 Alice 和 Bob。B.t.w, 不知道为啥看到的材料都是 Alice 和 Bob 的例子，难道是什么梗？超密编码超密编码 (Superdense coding/dense coding) 是一种量子通信协议，通过只发送少量的量子比特来通信许多经典比特的信息 [3]。见本期外传Skylark：量子计算应用Ⅰ：超密编码量子隐形传输量子隐形传输 (Quantum Teleportation) 是一种利用量子纠缠来传送量子态至任意距离的技术。见本期外传Skylark：量子计算应用Ⅱ：量子隐形传输本文使用 WPL/s 发布 @GitHub]]></summary></entry><entry><title type="html">量子计算应用Ⅰ：超密编码</title><link href="https://skylark0924.github.io/blog/2022/.md/" rel="alternate" type="text/html" title="量子计算应用Ⅰ：超密编码"/><published>2022-03-12T13:07:20+00:00</published><updated>2022-03-12T13:07:20+00:00</updated><id>https://skylark0924.github.io/blog/2022/.md</id><content type="html" xml:base="https://skylark0924.github.io/blog/2022/.md/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[量子计算应用Ⅰ：超密编码我记得在一次散步中，爱因斯坦突然停下来，看向我，问我是否真的相信月亮只有在我看着它时才存在。然后这次步行的其余时间，我们都在讨论物理学家应该用存在这个词来表达什么意思。(I recall that during one walk Einstein suddenly stopped, turned to me and asked whether I really believed that the moon exists only when I look at it. The rest of this walk was devoted to a discussion of what a physicist should mean by the term ‘to exist’.)—— Abraham Pais参考教材：老师的ppt和 Quantum Computation and Quantum Information量子计算应用Ⅰ：超密编码定义实现定义超密编码 (Superdense coding) 是基本量子力学的一个简单但令人惊讶的应用。它结合了基本量子力学的所有基本思想，因此是可以使用量子力学完成的信息处理任务的理想示例。超密编码涉及两方，通常称为“Alice”和“Bob”，彼此相距甚远。他们的目标是将一些经典信息从 Alice 传输给 Bob。假设 Alice 拥有她希望发送给 Bob 的两个经典信息位，但只允许向 Bob 发送一个量子位。她能达到她的目的吗？实现假设 Alice 和 Bob 最初共享一对处于纠缠状态的量子比特，即 EPR 对Alice 最初拥有第一个 qubit，而 Bob 拥有第二个 qubit，如图 2.3 所示。注意 是一个固定状态；Alice 不需要向 Bob 发送任何量子比特来准备这个状态。相反，一些第三方可能会提前准备纠缠状态，将一个量子比特发送给 Alice，另一个发送给 Bob。Alice 可以通过改变她的量子比特，并发送给 Bob ，从而将两个经典比特的信息传达给 Bob。以下为 Alice 的程序如果她希望将经典比特 “00” 发送给 Bob，那么她不需要对其量子比特进行操作；如果她希望发送 “01”，那么需要将相位翻转的 Z gate 应用于其量子比特；如果她希望发送 “10”，那么她需要将 X gate将应用于她的量子比特;如果她希望发送“11”，那么她将 iY 门应用于她的量子位。以 01 为例，展开写一下量子门电路变换那么，对于该 EPR 对的变换为Bob 通过在 Bell 基下测量 EPR 对来确定 Alice 发了哪两个经典比特。综上，超密编码可以通过传输一个量子比特，实现两个经典比特信息的传输。本文使用 WPL/s 发布 @GitHub]]></summary></entry></feed>